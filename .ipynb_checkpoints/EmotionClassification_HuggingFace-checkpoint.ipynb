{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47a0a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets,load_dataset\n",
    "from transformers import AutoTokenizer,AutoModel,AutoModelForSequenceClassification\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af709f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Demo\\AppData\\Local\\Temp\\ipykernel_1940\\1844879057.py:2: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  all_datasets = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61741 datasets currently available on the Hub\n",
      "The first 10 are: ['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "#looking at available datasets in the hub...\n",
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\n",
    "print(f\"The first 10 are: {all_datasets[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38e0300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading emotions dataset using load_dataset object\n",
    "emotion_dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc39b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(emotion_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0330fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can work like a dictonary to divide accordingly\n",
    "train_ds = emotion_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8288e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong',\n",
       "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
       "  'i am feeling grouchy'],\n",
       " 'label': [0, 0, 3, 2, 3]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f1a679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7fc05ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can change format of dataset by set_format argument\n",
    "emotion_dataset.set_format(type=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dabb1ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = emotion_dataset['train'][:]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6d454d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts label integer to text labels....\n",
    "def label_int2str(row):\n",
    "    return emotion_dataset['train'].features['label'].int2str(row)\n",
    "\n",
    "df['label_name'] = df['label'].apply(label_int2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "170cd368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e183cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Distribution of the classes over training dataset....\n",
    "df['label_name'].value_counts(ascending=True).plot.bar()\n",
    "plt.title(\"Frequency of Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eaaae718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tweet Lengths with respect to each class....\n",
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False,\n",
    "showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "409fe05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can set and reset the dataset format accordingly....\n",
    "emotion_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e70a2",
   "metadata": {},
   "source": [
    "# Character Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "098baae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenization done at a character level\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9adb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the text\n",
    "token2idx = {ch:idx for idx,ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f0ba84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "#getting input_ids for text\n",
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455bed33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input ids are one hot encoded and then they are turned back into embeddings of lower dimensions....\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encoded = F.one_hot(input_ids,num_classes=len(token2idx))\n",
    "one_hot_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7bb1eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encoded[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34edb0",
   "metadata": {},
   "source": [
    " Word Tokenization and subword tokenization \n",
    "subword tokenization combines best of character tokenization and word tokenization \n",
    "wordpiece tokenizer is a example of it i.e. subword tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522f45f",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "57516322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating a toknizer.....\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f2f27018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing it with sample piece of text....\n",
    "text = 'Tokenizing text is a core part of NLP'\n",
    "encoded_text = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "91719ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 2112, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5badf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'part', 'of', 'nl', '##p', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#converting ids to tokens\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_text['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d41bde24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing text is a core part of nlp\n"
     ]
    }
   ],
   "source": [
    "#converting tokens to string.....\n",
    "print(tokenizer.convert_tokens_to_string(['token', '##izing', 'text', 'is', 'a', 'core', 'part', 'of', 'nl', '##p']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3f9c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 512, ['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size,tokenizer.model_max_length,tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed01498",
   "metadata": {},
   "source": [
    "## Tokenizing whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a0edfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    '''method to convert Dataset object type dataset into tokens'''\n",
    "    return tokenizer(batch['text'],padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cce96b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(train_ds[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f085c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoded = emotion_ds.map(tokenize,batched=True,batch_size=None)\n",
    "#batch_size=None considers whole dataset as a single batch making all input sizes global length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "528c076c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_encoded['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677cc75",
   "metadata": {},
   "source": [
    "## Classification model using features using transformer model....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c62576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset = emotion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "959b2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining tokenizer and model used to build classification model.......\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "#initialising tokenizer.....\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#initialising model......\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "28ab371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to tokenize each dataset using map function on the dataset object.....\n",
    "def tokenize(batch):\n",
    "    '''method to convert Dataset object type dataset into tokens'''\n",
    "    return tokenizer(batch['text'],padding=True,truncation=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "494d7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing toknizer and feautre extraction on the sample text of data....\n",
    "text = \"this is a test\"\n",
    "\n",
    "test_ip = tokenizer(text,return_tensors='pt')\n",
    "\n",
    "#if we are using model on cuda/cpu then we hve to move input_ids , attention mask to device accordingly..\n",
    "# test_ip_ = {k:v.to(device) for k,v in test_ip.items()} device=cuda/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d867d8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2c2e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "#while we are extracting the feature no_grad is used so that only tensors are come with less memory footprit....\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_ip)\n",
    "    \n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d9ca11d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "891d39c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a7dc4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del outputs, test_ip_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2f53c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can extract all the hidden states in dataset with map function on dataset object......\n",
    "def extract_hidden_states(batch):\n",
    "    \n",
    "    #place items to cuda\n",
    "    inputs = {k:v for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    \n",
    "    #extracting all the last layer outputs from model.......\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).last_hidden_state\n",
    "    \n",
    "    return {'hidden_state': output[:,0]} #this adds extra dict value to existing dataset object with hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f32afb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc6e2a0cccb4328bb8ec4c205013001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4006a7e4a1a49bf921636f0bb8ea484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382867c19c074575b9562dfee9fa540b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61e136b090d40ab96f85bafc9929fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizing the dataset.....\n",
    "emotion_encoded = emotion_dataset.map(tokenize,batched=True,batch_size=None)\n",
    "\n",
    "#before sending to feature extraction we should need to  change format type for col used by model.....\n",
    "emotion_encoded.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "#calling the extract function for feature extraction and this should be called on the tokenized dataset...\n",
    "emotions_hidden = emotion_encoded.map(extract_hidden_states,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb0786",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741edd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After generating the features of dataset...\n",
    "#now we are extracting the train and validation dataset training on ML model......\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing feature of each class using UMAP as it projects features to very low dimensional spaces.....\n",
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt to show distribution of each class.....\n",
    "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotion_ds[\"train\"].features[\"label\"].names\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d96b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the Logisitc Regression Model for features........\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train,y_train)\n",
    "lr_clf.score(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy classifier is baseline model building technique where models can be build using \n",
    "# different strategies like majority class etc..\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dum_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dum_clf.fit(X_train,y_train)\n",
    "dum_clf.score(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e1317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to plot the confusion matrix.....\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7073bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the confusion matrix for the LR model predictions....\n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae42ca",
   "metadata": {},
   "source": [
    "## Fine-Tuning with Huggingface Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc68c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset.....\n",
    "emtions_ds = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507cc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiating a toknizer.....\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#initiating the model for classification since we are dealing with text classification we use AutoModelForSequenceClasssification \n",
    "clf_model = AutoModelForSequenceClassification(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c5d1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to tokenize each dataset using map function on the dataset object.....\n",
    "def tokenize(batch):\n",
    "    '''method to convert Dataset object type dataset into tokens'''\n",
    "    return tokenizer(batch['text'],padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfdfe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e0210483a943dc940a9b7f408c7db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842b81aa8af84f98af71a6ed898af3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b8aecb36f4feab6e918b8fb560050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#toknizing whole dataset\n",
    "emotion_encoded = emtions_ds.map(tokinze,batched=True,batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b9de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can pass method to evaluate model with metrics we are interested to caputre during training...\n",
    "#following method captures F1_Socre and Accurarcy of model...\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds  = pred.predictions.argmax(-1)\n",
    "    f1     = f1_score(labels,preds,average='weighted')\n",
    "    acc    = accuracy_score(labels,preds)\n",
    "    return {\"accuracy\":acc,\"f1\":f1}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd3e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of writing training loop we have inbuild API that handle training of model....\n",
    "from transformers import Trainer,TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emtions_ds['train'])//batch_size\n",
    "\n",
    "#defining training metrics required for training and hyper-parameters....\n",
    "train_args = TrainingArguments(output_dir = model_name+'_emotion_ft',\n",
    "                                num_train_epochs=2,\n",
    "                                learning_rate=2e-5,\n",
    "                                per_device_train_batch_size=batch_size,\n",
    "                                per_device_eval_batch_size=batch_size,\n",
    "                                weight_decay=0.01,\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                disable_tqdm=False,\n",
    "                                logging_steps=logging_steps,\n",
    "                                log_level=\"error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca565975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the trainer method is used to encapsulate traing args with model,tokenizer and datasets and start training....\n",
    "trainer = Trainer(model=f_model,args=train_args,\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 train_dataset=emotion_encoded['train'],\n",
    "                 eval_dataset=emotion_encoded['validation'],\n",
    "                 tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edeb7c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 10:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>0.326990</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.901146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.225047</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.923415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5530635375976563, metrics={'train_runtime': 655.4719, 'train_samples_per_second': 48.82, 'train_steps_per_second': 0.763, 'total_flos': 720342861696000.0, 'train_loss': 0.5530635375976563, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training of model start....\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c31dc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for prediction we have trainer.predict() to get predictions.....\n",
    "pred_outputs = trainer.predict(emotion_encoded['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edefa4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.22504660487174988,\n",
       " 'test_accuracy': 0.9235,\n",
       " 'test_f1': 0.9234149719033905,\n",
       " 'test_runtime': 16.0595,\n",
       " 'test_samples_per_second': 124.537,\n",
       " 'test_steps_per_second': 1.993}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it has different attraibute that capture prediction metrics and predictions....\n",
    "pred_outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8421e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the predictions from raw_prediction using the argmax function\n",
    "y_preds = np.argmax(pred_outputs.predictions,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "788343ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes that are present in classification....\n",
    "labels = emotion_encoded['validation'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e3f333c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting CM for validation set....\n",
    "plot_confusion_matrix(y_preds,emotion_encoded['validation']['label'],labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ec719",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee490c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb42e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing error analysis on model prediction where model failed to get the correct predictions by getting the loss between them..\n",
    "def forward_pass_with_label(batch):\n",
    "    '''method adds the loss and predictions to existing dataset object..'''\n",
    "    \n",
    "    inputs = {k:v for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model = f_model.to('cpu')\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits,axis=-1)\n",
    "        loss = cross_entropy(output.logits,batch['label'],reduction='none')\n",
    "        \n",
    "        return {\"loss\":loss,\"predicted_label\":pred_label}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ed1deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoded.set_format(\"torch\",columns=['input_ids','attention_mask','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0541b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b548262bb3fb40e3bd010d225402a46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotion_encoded['validation'] = emotion_encoded['validation'].map(forward_pass_with_label,batched=True,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d6b9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to convert integer labels into classes....\n",
    "def label_int2str(row):\n",
    "    return emtions_ds['validation'].features['label'].int2str(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b818fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoded.set_format(\"pandas\")\n",
    "cols = ['text','label','predicted_label','loss']\n",
    "\n",
    "df_test = emotion_encoded['validation'][:][cols]\n",
    "df_test['label'] = df_test['label'].apply(label_int2str)\n",
    "df_test['predicted_label'] = df_test['predicted_label'].apply(label_int2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9efe922b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>i feel that he was being overshadowed by the s...</td>\n",
       "      <td>love</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.756338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>i feel badly about reneging on my commitment t...</td>\n",
       "      <td>love</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.715227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>i as representative of everything thats wrong ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.660019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>i called myself pro life and voted for perry w...</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.272655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label  \\\n",
       "1801  i feel that he was being overshadowed by the s...      love   \n",
       "882   i feel badly about reneging on my commitment t...      love   \n",
       "1950  i as representative of everything thats wrong ...  surprise   \n",
       "1963  i called myself pro life and voted for perry w...       joy   \n",
       "\n",
       "     predicted_label      loss  \n",
       "1801         sadness  5.756338  \n",
       "882          sadness  5.715227  \n",
       "1950         sadness  5.660019  \n",
       "1963         sadness  5.272655  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\",ascending=False).head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b37334c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>i got to christmas feeling positive about the ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.017439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>i feel he is an terrific really worth bet</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.018117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>i feel is more energetic in urban singapore th...</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.018451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>i feel this way about blake lively</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.018497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label predicted_label  \\\n",
       "578   i got to christmas feeling positive about the ...   joy             joy   \n",
       "632           i feel he is an terrific really worth bet   joy             joy   \n",
       "856   i feel is more energetic in urban singapore th...   joy             joy   \n",
       "1263                 i feel this way about blake lively   joy             joy   \n",
       "\n",
       "          loss  \n",
       "578   0.017439  \n",
       "632   0.018117  \n",
       "856   0.018451  \n",
       "1263  0.018497  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\",ascending=True).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1153b9",
   "metadata": {},
   "source": [
    "# Pipeline of model\n",
    "\n",
    "#### After model has trained and saved we can use it in pipeline object for direct utilization by directing model to it path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0071bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c672f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defing model path and getting started with \"text-classification\" task in pipeline object....\n",
    "model_id = \"D:/AI_ML_Code/NLP_Codes/distilbert-base-uncased_emotion_ft/checkpoint-500\"\n",
    "classifier = pipeline(\"text-classification\",model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfe7f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on the test......\n",
    "test_tweet = \"I saw a movie today and it was really good\"\n",
    "preds = classifier(test_tweet,top_k=None) #top_k is used to get top k highly probable classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "340ec0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9642190933227539},\n",
       " {'label': 'LABEL_0', 'score': 0.012037712149322033},\n",
       " {'label': 'LABEL_2', 'score': 0.010250387713313103},\n",
       " {'label': 'LABEL_5', 'score': 0.006152282934635878},\n",
       " {'label': 'LABEL_4', 'score': 0.003751580137759447},\n",
       " {'label': 'LABEL_3', 'score': 0.003588981693610549}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f010fa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_df = pd.DataFrame(preds[:]).sort_values('label')\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{test_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a4bf8",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f4a0bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray([[1,3,5,7],\n",
    "               [3,4,6,2],\n",
    "               [8,3,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "54c70427",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x.mean(axis=0) \n",
    "std  = x.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fb7b7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x-mean/np.sqrt(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5053c89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.66870642, -1.52158438,  1.00949886,  1.27582572]),\n",
       " array([2.94392029, 0.47140452, 1.69967317, 2.62466929]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.mean(axis=0),x_.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1352445",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cf219245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = np.array([[0.31,0.14,0.93],[0.14,0.88,0.98]]) # \"Popcorn Popped.\"\n",
    "sentence2 = np.array([[0.85,0.2,0.14],[0.46,0.61,0.49]]) # \"Tea Steeped.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e57439e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average and Variace for First Sentence:\n",
    "average1 = sentence1.mean()\n",
    "variance1 = sentence1.var()\n",
    "\n",
    "# Average and Variance for Second Sentence:\n",
    "average2 = sentence2.mean()\n",
    "variance2 = sentence2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "76508727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1:\n",
      "[[0.31 0.14 0.93]\n",
      " [0.14 0.88 0.98]]\n",
      "\n",
      " Sentence1 (Normalized):\n",
      "[[-0.68074565 -1.1375618   0.98528975]\n",
      " [-1.1375618   0.85093206  1.11964744]]\n",
      "\n",
      "Sentence2:\n",
      "[[0.85 0.2  0.14]\n",
      " [0.46 0.61 0.49]]\n",
      "\n",
      " Sentence2 (Normalized):\n",
      "[[ 1.63221997 -1.07657062 -1.32661282]\n",
      " [ 0.00694562  0.63205114  0.13196672]]\n"
     ]
    }
   ],
   "source": [
    "# Sentence 1 Normalization:\n",
    "sentence1_norm = (sentence1 - average1)/(np.sqrt(variance1))\n",
    "print(f\"Sentence1:\\n{sentence1}\\n\\n Sentence1 (Normalized):\\n{sentence1_norm}\\n\")\n",
    "\n",
    "# Sentence 2 Normalization:\n",
    "sentence2_norm = (sentence2 - average2)/(np.sqrt(variance2))\n",
    "print(f\"Sentence2:\\n{sentence2}\\n\\n Sentence2 (Normalized):\\n{sentence2_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d8a8f348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1:\n",
      "tensor([[0.3100, 0.1400, 0.9300],\n",
      "        [0.1400, 0.8800, 0.9800]], dtype=torch.float64)\n",
      "\n",
      " Sentence1 (Normalized):\n",
      "tensor([[-0.6807, -1.1375,  0.9853],\n",
      "        [-1.1375,  0.8509,  1.1196]], grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "Sentence2:\n",
      "tensor([[0.8500, 0.2000, 0.1400],\n",
      "        [0.4600, 0.6100, 0.4900]], dtype=torch.float64)\n",
      "\n",
      " Sentence2 (Normalized):\n",
      "tensor([[ 1.6321, -1.0765, -1.3265],\n",
      "        [ 0.0069,  0.6320,  0.1320]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch1 = torch.from_numpy(sentence1) # \"Popcorn Popped.\"\n",
    "torch2 = torch.from_numpy(sentence2) # \"Tea Steeped.\"\n",
    "\n",
    "layer_norm = torch.nn.LayerNorm(torch1.size())\n",
    "\n",
    "# Sentence 1 Normalization:\n",
    "torch1_norm = layer_norm(torch1.float())\n",
    "print(f\"Sentence1:\\n{torch1}\\n\\n Sentence1 (Normalized):\\n{torch1_norm}\\n\")\n",
    "\n",
    "# Sentence 2 Normalization:\n",
    "torch2_norm = layer_norm(torch2.float())\n",
    "print(f\"Sentence2:\\n{torch2}\\n\\n Sentence2 (Normalized):\\n{torch2_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "5d2f31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input matrixtorch.Size([2, 2, 3])\n",
      "Layer Normalized values:\n",
      " tensor([[[-0.6807, -1.1375,  0.9853],\n",
      "         [-1.1375,  0.8509,  1.1196]],\n",
      "\n",
      "        [[ 1.6321, -1.0765, -1.3265],\n",
      "         [ 0.0069,  0.6320,  0.1320]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#stacking examples on upon other\n",
    "tr = torch.stack((torch1,torch2))\n",
    "print(f'Shape of the input matrix{tr.shape}')\n",
    "\n",
    "ln_norm = layer_norm(tr.float())\n",
    "print(f'Layer Normalized values:\\n {ln_norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350cc4f",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "276dd0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Input shape : torch.Size([2, 2, 3])\n",
      "Input:\n",
      " tensor([[[0.3100, 0.1400, 0.9300],\n",
      "         [0.1400, 0.8800, 0.9800]],\n",
      "\n",
      "        [[0.8500, 0.2000, 0.1400],\n",
      "         [0.4600, 0.6100, 0.4900]]], dtype=torch.float64)\n",
      "reshaped inputs after reshaping : torch.Size([2, 3, 2])\n",
      "inputs after reshaping :\n",
      " tensor([[[0.3100, 0.1400],\n",
      "         [0.1400, 0.8800],\n",
      "         [0.9300, 0.9800]],\n",
      "\n",
      "        [[0.8500, 0.4600],\n",
      "         [0.2000, 0.6100],\n",
      "         [0.1400, 0.4900]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#let us follow up with same example above then we have 2 sentences of sequence length 2 so shape is (2,2,3)\n",
    "inputs = tr\n",
    "print(f'Current Input shape : {inputs.shape}')\n",
    "print(f'Input:\\n {inputs}')\n",
    "\n",
    "#now we are reshaping inputs into (N,C/Features,SeqLen) as batch norm layer follows that pattern\n",
    "re_inputs = inputs.transpose(2,1)\n",
    "print(f'reshaped inputs after reshaping : {re_inputs.shape}')\n",
    "print(f'inputs after reshaping :\\n {re_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "baadb29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values across features : \n",
      " tensor([[0.5800, 0.3000],\n",
      "        [0.1700, 0.7450],\n",
      "        [0.5350, 0.7350]], dtype=torch.float64)\n",
      "Variance values across features : \n",
      " tensor([[0.0729, 0.0256],\n",
      "        [0.0009, 0.0182],\n",
      "        [0.1560, 0.0600]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#average and variance\n",
    "in_mean = re_inputs.mean(axis=0)\n",
    "in_var  = re_inputs.var(axis=0,unbiased=False)\n",
    "\n",
    "print(f\"Mean values across features : \\n {in_mean}\")\n",
    "print(f\"Variance values across features : \\n {in_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "1f4a23f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean values across features : \n",
      " tensor([[0.4400],\n",
      "        [0.4575],\n",
      "        [0.6350]], dtype=torch.float64)\n",
      "Variance values across features : \n",
      " tensor([[0.0688],\n",
      "        [0.0922],\n",
      "        [0.1180]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#average and variance\n",
    "in_mean1 = in_mean.mean(axis=1,keepdim=True)\n",
    "in_var1  = ((re_inputs - in_mean1.mean(1,keepdim=True))**2).mean(0).mean(1,keepdim=True)\n",
    "\n",
    "print(f\"Mean values across features : \\n {in_mean1}\")\n",
    "print(f\"Variance values across features : \\n {in_var1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "57b2b68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch normalized input is : \n",
      " tensor([[[-0.4954, -1.1433],\n",
      "         [-1.0455,  1.3913],\n",
      "         [ 0.8587,  1.0042]],\n",
      "\n",
      "        [[ 1.5625,  0.0762],\n",
      "         [-0.8479,  0.5022],\n",
      "         [-1.4408, -0.4221]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#final batch normalized layers values\n",
    "bat_norm = (re_inputs - in_mean1)/torch.sqrt(in_var1)\n",
    "print(f\"batch normalized input is : \\n {bat_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "8e4fc833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4954, -1.1433],\n",
       "         [-1.0455,  1.3913],\n",
       "         [ 0.8587,  1.0042]],\n",
       "\n",
       "        [[ 1.5625,  0.0762],\n",
       "         [-0.8479,  0.5022],\n",
       "         [-1.4408, -0.4221]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the output by avearaing over multiple axes....\n",
    "(re_inputs - re_inputs.mean((0,2),keepdim=True)) / torch.sqrt(re_inputs.var((0,2),unbiased=False,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "c1dca379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch normalized input is : \n",
      " tensor([[[-0.4954, -1.1432],\n",
      "         [-1.0455,  1.3912],\n",
      "         [ 0.8587,  1.0042]],\n",
      "\n",
      "        [[ 1.5624,  0.0762],\n",
      "         [-0.8479,  0.5022],\n",
      "         [-1.4408, -0.4220]]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#defining batch normalization layer with number of features......\n",
    "batch_norm = torch.nn.BatchNorm1d(3)\n",
    "\n",
    "#batch normalized output...\n",
    "bat_norm_layer = batch_norm(re_inputs.float())\n",
    "print(f\"batch normalized input is : \\n {bat_norm_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718d28c",
   "metadata": {},
   "source": [
    "# Calculating variance over  multiple axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "0b1195e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance over the first and second axes: [9.66666667 9.66666667]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3D matrix\n",
    "matrix = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "\n",
    "# Calculate the variance over the first and second axes\n",
    "variance_axes_1_and_2 = np.var(matrix, axis=(0, 2))\n",
    "\n",
    "# Print the results\n",
    "print(\"Variance over the first and second axes:\", variance_axes_1_and_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "0a5e811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over the first axes:\n",
      " [[4. 5. 6.]\n",
      " [7. 8. 9.]]\n",
      "Mean over the second axes on first axes:\n",
      " [[5.]\n",
      " [8.]]\n"
     ]
    }
   ],
   "source": [
    "# the abvoe operation can also be acheived via individual operations.....\n",
    "mean_axes0 = matrix.mean(axis=0)\n",
    "print(\"Mean over the first axes:\\n\", mean_axes0)\n",
    "\n",
    "mean_axes1 = mean_axes0.mean(1,keepdims=True)\n",
    "print(\"Mean over the second axes on first axes:\\n\", mean_axes1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "cc7b2669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Varaince over the first and second axes:\n",
      " [9.66666667 9.66666667]\n"
     ]
    }
   ],
   "source": [
    "#varaince over multiple axes formula....\n",
    "var_ = ((matrix - mean_axes1)**2).mean(0).mean(1)\n",
    "print(\"Varaince over the first and second axes:\\n\", var_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936bb25",
   "metadata": {},
   "source": [
    "# Applying batch and layer normalization on same 2D matrix....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "6c66cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix = torch.tensor([[1,2,3],\n",
    "                            [4,5,6],\n",
    "                            [7,8,9]],dtype=float)\n",
    "\n",
    "\n",
    "l_norm = torch.nn.LayerNorm(3)\n",
    "b_norm = torch.nn.BatchNorm1d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "926e525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Normalization with mathematic output :\n",
      " tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#layer normalized output calculated from matrix...\n",
    "mat_l_norm = (input_matrix - input_matrix.mean(1,keepdim=True)) / torch.sqrt(input_matrix.var(1,keepdim=True,unbiased=False))\n",
    "print(f\"Layer Normalization with mathematic output :\\n {mat_l_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "937dbea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247,  0.0000,  1.2247],\n",
       "        [-1.2247,  0.0000,  1.2247],\n",
       "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#layer normalized output from layer....\n",
    "l_norm(input_matrix.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "cbda778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247, -1.2247, -1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 1.2247,  1.2247,  1.2247]], dtype=torch.float64)"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bacth Normalizatio we calculate mean by adding up rows and dividing each colum values by #rowsdd\n",
    "(input_matrix - input_matrix.mean(0))/torch.sqrt(input_matrix.var(0,unbiased=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "e778e265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2247, -1.2247, -1.2247],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 1.2247,  1.2247,  1.2247]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch normalized output from layer......\n",
    "b_norm(input_matrix.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "811be85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1f5ad_row0_col0, #T_1f5ad_row1_col0 {\n",
       "  width: 300px;\n",
       "  height: 100px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1f5ad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1f5ad_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_1f5ad_level0_col1\" class=\"col_heading level0 col1\" >number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5ad_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1f5ad_row0_col0\" class=\"data row0 col0\" >foo foo foo foo foo foo foo foo</td>\n",
       "      <td id=\"T_1f5ad_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1f5ad_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1f5ad_row1_col0\" class=\"data row1 col0\" >bar bar bar bar bar</td>\n",
       "      <td id=\"T_1f5ad_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c524a13650>"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test data\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'text': ['foo foo foo foo foo foo foo foo', 'bar bar bar bar bar'],\n",
    "                 'number': [1, 2]})\n",
    "df.style.set_properties(subset=['text'], **{'width': '300px','height': '100px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "18a144f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6404180526733398 0.6404180765580826\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "test_tensor = torch.randn((3,4,5))\n",
    "v1 = test_tensor.var(dim=[0,2], unbiased=False)[0]  # pick the first coordinate of the result\n",
    "mean = 0\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        mean += test_tensor[i, 0, j].item()\n",
    "mean /= 15\n",
    "v2 = 0\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        v2 += (test_tensor[i, 0, j].item() - mean)**2\n",
    "v2 /= 15\n",
    "print(v1.item(), v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "ed35ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3305)\n",
      "tensor(0.9763)\n",
      "tensor(-2.0650)\n",
      "tensor(-0.9852)\n",
      "tensor(-0.3326)\n",
      "tensor(-0.4351)\n",
      "tensor(0.3618)\n",
      "tensor(-0.1840)\n",
      "tensor(0.2669)\n",
      "tensor(1.2400)\n",
      "tensor(-0.4002)\n",
      "tensor(-0.0227)\n",
      "tensor(-0.1419)\n",
      "tensor(-0.1013)\n",
      "tensor(-1.3679)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        print(test_tensor[i, 0, j])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c7d72d4",
   "metadata": {},
   "source": [
    "rnn_ = torch.nn.RNN(input_size=5,hidden_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "e1f2f606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_.weight_hh_l0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "ec6f45c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RNN in module torch.nn.modules.rnn object:\n",
      "\n",
      "class RNN(RNNBase)\n",
      " |  RNN(*args, **kwargs)\n",
      " |  \n",
      " |  Applies a multi-layer Elman RNN with :math:`\\tanh` or :math:`\\text{ReLU}` non-linearity to an\n",
      " |  input sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |      h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh})\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is\n",
      " |  the input at time `t`, and :math:`h_{(t-1)}` is the hidden state of the\n",
      " |  previous layer at time `t-1` or the initial hidden state at time `0`.\n",
      " |  If :attr:`nonlinearity` is ``'relu'``, then :math:`\\text{ReLU}` is used instead of :math:`\\tanh`.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input `x`\n",
      " |      hidden_size: The number of features in the hidden state `h`\n",
      " |      num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
      " |          would mean stacking two RNNs together to form a `stacked RNN`,\n",
      " |          with the second RNN taking in outputs of the first RNN and\n",
      " |          computing the final results. Default: 1\n",
      " |      nonlinearity: The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``\n",
      " |      bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
      " |          Default: ``True``\n",
      " |      batch_first: If ``True``, then the input and output tensors are provided\n",
      " |          as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
      " |          Note that this does not apply to hidden or cell states. See the\n",
      " |          Inputs/Outputs sections below for details.  Default: ``False``\n",
      " |      dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
      " |          RNN layer except the last layer, with dropout probability equal to\n",
      " |          :attr:`dropout`. Default: 0\n",
      " |      bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``\n",
      " |  \n",
      " |  Inputs: input, h_0\n",
      " |      * **input**: tensor of shape :math:`(L, H_{in})` for unbatched input,\n",
      " |        :math:`(L, N, H_{in})` when ``batch_first=False`` or\n",
      " |        :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n",
      " |        the input sequence.  The input can also be a packed variable length sequence.\n",
      " |        See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
      " |        :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
      " |      * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n",
      " |        :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the initial hidden\n",
      " |        state for the input sequence batch. Defaults to zeros if not provided.\n",
      " |  \n",
      " |      where:\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              N ={} & \\text{batch size} \\\\\n",
      " |              L ={} & \\text{sequence length} \\\\\n",
      " |              D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
      " |              H_{in} ={} & \\text{input\\_size} \\\\\n",
      " |              H_{out} ={} & \\text{hidden\\_size}\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |  Outputs: output, h_n\n",
      " |      * **output**: tensor of shape :math:`(L, D * H_{out})` for unbatched input,\n",
      " |        :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n",
      " |        :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n",
      " |        `(h_t)` from the last layer of the RNN, for each `t`. If a\n",
      " |        :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n",
      " |        will also be a packed sequence.\n",
      " |      * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n",
      " |        :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the final hidden state\n",
      " |        for each element in the batch.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
      " |          of shape `(hidden_size, input_size)` for `k = 0`. Otherwise, the shape is\n",
      " |          `(hidden_size, num_directions * hidden_size)`\n",
      " |      weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
      " |          of shape `(hidden_size, hidden_size)`\n",
      " |      bias_ih_l[k]: the learnable input-hidden bias of the k-th layer,\n",
      " |          of shape `(hidden_size)`\n",
      " |      bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer,\n",
      " |          of shape `(hidden_size)`\n",
      " |  \n",
      " |  .. note::\n",
      " |      All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
      " |      where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
      " |  \n",
      " |  .. note::\n",
      " |      For bidirectional RNNs, forward and backward are directions 0 and 1 respectively.\n",
      " |      Example of splitting the output layers when ``batch_first=False``:\n",
      " |      ``output.view(seq_len, batch, num_directions, hidden_size)``.\n",
      " |  \n",
      " |  .. note::\n",
      " |      ``batch_first`` argument is ignored for unbatched inputs.\n",
      " |  \n",
      " |  .. include:: ../cudnn_rnn_determinism.rst\n",
      " |  \n",
      " |  .. include:: ../cudnn_persistent_rnn.rst\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.RNN(10, 20, 2)\n",
      " |      >>> input = torch.randn(5, 3, 10)\n",
      " |      >>> h0 = torch.randn(2, 3, 20)\n",
      " |      >>> output, hn = rnn(input, h0)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RNN\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input, hx=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setattr__(self, attr, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  check_forward_args(self, input: torch.Tensor, hidden: torch.Tensor, batch_sizes: Optional[torch.Tensor])\n",
      " |  \n",
      " |  check_hidden_size(self, hx: torch.Tensor, expected_hidden_size: Tuple[int, int, int], msg: str = 'Expected hidden size {}, got {}') -> None\n",
      " |  \n",
      " |  check_input(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> None\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  flatten_parameters(self) -> None\n",
      " |      Resets parameter data pointer so that they can use faster code paths.\n",
      " |      \n",
      " |      Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
      " |      Otherwise, it's a no-op.\n",
      " |  \n",
      " |  get_expected_hidden_size(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> Tuple[int, int, int]\n",
      " |  \n",
      " |  permute_hidden(self, hx: torch.Tensor, permutation: Optional[torch.Tensor])\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from RNNBase:\n",
      " |  \n",
      " |  __constants__ = ['mode', 'input_size', 'hidden_size', 'num_layers', 'b...\n",
      " |  \n",
      " |  __jit_unused_properties__ = ['all_weights']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rnn_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d083667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for conv1.weight:\n",
      "tensor([[[[ 0.1940, -0.1508,  0.0042],\n",
      "          [-0.0445,  0.2186,  0.0947],\n",
      "          [-0.0478,  0.8126, -0.1507]],\n",
      "\n",
      "         [[-0.0572,  0.0430, -0.1653],\n",
      "          [ 0.1535,  0.5712,  0.5737],\n",
      "          [-0.0700, -0.0625, -0.4551]],\n",
      "\n",
      "         [[-0.0854,  0.8557,  0.0938],\n",
      "          [ 0.0363,  0.0904,  0.1206],\n",
      "          [-0.0315,  0.0883, -0.1089]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2183, -0.2517, -0.2773],\n",
      "          [-0.3895, -0.5221, -0.4192],\n",
      "          [ 0.2318, -0.8112,  0.0566]],\n",
      "\n",
      "         [[ 0.6148, -0.2857, -0.0075],\n",
      "          [-0.9015,  0.4022,  0.1030],\n",
      "          [ 0.3806, -0.3457,  0.4914]],\n",
      "\n",
      "         [[-0.3601, -0.0821,  0.1402],\n",
      "          [-0.0793,  0.3597, -0.0153],\n",
      "          [ 0.3947, -0.1202,  0.5051]]],\n",
      "\n",
      "\n",
      "        [[[-0.2278,  0.8215, -0.9022],\n",
      "          [-0.4375,  0.2821,  0.3402],\n",
      "          [ 0.5904, -0.0485,  0.3070]],\n",
      "\n",
      "         [[ 0.3793, -0.0146, -0.9801],\n",
      "          [-0.2815, -0.3339,  1.0883],\n",
      "          [-0.1059, -0.7793, -0.2768]],\n",
      "\n",
      "         [[-0.3624, -0.3754, -0.5587],\n",
      "          [ 0.3422, -0.0282, -1.0570],\n",
      "          [ 1.0860,  0.1111,  0.4296]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.6015,  0.0847,  0.1982],\n",
      "          [ 0.0451, -0.1672, -0.9543],\n",
      "          [ 0.4976,  0.6814,  0.2404]],\n",
      "\n",
      "         [[-0.1588, -0.4240,  0.2105],\n",
      "          [-0.0988,  0.1890, -0.1163],\n",
      "          [-0.6210,  0.3328,  0.0039]],\n",
      "\n",
      "         [[ 0.3980, -0.3672,  0.8546],\n",
      "          [ 0.6118, -0.4438,  0.0049],\n",
      "          [-0.3323, -0.2533,  0.1067]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1137,  0.1807,  0.0864],\n",
      "          [-0.3008, -0.6279, -0.3043],\n",
      "          [-0.2884, -0.0933,  0.1974]],\n",
      "\n",
      "         [[-0.5898,  0.5473, -0.1203],\n",
      "          [-0.1856,  0.0029, -0.6836],\n",
      "          [-0.1397, -0.1284,  0.0380]],\n",
      "\n",
      "         [[-0.0377, -0.1056,  0.0176],\n",
      "          [-0.1714, -0.4576, -0.8323],\n",
      "          [ 0.0943, -0.5733, -0.3735]]],\n",
      "\n",
      "\n",
      "        [[[-0.3186,  0.2800,  0.1310],\n",
      "          [ 0.0900, -0.7384,  0.4120],\n",
      "          [-0.0413, -0.4802,  0.2941]],\n",
      "\n",
      "         [[ 0.5550,  0.2743,  0.0706],\n",
      "          [ 0.1471,  0.3631,  0.0651],\n",
      "          [-0.5586,  0.4863, -0.2540]],\n",
      "\n",
      "         [[-0.3865,  0.3080,  0.3008],\n",
      "          [ 0.4906,  0.1523,  0.6144],\n",
      "          [ 0.1842,  0.4189, -0.1699]]]])\n",
      "Gradient for norm1.weight:\n",
      "tensor([-0.1485,  0.1421, -0.1155,  0.0724,  0.3105, -0.0991,  0.0463, -0.0189,\n",
      "        -0.5049, -0.0967, -0.0471, -0.1206,  0.1074,  0.1881, -0.3151,  0.1344,\n",
      "         0.2063,  0.1198, -0.1908,  0.1462,  0.0737, -0.0301,  0.2707, -0.0182,\n",
      "         0.1254,  0.0782, -0.1980, -0.0855,  0.0506, -0.2533,  0.1310, -0.0819,\n",
      "        -0.0717,  0.1531, -0.0456, -0.1718, -0.3793,  0.6203,  0.0418, -0.0833,\n",
      "         0.2847,  0.2364, -0.4128,  0.0380,  0.4342,  0.1276,  0.0920, -0.0524,\n",
      "        -0.1734, -0.2387,  0.2814,  0.2629,  0.2124, -0.0282,  0.0492,  0.0427,\n",
      "         0.0680,  0.0278, -0.1513,  0.2319, -0.1874,  0.2720, -0.4998, -0.0495])\n",
      "Gradient for norm1.bias:\n",
      "tensor([-2.2322e-01,  2.3032e-01, -8.2896e-02,  1.2024e-01,  5.4764e-01,\n",
      "         2.1811e-01,  1.6510e-01, -1.1627e-01, -4.3732e-01, -3.0979e-01,\n",
      "         8.6498e-02, -1.3119e-01,  2.5356e-02, -2.1399e-02, -1.0653e-01,\n",
      "        -5.3413e-02,  2.9813e-01,  5.4381e-02, -8.8589e-02, -1.5719e-01,\n",
      "        -1.3203e-01, -6.1914e-03,  9.7189e-02, -1.5250e-01,  1.7753e-01,\n",
      "         3.1090e-02, -3.0418e-01, -1.2947e-01, -1.4454e-01, -2.0019e-02,\n",
      "         2.8778e-01,  2.2341e-02, -2.7164e-04,  5.3070e-02,  4.4222e-02,\n",
      "        -2.2355e-01, -8.2289e-02,  2.8403e-01, -2.9009e-02, -1.9221e-01,\n",
      "         1.4754e-01,  5.2029e-02, -2.1681e-01, -8.6217e-02,  2.3382e-01,\n",
      "         2.5907e-02,  6.2677e-03, -1.1648e-01, -2.3868e-02, -7.4256e-02,\n",
      "         3.3478e-01,  2.8096e-01,  2.2799e-01,  1.9604e-01, -4.7298e-02,\n",
      "         5.2348e-02,  4.1214e-02,  9.5987e-02, -4.8647e-02,  2.4655e-01,\n",
      "        -2.7680e-01,  3.5342e-01, -2.4084e-01, -8.4216e-02])\n",
      "Gradient for layer1.0.conv1.weight:\n",
      "tensor([[[[-0.1337, -0.0249, -0.3246],\n",
      "          [-0.3916,  0.0557,  0.1078],\n",
      "          [-0.0569,  0.2128, -0.1798]],\n",
      "\n",
      "         [[-0.0136,  0.1986, -0.7205],\n",
      "          [ 0.2154, -0.1997, -0.0683],\n",
      "          [ 0.2571, -0.0683, -0.2900]],\n",
      "\n",
      "         [[-0.0071, -0.5754, -0.1220],\n",
      "          [ 0.0495, -0.6241, -0.3090],\n",
      "          [ 0.0469,  0.3217, -0.0618]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6862,  0.1084, -0.0919],\n",
      "          [-0.4214, -0.3549, -0.2172],\n",
      "          [ 0.3228, -0.0904,  0.0804]],\n",
      "\n",
      "         [[ 0.7868, -0.7905, -0.2940],\n",
      "          [-0.5162,  0.0807, -0.5892],\n",
      "          [-0.3797, -0.3837, -0.5924]],\n",
      "\n",
      "         [[-0.0877,  0.0381, -0.0573],\n",
      "          [ 0.5538,  0.2953,  0.4745],\n",
      "          [-0.1951, -0.1362, -0.1550]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2032,  0.3434, -0.3593],\n",
      "          [-0.3752,  0.5737,  0.5974],\n",
      "          [ 0.0854, -0.3323,  0.3938]],\n",
      "\n",
      "         [[ 0.0102,  0.0522,  0.3809],\n",
      "          [-0.2439, -0.5050, -0.1576],\n",
      "          [ 0.4076, -0.2822,  0.0991]],\n",
      "\n",
      "         [[ 0.2184, -0.2728, -0.4293],\n",
      "          [ 0.0204,  0.0128,  0.1765],\n",
      "          [ 0.0689,  0.7461,  0.3297]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6912, -0.2587,  0.2474],\n",
      "          [-0.2259, -0.0630, -0.0350],\n",
      "          [ 0.1782,  0.2129,  0.2589]],\n",
      "\n",
      "         [[ 0.1334,  0.0959,  0.2175],\n",
      "          [ 0.1439, -0.1437, -0.1390],\n",
      "          [-0.2289,  0.0748, -0.6392]],\n",
      "\n",
      "         [[-0.7124,  0.4548,  0.1096],\n",
      "          [-0.0700, -0.0358,  0.3431],\n",
      "          [ 0.1636, -0.3212, -0.3307]]],\n",
      "\n",
      "\n",
      "        [[[-0.1529, -0.0496,  0.2578],\n",
      "          [ 0.1106, -0.5751,  0.2968],\n",
      "          [-0.1161,  0.1212, -0.3363]],\n",
      "\n",
      "         [[-0.4466, -0.3708,  0.4583],\n",
      "          [ 0.0217, -0.2079, -0.2551],\n",
      "          [-0.2648,  0.0976,  0.5673]],\n",
      "\n",
      "         [[ 0.0719, -0.1529,  0.0646],\n",
      "          [ 0.2327, -0.1043, -0.2610],\n",
      "          [-0.1787,  0.2807,  0.3442]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1059,  0.5693, -0.1407],\n",
      "          [-0.0902, -0.3973, -0.2314],\n",
      "          [-0.3098,  0.0737,  0.7129]],\n",
      "\n",
      "         [[ 0.4116, -0.3686,  0.1075],\n",
      "          [-0.0027,  0.4351, -0.0333],\n",
      "          [-0.0436, -0.1351, -0.3197]],\n",
      "\n",
      "         [[ 0.0321, -0.1957,  0.8735],\n",
      "          [ 0.0932,  0.2533, -0.9192],\n",
      "          [-0.2597, -0.1425, -0.0608]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3190, -0.6430, -0.2090],\n",
      "          [-0.1970, -0.0506, -0.1442],\n",
      "          [ 0.1860, -0.1049, -0.4841]],\n",
      "\n",
      "         [[ 0.6173,  0.1562, -0.3351],\n",
      "          [ 0.1010,  0.4455,  0.1330],\n",
      "          [-0.2357, -0.1846,  0.0750]],\n",
      "\n",
      "         [[-0.0393, -0.4149, -0.1147],\n",
      "          [-0.0856,  0.2226,  0.4198],\n",
      "          [ 0.0422, -0.0397, -0.1436]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1420, -0.6488,  0.2511],\n",
      "          [ 0.0323, -0.0965, -0.1564],\n",
      "          [-0.2709,  0.2798, -0.4870]],\n",
      "\n",
      "         [[ 0.0331,  0.1331,  0.0675],\n",
      "          [ 0.4856,  0.1443,  0.1552],\n",
      "          [-0.2467,  0.1756, -0.1915]],\n",
      "\n",
      "         [[-0.1016,  0.3065,  0.0456],\n",
      "          [ 0.3854,  0.2342,  0.0580],\n",
      "          [ 0.0401, -0.0721,  0.0169]]],\n",
      "\n",
      "\n",
      "        [[[-0.2237, -0.0406, -0.0554],\n",
      "          [ 0.2121,  0.1364, -0.4654],\n",
      "          [ 0.5130, -0.0288,  0.8318]],\n",
      "\n",
      "         [[ 0.0423, -0.2752, -0.1782],\n",
      "          [ 0.1935, -0.0147, -0.0700],\n",
      "          [ 0.0390, -0.0594,  0.2343]],\n",
      "\n",
      "         [[-0.4531,  0.1578, -0.1653],\n",
      "          [ 0.0914,  0.0640, -0.3503],\n",
      "          [-0.5828, -0.0526, -0.2978]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1158, -0.4190,  0.1206],\n",
      "          [ 0.1925, -0.1195, -0.3392],\n",
      "          [-0.8007, -0.0103,  0.1698]],\n",
      "\n",
      "         [[-0.1461, -0.0758, -0.3326],\n",
      "          [-0.5246,  0.1280,  0.3778],\n",
      "          [ 0.3765, -0.1123, -0.4756]],\n",
      "\n",
      "         [[ 0.6756, -0.3989,  0.3088],\n",
      "          [ 0.0848,  0.0020,  0.0968],\n",
      "          [ 0.3722, -0.2786, -0.1147]]],\n",
      "\n",
      "\n",
      "        [[[-0.1788,  0.2903, -0.2099],\n",
      "          [-0.6841, -0.4744,  0.1748],\n",
      "          [ 0.0086,  0.0903, -0.1272]],\n",
      "\n",
      "         [[ 0.1033,  0.7536,  0.1906],\n",
      "          [ 0.0252, -0.1472,  0.4041],\n",
      "          [-0.4227, -0.2132,  0.3330]],\n",
      "\n",
      "         [[-0.2885,  0.3765,  0.2096],\n",
      "          [-0.2525, -0.3185, -0.0620],\n",
      "          [-0.8081,  0.4250, -0.1619]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1664, -0.1412,  0.4343],\n",
      "          [-0.0499,  0.1672, -0.7731],\n",
      "          [ 0.4728, -0.6592,  0.3155]],\n",
      "\n",
      "         [[ 0.3176,  0.2944, -0.3030],\n",
      "          [ 0.3997,  0.3551,  0.4840],\n",
      "          [-0.3687,  0.0285, -0.1666]],\n",
      "\n",
      "         [[ 0.1022, -0.2175,  0.2665],\n",
      "          [ 0.4921, -0.1653, -0.0700],\n",
      "          [ 0.0306,  0.3263, -0.2085]]]])\n",
      "Gradient for layer1.0.norm1.weight:\n",
      "tensor([ 0.0845,  0.1655,  0.0710, -0.0465, -0.0260,  0.0950,  0.0889, -0.1284,\n",
      "        -0.1861,  0.2929, -0.1281, -0.0849, -0.0198,  0.1173,  0.0525, -0.0292,\n",
      "         0.2437, -0.1968,  0.1315, -0.0340,  0.0924, -0.0781, -0.1499, -0.0580,\n",
      "         0.0684, -0.2118, -0.2270, -0.3085,  0.0089,  0.0707,  0.0167,  0.1333,\n",
      "         0.2670, -0.2512,  0.2147, -0.0136,  0.0576,  0.1872,  0.0218,  0.0373,\n",
      "        -0.1640, -0.0257, -0.0981,  0.2474, -0.0730, -0.1977,  0.1996, -0.0886,\n",
      "        -0.1592,  0.1166, -0.0163, -0.0371, -0.2985, -0.3182,  0.0859,  0.0861,\n",
      "         0.3254,  0.0663, -0.1516,  0.0071, -0.0329,  0.0797,  0.1166, -0.0105])\n",
      "Gradient for layer1.0.norm1.bias:\n",
      "tensor([ 5.8762e-02,  2.4749e-01,  2.6687e-02, -5.7421e-02, -8.2667e-02,\n",
      "        -7.1540e-02,  2.4287e-02, -9.6221e-02, -9.8518e-02,  1.7671e-01,\n",
      "        -1.1508e-01,  1.0714e-01, -4.6322e-02,  1.1365e-01, -6.5814e-02,\n",
      "        -1.9949e-02,  2.3073e-01, -9.2228e-02,  3.1155e-02, -1.4228e-01,\n",
      "         1.0184e-01,  6.8049e-02, -1.2647e-01, -1.7966e-01,  1.5898e-01,\n",
      "        -1.3109e-01, -1.1602e-01, -5.4788e-03, -2.4761e-02, -6.9750e-03,\n",
      "         5.9127e-02,  3.3880e-02,  1.9066e-01, -1.1949e-01,  2.4698e-01,\n",
      "        -1.8276e-01, -7.9594e-02,  1.3357e-01,  5.6294e-03, -9.3665e-02,\n",
      "         3.3475e-02, -6.5363e-03,  1.2060e-01,  1.6433e-01, -1.7918e-01,\n",
      "        -8.4340e-02,  5.2234e-02, -2.1827e-01, -1.1829e-01,  5.1182e-02,\n",
      "         7.8230e-03, -6.4412e-02, -2.3212e-01, -1.4658e-01,  1.1989e-01,\n",
      "         6.7729e-02,  1.3633e-01,  1.3597e-01, -2.1869e-01,  1.8754e-01,\n",
      "        -1.3486e-01,  1.2996e-01,  1.4181e-04,  1.1920e-01])\n",
      "Gradient for layer1.0.conv2.weight:\n",
      "tensor([[[[ 1.8671e-01, -7.2904e-02,  9.8392e-03],\n",
      "          [-2.0836e-01,  2.8449e-01, -2.4873e-01],\n",
      "          [-2.2259e-02,  3.7404e-01,  7.0343e-02]],\n",
      "\n",
      "         [[-3.6186e-01,  1.9204e-01, -2.0775e-01],\n",
      "          [ 2.3416e-02, -1.2302e-01, -4.3010e-02],\n",
      "          [ 1.6203e-01, -2.2621e-01,  3.3966e-01]],\n",
      "\n",
      "         [[-3.2705e-01, -1.4540e-01, -1.1430e-01],\n",
      "          [ 7.3662e-02, -3.7885e-01,  1.6664e-01],\n",
      "          [-1.5303e-02, -2.6849e-02, -3.4233e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.2522e-02, -5.7232e-01, -4.2081e-02],\n",
      "          [ 2.4985e-02, -7.8496e-02,  3.3090e-01],\n",
      "          [-4.3405e-04,  1.8582e-02,  1.2180e-01]],\n",
      "\n",
      "         [[-1.9248e-02, -7.7263e-01, -4.1850e-01],\n",
      "          [-1.7816e-01, -2.5299e-01, -2.8861e-01],\n",
      "          [ 1.3850e-01, -5.5105e-01, -2.7380e-01]],\n",
      "\n",
      "         [[ 8.4320e-02, -3.2248e-01, -4.7481e-01],\n",
      "          [ 5.4083e-01,  2.2741e-01, -4.0630e-01],\n",
      "          [ 5.6416e-02,  2.0764e-01, -1.3859e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0489e-01, -1.1098e-01,  1.0722e-01],\n",
      "          [ 1.7342e-01,  1.5993e-01, -3.3794e-01],\n",
      "          [-3.6958e-02, -1.8555e-01,  3.4435e-01]],\n",
      "\n",
      "         [[ 1.1465e-02, -8.9575e-02, -1.4955e-01],\n",
      "          [-6.6287e-03, -4.6352e-02, -8.6256e-02],\n",
      "          [ 1.0186e-01,  1.7110e-01,  2.1634e-01]],\n",
      "\n",
      "         [[ 9.4217e-03,  6.0170e-02,  3.2759e-01],\n",
      "          [-1.3930e-02,  8.7044e-02, -2.2333e-01],\n",
      "          [-4.1663e-01, -1.2308e-01,  2.3225e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.5655e-02, -2.3119e-01,  4.2942e-02],\n",
      "          [-3.0039e-01, -3.8863e-02,  1.9616e-01],\n",
      "          [-8.2222e-01, -7.4159e-01, -5.1553e-02]],\n",
      "\n",
      "         [[-6.4756e-01, -1.4293e-01, -1.4814e-01],\n",
      "          [ 3.8525e-01, -1.9028e-01,  1.0203e-01],\n",
      "          [-3.0854e-01,  1.9093e-01, -3.8884e-01]],\n",
      "\n",
      "         [[ 6.7533e-01,  2.6375e-01, -2.5309e-01],\n",
      "          [ 3.6569e-01, -7.9303e-02,  8.6382e-02],\n",
      "          [ 2.7414e-01,  2.4553e-01,  1.5693e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.0437e-02, -2.5123e-01,  5.3217e-01],\n",
      "          [-1.0558e-01, -3.6306e-01,  2.5774e-01],\n",
      "          [ 5.5456e-01, -7.5817e-02, -2.2296e-01]],\n",
      "\n",
      "         [[ 8.8809e-02, -5.1914e-01,  3.1158e-01],\n",
      "          [ 4.1549e-01, -4.6370e-02,  4.0469e-02],\n",
      "          [ 1.0315e-01,  2.7892e-01, -1.6208e-01]],\n",
      "\n",
      "         [[-5.3471e-01, -2.1000e-01,  5.7041e-02],\n",
      "          [-5.6787e-01,  4.0151e-01, -3.9303e-02],\n",
      "          [-1.4870e-01, -1.4413e-01,  3.6656e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.0960e-01, -1.1725e-01,  3.8388e-02],\n",
      "          [-2.2574e-01, -5.4634e-02, -4.2098e-01],\n",
      "          [ 8.6205e-02,  1.8215e-01, -2.4722e-02]],\n",
      "\n",
      "         [[ 6.3451e-02, -3.5985e-01,  4.3776e-02],\n",
      "          [-1.7392e-01,  3.8246e-01,  4.5872e-01],\n",
      "          [-2.2459e-02, -1.7683e-01,  3.4605e-01]],\n",
      "\n",
      "         [[ 4.4985e-02, -3.6815e-02, -1.4187e-01],\n",
      "          [ 1.2090e-01,  1.9822e-01,  2.3194e-02],\n",
      "          [ 9.8353e-02, -4.0942e-02, -6.7077e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.1884e-01, -1.2578e-01, -1.7045e-01],\n",
      "          [-1.4022e-02, -2.6339e-01, -9.3362e-02],\n",
      "          [ 3.4305e-01, -7.1371e-02, -1.5874e-01]],\n",
      "\n",
      "         [[-1.4715e-01, -1.7021e-01, -6.2995e-02],\n",
      "          [-2.9915e-01,  1.0333e-01, -4.1078e-01],\n",
      "          [ 3.3082e-01,  2.1417e-01,  3.6908e-01]],\n",
      "\n",
      "         [[ 2.3548e-01,  5.1871e-01, -8.9010e-03],\n",
      "          [ 2.4440e-01, -2.0640e-01,  2.8170e-01],\n",
      "          [ 7.7059e-02, -2.4880e-01,  2.9105e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6017e-01, -7.8211e-03,  3.1559e-01],\n",
      "          [ 9.6467e-02, -6.2230e-02, -5.8096e-02],\n",
      "          [-2.5990e-01,  1.8443e-01, -1.6421e-01]],\n",
      "\n",
      "         [[-1.9178e-01, -1.2119e-01, -6.4331e-02],\n",
      "          [ 1.3073e-01,  2.0136e-01,  6.6074e-01],\n",
      "          [ 7.5281e-02,  4.9986e-02, -2.6322e-02]],\n",
      "\n",
      "         [[-2.2625e-01, -4.1647e-01,  6.0114e-02],\n",
      "          [ 6.4644e-01, -1.8636e-01,  2.4457e-01],\n",
      "          [ 1.1075e-01, -4.0868e-03,  6.1995e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0398e-01,  5.3201e-01,  7.6633e-02],\n",
      "          [-4.1285e-01,  4.6816e-02,  1.9264e-01],\n",
      "          [-1.6915e-01,  2.6383e-01, -1.2353e-01]],\n",
      "\n",
      "         [[-8.2978e-02, -1.2606e-01, -7.6255e-02],\n",
      "          [-5.3289e-01,  2.0992e-01, -2.7871e-01],\n",
      "          [-4.2949e-01,  1.4098e-01,  4.3254e-01]],\n",
      "\n",
      "         [[ 1.1692e-01,  2.6019e-01,  2.1071e-01],\n",
      "          [ 1.4944e-01,  1.4332e-01, -1.5441e-01],\n",
      "          [ 6.1782e-01,  1.7472e-01, -3.3404e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7613e-01,  5.8433e-02, -2.6191e-01],\n",
      "          [ 7.6710e-02, -1.2524e-01,  3.5418e-01],\n",
      "          [-3.2846e-01, -2.2125e-02,  5.5473e-01]],\n",
      "\n",
      "         [[ 3.9391e-01,  2.5091e-01, -1.4509e-01],\n",
      "          [ 1.4046e-01,  1.5911e-01, -3.0237e-01],\n",
      "          [-5.2655e-02, -3.6565e-02, -3.1482e-02]],\n",
      "\n",
      "         [[ 1.6082e-01,  6.5279e-01,  2.5310e-01],\n",
      "          [ 2.4648e-01, -1.6222e-01, -1.9638e-01],\n",
      "          [-3.6614e-01,  1.1979e-01, -1.1620e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3871e-01,  1.4027e-01, -5.7930e-01],\n",
      "          [ 3.8375e-01,  3.4798e-01,  2.6704e-01],\n",
      "          [ 1.7811e-01, -2.2407e-01,  1.9250e-01]],\n",
      "\n",
      "         [[-2.3567e-01, -4.2625e-01,  1.3847e-01],\n",
      "          [-3.4272e-01, -6.7835e-01,  4.2208e-01],\n",
      "          [ 1.9774e-01,  2.2141e-01, -4.6509e-02]],\n",
      "\n",
      "         [[ 9.4595e-02, -1.1464e-01,  6.6473e-01],\n",
      "          [-2.8492e-02,  1.3676e-01, -2.8524e-01],\n",
      "          [-4.0433e-01, -9.7569e-02,  3.2401e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6446e-01, -6.7211e-02,  2.9874e-01],\n",
      "          [ 2.8197e-01,  4.5414e-01,  5.4590e-02],\n",
      "          [ 8.5681e-02,  2.0772e-01, -5.0928e-02]],\n",
      "\n",
      "         [[-1.2781e-01, -6.3414e-01, -4.0236e-01],\n",
      "          [ 3.1654e-01, -3.9334e-01, -1.8963e-01],\n",
      "          [ 4.2661e-02,  4.8105e-01,  1.6911e-03]],\n",
      "\n",
      "         [[ 4.9172e-01,  2.3291e-01, -3.6873e-01],\n",
      "          [ 1.7582e-01, -1.2383e-01, -6.8236e-02],\n",
      "          [-3.1086e-01,  6.5073e-01, -2.4859e-01]]]])\n",
      "Gradient for layer1.0.norm2.weight:\n",
      "tensor([ 0.0500, -0.0616,  0.0097, -0.1173, -0.0556,  0.1408,  0.0688,  0.0453,\n",
      "         0.0337, -0.0698,  0.0768, -0.1325, -0.0242, -0.0026, -0.0358, -0.1512,\n",
      "        -0.0453, -0.1385, -0.0783,  0.1173,  0.0730, -0.1701,  0.0785,  0.1619,\n",
      "         0.1214, -0.0932, -0.1062,  0.1162, -0.0874,  0.1028, -0.1964,  0.1006,\n",
      "         0.0912, -0.0544,  0.1052, -0.2625,  0.1090, -0.0304,  0.1379, -0.0012,\n",
      "        -0.1246,  0.0262, -0.0530,  0.0210,  0.2274,  0.0136, -0.2390, -0.1537,\n",
      "        -0.0789,  0.0358,  0.1318, -0.1052,  0.0738,  0.0799, -0.1269,  0.0308,\n",
      "        -0.0376,  0.0710, -0.1230,  0.0734,  0.0810,  0.1191, -0.1824, -0.0065])\n",
      "Gradient for layer1.0.norm2.bias:\n",
      "tensor([-0.1662,  0.0160,  0.0359,  0.0893,  0.1582, -0.0414, -0.0244, -0.0286,\n",
      "        -0.0027, -0.0103, -0.0268, -0.1699,  0.0250,  0.1569,  0.0111,  0.0206,\n",
      "         0.0413,  0.0096,  0.0121, -0.0137, -0.0163, -0.0637,  0.2063,  0.1833,\n",
      "         0.0729,  0.1258, -0.1245, -0.0230, -0.1052, -0.0079, -0.0033,  0.0760,\n",
      "         0.1262, -0.0301,  0.0787, -0.2271,  0.1357,  0.0298,  0.1326, -0.1200,\n",
      "        -0.0670,  0.0378, -0.0595,  0.0223,  0.1418, -0.1139, -0.2160,  0.0103,\n",
      "         0.0159,  0.0197,  0.0608,  0.0813,  0.0843,  0.0593,  0.0014, -0.0074,\n",
      "         0.0385,  0.1239,  0.0525,  0.1122,  0.0623,  0.1145, -0.1277, -0.0297])\n",
      "Gradient for layer1.1.conv1.weight:\n",
      "tensor([[[[-0.2524,  0.1673, -0.4913],\n",
      "          [-0.2835,  0.0463,  0.2086],\n",
      "          [-0.1523, -0.1444, -0.0660]],\n",
      "\n",
      "         [[-0.2991, -0.0334, -0.3492],\n",
      "          [ 0.3892, -0.2212, -0.0760],\n",
      "          [ 0.1366, -0.0440, -0.3938]],\n",
      "\n",
      "         [[ 0.0441, -0.1706, -0.0169],\n",
      "          [-0.5413, -0.0472,  0.1665],\n",
      "          [ 0.1039, -0.0968, -0.1160]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4028,  0.0102, -0.0949],\n",
      "          [ 0.2121, -0.1044, -0.0391],\n",
      "          [ 0.0309,  0.0855,  0.1231]],\n",
      "\n",
      "         [[-0.3640, -0.4244, -0.2094],\n",
      "          [-0.0694, -0.1217, -0.2746],\n",
      "          [ 0.0998,  0.1282, -0.0807]],\n",
      "\n",
      "         [[-0.5579, -0.0354,  0.0044],\n",
      "          [ 0.4559,  0.0491,  0.1816],\n",
      "          [ 0.2507, -0.5560, -0.1661]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4900,  0.3503,  0.1638],\n",
      "          [-0.2101,  0.5145, -0.2306],\n",
      "          [-0.7337,  0.0456,  0.3872]],\n",
      "\n",
      "         [[-0.0625, -0.0507,  0.1545],\n",
      "          [ 0.1488,  0.3551,  0.2039],\n",
      "          [ 0.3393,  0.0403,  0.2316]],\n",
      "\n",
      "         [[ 0.5534,  0.0012, -0.0194],\n",
      "          [ 0.2041,  0.3442,  0.4094],\n",
      "          [ 0.1482,  0.1390, -0.1343]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1497,  0.0174, -0.0738],\n",
      "          [ 0.3145,  0.1516,  0.2823],\n",
      "          [-0.0155, -0.2091, -0.0701]],\n",
      "\n",
      "         [[ 0.0128,  0.4975, -0.0119],\n",
      "          [-0.4306,  0.0160,  0.3046],\n",
      "          [ 0.2818,  0.2287,  0.1578]],\n",
      "\n",
      "         [[ 0.0156, -0.0618, -0.2252],\n",
      "          [-0.0947,  0.2153,  0.2120],\n",
      "          [ 0.0740,  0.0854,  0.2981]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1000,  0.5040,  0.0333],\n",
      "          [-0.0386, -0.3692,  0.1112],\n",
      "          [ 0.1044, -0.4236,  0.0542]],\n",
      "\n",
      "         [[ 0.1988, -0.1448, -0.0640],\n",
      "          [-0.0210, -0.2805,  0.3384],\n",
      "          [-0.0406,  0.2596, -0.3710]],\n",
      "\n",
      "         [[-0.0617, -0.1167, -0.5589],\n",
      "          [ 0.2161,  0.2722, -0.1600],\n",
      "          [ 0.0653,  0.0205,  0.0215]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4321, -0.0395,  0.2661],\n",
      "          [ 0.2236, -0.0263,  0.0641],\n",
      "          [ 0.0785,  0.2183,  0.2684]],\n",
      "\n",
      "         [[ 0.0098,  0.4583,  0.1436],\n",
      "          [ 0.0455,  0.2819,  0.0976],\n",
      "          [ 0.1113,  0.1415,  0.0490]],\n",
      "\n",
      "         [[ 0.0485,  0.0377, -0.1760],\n",
      "          [ 0.0635, -0.0094,  0.0816],\n",
      "          [-0.2834, -0.0139, -0.1420]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2827, -0.2519, -0.1069],\n",
      "          [-0.1006,  0.0255,  0.1121],\n",
      "          [ 0.1429,  0.3742, -0.1327]],\n",
      "\n",
      "         [[-0.0926,  0.2055,  0.0146],\n",
      "          [-0.2647, -0.4341,  0.0230],\n",
      "          [-0.1123,  0.0450, -0.0583]],\n",
      "\n",
      "         [[-0.3970, -0.1458, -0.2030],\n",
      "          [-0.1611, -0.0472, -0.1030],\n",
      "          [-0.0021,  0.1734,  0.2876]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0109,  0.0982,  0.2116],\n",
      "          [-0.1105,  0.1931,  0.1125],\n",
      "          [ 0.0152,  0.2809, -0.0629]],\n",
      "\n",
      "         [[ 0.2818, -0.1911,  0.1633],\n",
      "          [-0.4505, -0.0201, -0.0679],\n",
      "          [ 0.1455,  0.3036,  0.1895]],\n",
      "\n",
      "         [[-0.0316, -0.0323, -0.0860],\n",
      "          [-0.2217, -0.1401,  0.1886],\n",
      "          [ 0.0995, -0.0538,  0.0090]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0842, -0.2105, -0.0047],\n",
      "          [ 0.2630,  0.1238,  0.1467],\n",
      "          [-0.3575,  0.2201,  0.2984]],\n",
      "\n",
      "         [[-0.4570,  0.1372, -0.1218],\n",
      "          [ 0.2994,  0.0290,  0.1810],\n",
      "          [ 0.1174, -0.1220,  0.4143]],\n",
      "\n",
      "         [[-0.0225,  0.1526,  0.1600],\n",
      "          [ 0.2055,  0.0457, -0.1781],\n",
      "          [-0.1231, -0.1022, -0.0797]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2897,  0.3704,  0.2862],\n",
      "          [-0.1981,  0.0853,  0.1345],\n",
      "          [ 0.2147, -0.1258,  0.0677]],\n",
      "\n",
      "         [[ 0.2297,  0.2696, -0.0864],\n",
      "          [ 0.3437,  0.0981,  0.1372],\n",
      "          [ 0.0491,  0.0421,  0.0128]],\n",
      "\n",
      "         [[-0.0321, -0.2539, -0.1856],\n",
      "          [ 0.2412,  0.0905, -0.1995],\n",
      "          [ 0.3381,  0.5908,  0.2352]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2892, -0.1943,  0.1315],\n",
      "          [ 0.1464, -0.0252, -0.0414],\n",
      "          [-0.1949,  0.0140, -0.2278]],\n",
      "\n",
      "         [[ 0.3412,  0.2063,  0.0166],\n",
      "          [ 0.1111, -0.3818,  0.2070],\n",
      "          [-0.2221, -0.1279,  0.0934]],\n",
      "\n",
      "         [[-0.2166, -0.0470,  0.3150],\n",
      "          [-0.0978,  0.1487, -0.0485],\n",
      "          [ 0.4020,  0.0218, -0.4687]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2363, -0.3126, -0.1061],\n",
      "          [ 0.0047,  0.1931,  0.0708],\n",
      "          [ 0.0741, -0.0694, -0.0039]],\n",
      "\n",
      "         [[ 0.3453,  0.0174,  0.0779],\n",
      "          [-0.1097,  0.2563, -0.1940],\n",
      "          [ 0.0095,  0.0200,  0.1609]],\n",
      "\n",
      "         [[ 0.0038, -0.0506, -0.2151],\n",
      "          [ 0.2733, -0.0064, -0.1086],\n",
      "          [ 0.1889, -0.1250,  0.0298]]]])\n",
      "Gradient for layer1.1.norm1.weight:\n",
      "tensor([-8.2276e-02,  1.2720e-01, -8.8022e-02,  8.1515e-02,  8.2965e-02,\n",
      "        -2.0922e-02, -3.8695e-02,  2.7965e-02, -5.1533e-02, -2.0469e-01,\n",
      "         1.5986e-01, -3.1314e-02, -4.9378e-02,  7.5021e-02, -6.7836e-02,\n",
      "        -1.5380e-01,  1.0962e-01, -2.0205e-02, -4.3812e-02,  1.5633e-01,\n",
      "         1.0348e-01,  1.4711e-01,  1.3710e-01, -7.5572e-02, -2.2690e-01,\n",
      "         2.5079e-02, -3.7835e-02,  5.7648e-02,  5.9759e-02, -3.2712e-02,\n",
      "         6.1649e-02, -2.0222e-01, -3.1753e-03, -1.0960e-01,  1.4943e-01,\n",
      "         6.4320e-03,  6.3868e-02, -1.9075e-01, -8.6949e-02, -1.6319e-04,\n",
      "        -6.3130e-02,  2.1834e-01, -5.3495e-03, -4.0984e-03,  1.2586e-01,\n",
      "        -6.4213e-02, -1.5106e-01,  6.7788e-02, -2.8437e-01, -7.4363e-02,\n",
      "        -2.6012e-02, -4.7262e-02,  8.8073e-02,  1.6823e-02,  1.0909e-02,\n",
      "        -4.7162e-02, -2.7567e-02,  1.2997e-02,  1.7276e-01,  1.4330e-01,\n",
      "        -1.4293e-02, -1.2549e-01,  2.0695e-01,  5.6867e-02])\n",
      "Gradient for layer1.1.norm1.bias:\n",
      "tensor([ 0.0010,  0.0331, -0.1063,  0.0264,  0.1140, -0.0228,  0.0113, -0.0101,\n",
      "        -0.0819, -0.2410,  0.0753,  0.0360,  0.0485,  0.0517, -0.1111, -0.0232,\n",
      "         0.1472, -0.0211, -0.0796,  0.0539,  0.2033,  0.1155,  0.0215, -0.0291,\n",
      "        -0.1775, -0.0673, -0.0920, -0.1177,  0.0530, -0.0662,  0.0204, -0.3343,\n",
      "         0.0137, -0.0134,  0.1798, -0.0194,  0.0306, -0.0750, -0.0097,  0.0637,\n",
      "         0.0109, -0.0049,  0.0216,  0.0342, -0.0258, -0.0759, -0.1049,  0.0883,\n",
      "        -0.0705,  0.0092, -0.0732,  0.0242,  0.0028,  0.0228, -0.0475, -0.0860,\n",
      "         0.0294, -0.0062,  0.0154,  0.0411,  0.1239, -0.0337,  0.1528,  0.0551])\n",
      "Gradient for layer1.1.conv2.weight:\n",
      "tensor([[[[ 1.1912e-01, -9.2476e-02, -8.7050e-02],\n",
      "          [ 1.0318e-01, -3.6440e-04, -4.0022e-01],\n",
      "          [ 2.7088e-01, -8.4081e-02, -2.1304e-01]],\n",
      "\n",
      "         [[-4.7925e-01,  1.4982e-01,  1.3611e-01],\n",
      "          [ 8.5844e-02,  2.6703e-01, -3.2677e-01],\n",
      "          [-3.1905e-02,  3.4534e-01, -1.6232e-01]],\n",
      "\n",
      "         [[ 1.9789e-02, -5.7672e-02, -3.2217e-01],\n",
      "          [-1.9217e-01, -7.6885e-02,  9.2981e-02],\n",
      "          [-5.1009e-02,  1.5490e-01,  3.0967e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.4794e-02, -2.4530e-01, -1.4290e-01],\n",
      "          [-1.6818e-01, -2.3513e-01, -2.7475e-01],\n",
      "          [ 2.6109e-01,  6.1506e-02,  1.6510e-01]],\n",
      "\n",
      "         [[ 1.7867e-02, -8.5395e-02, -2.1471e-01],\n",
      "          [-1.7646e-01, -2.9936e-01, -8.3848e-02],\n",
      "          [-1.5577e-01,  2.2514e-01, -8.3944e-02]],\n",
      "\n",
      "         [[ 4.9786e-03,  9.9892e-02, -2.6052e-01],\n",
      "          [ 1.8374e-03, -5.5451e-02,  1.2640e-01],\n",
      "          [-2.0525e-01, -2.4335e-01, -3.3894e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0350e-01,  3.9400e-01, -1.2276e-01],\n",
      "          [ 6.4137e-02,  7.5904e-02, -1.1236e-01],\n",
      "          [ 1.4750e-01,  5.0205e-02, -1.9488e-01]],\n",
      "\n",
      "         [[-1.2490e-01, -9.3549e-02,  1.7222e-01],\n",
      "          [ 9.6535e-02,  4.9959e-02,  1.0017e-01],\n",
      "          [ 1.9622e-02, -3.3216e-02, -1.7140e-01]],\n",
      "\n",
      "         [[ 1.3044e-01, -1.6545e-02,  6.2454e-02],\n",
      "          [ 2.5119e-01, -2.1378e-01, -1.9662e-01],\n",
      "          [ 1.2558e-01,  4.4212e-02,  1.1066e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9049e-01, -5.8061e-02,  1.0729e-01],\n",
      "          [-4.6713e-02,  2.7256e-02,  1.7859e-02],\n",
      "          [ 2.7887e-01,  3.0180e-02,  2.4581e-01]],\n",
      "\n",
      "         [[-2.7193e-02,  9.3769e-02, -9.6719e-03],\n",
      "          [ 8.9824e-02, -1.4707e-02, -3.6900e-01],\n",
      "          [-1.9420e-01, -2.3590e-01,  9.7664e-02]],\n",
      "\n",
      "         [[ 9.9328e-02, -6.2258e-02, -3.6257e-01],\n",
      "          [ 1.0394e-02, -9.9696e-02, -1.1481e-01],\n",
      "          [-2.5564e-02, -7.4850e-02, -2.0668e-01]]],\n",
      "\n",
      "\n",
      "        [[[-7.3231e-02,  2.8292e-01,  2.1890e-01],\n",
      "          [-8.8481e-02,  1.5398e-01, -8.3020e-02],\n",
      "          [-1.3524e-01, -1.8903e-02,  2.6657e-01]],\n",
      "\n",
      "         [[-1.2763e-01,  2.6576e-01, -2.0264e-01],\n",
      "          [-8.2365e-02,  7.9369e-02,  6.2757e-02],\n",
      "          [-5.6284e-02, -2.3482e-01,  2.7738e-02]],\n",
      "\n",
      "         [[ 3.4160e-02, -5.4777e-02,  7.6192e-02],\n",
      "          [-2.9775e-02,  2.3732e-01,  1.5597e-01],\n",
      "          [-1.1398e-01, -1.9909e-01,  1.9818e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9204e-01, -2.2446e-01,  7.1328e-02],\n",
      "          [ 1.0821e-01,  1.0723e-01, -4.6710e-02],\n",
      "          [ 2.3582e-02,  2.8588e-02, -2.1847e-01]],\n",
      "\n",
      "         [[-5.4707e-02,  7.8679e-02,  2.8084e-01],\n",
      "          [-1.8867e-02, -3.4152e-01,  3.7769e-01],\n",
      "          [ 3.5603e-02, -1.3611e-01,  8.2061e-02]],\n",
      "\n",
      "         [[ 7.7701e-02,  2.1020e-02,  3.1759e-01],\n",
      "          [-2.4357e-01, -4.2426e-03, -2.9682e-01],\n",
      "          [ 2.3067e-02,  4.1669e-01, -6.0413e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.8906e-01, -2.7118e-01, -2.6642e-01],\n",
      "          [-3.1221e-02,  3.6710e-01,  5.3356e-02],\n",
      "          [ 9.4243e-02, -3.3146e-01, -1.0954e-01]],\n",
      "\n",
      "         [[-2.7761e-01,  9.9274e-02, -1.2692e-02],\n",
      "          [ 1.0249e-01, -3.5344e-01, -2.5882e-01],\n",
      "          [ 9.4570e-02, -8.7282e-02,  2.1275e-01]],\n",
      "\n",
      "         [[-2.5814e-02, -5.9383e-02,  3.7320e-02],\n",
      "          [-1.1760e-01,  1.3738e-02, -3.3866e-01],\n",
      "          [ 8.0851e-02,  2.1463e-01, -2.9512e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.6891e-02,  1.5134e-02,  2.1538e-01],\n",
      "          [-3.3835e-02,  8.7858e-02,  1.5167e-01],\n",
      "          [-1.2563e-02,  2.0191e-01,  6.7247e-02]],\n",
      "\n",
      "         [[ 3.4162e-02, -1.4373e-01,  3.7555e-01],\n",
      "          [ 2.3911e-01,  1.4151e-01, -1.1304e-01],\n",
      "          [ 1.4284e-01, -6.4149e-02, -1.8689e-02]],\n",
      "\n",
      "         [[ 2.7783e-01,  2.5664e-01, -2.0568e-01],\n",
      "          [-2.4162e-01,  3.3793e-01,  1.2889e-01],\n",
      "          [-1.1116e-01,  6.2665e-02,  2.5294e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1674e-01,  2.9447e-01, -1.2786e-01],\n",
      "          [-2.9266e-01, -1.3970e-01, -2.1350e-01],\n",
      "          [ 1.9239e-01, -2.8655e-01,  7.9659e-02]],\n",
      "\n",
      "         [[ 1.1934e-01,  1.3646e-01, -1.0072e-01],\n",
      "          [ 4.6069e-02, -1.6419e-03, -1.2046e-01],\n",
      "          [ 1.6422e-01, -2.9299e-01, -2.4204e-01]],\n",
      "\n",
      "         [[ 7.0915e-02, -1.5470e-01, -1.4585e-01],\n",
      "          [ 1.5052e-01,  3.0872e-01, -1.9230e-01],\n",
      "          [ 2.1717e-01,  2.7779e-02,  2.7535e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5338e-01, -1.0499e-01, -1.1183e-01],\n",
      "          [-1.4362e-01,  1.9992e-01, -6.9640e-02],\n",
      "          [ 9.2069e-02, -3.3159e-01,  7.9006e-03]],\n",
      "\n",
      "         [[ 4.5648e-03,  1.6123e-02,  2.4382e-01],\n",
      "          [ 4.0762e-01, -5.3741e-02, -3.6050e-01],\n",
      "          [ 1.6016e-01, -1.1956e-01, -4.6207e-01]],\n",
      "\n",
      "         [[ 8.5976e-02, -2.3014e-01,  2.4370e-02],\n",
      "          [ 1.3397e-02,  2.2402e-01,  1.8534e-01],\n",
      "          [-1.1317e-01,  3.5524e-01,  8.9165e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2863e-02, -1.1248e-01,  2.1306e-01],\n",
      "          [ 1.3998e-01, -5.4448e-02,  4.2487e-02],\n",
      "          [-1.2875e-01,  9.1227e-02,  2.3894e-01]],\n",
      "\n",
      "         [[-1.3242e-01,  9.2336e-03,  1.1713e-01],\n",
      "          [ 7.5546e-02, -1.2491e-01, -4.1291e-02],\n",
      "          [-3.8729e-02, -3.3443e-01, -9.3277e-02]],\n",
      "\n",
      "         [[ 3.0403e-01,  6.8411e-02, -2.1809e-01],\n",
      "          [ 3.5828e-01,  1.2134e-01,  2.3133e-01],\n",
      "          [ 1.9506e-01, -7.5349e-02, -1.2093e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0109e-01,  3.6261e-01, -1.3252e-01],\n",
      "          [ 1.2694e-01,  6.7490e-02, -7.6884e-02],\n",
      "          [ 1.8653e-01, -4.3482e-02,  3.9272e-02]],\n",
      "\n",
      "         [[-8.3023e-03, -6.3534e-02, -2.0116e-01],\n",
      "          [-2.0020e-01, -8.5954e-02,  8.9641e-02],\n",
      "          [-5.7364e-02, -7.7525e-02, -1.0863e-01]],\n",
      "\n",
      "         [[-1.4972e-01,  4.3725e-01,  2.0461e-01],\n",
      "          [ 2.7345e-01, -9.1081e-02, -2.7877e-01],\n",
      "          [ 8.9781e-02,  5.0010e-02, -2.0393e-01]]]])\n",
      "Gradient for layer1.1.norm2.weight:\n",
      "tensor([-0.0946,  0.1202, -0.0195,  0.0034,  0.0128,  0.1029,  0.0709, -0.0429,\n",
      "         0.0544,  0.1337, -0.2097,  0.0141,  0.1187, -0.0446,  0.0448,  0.0188,\n",
      "        -0.0237,  0.1100, -0.0105, -0.0240, -0.1025, -0.1084,  0.0788,  0.0605,\n",
      "         0.0463, -0.0228, -0.1143, -0.2700, -0.0076, -0.0806,  0.0502, -0.1109,\n",
      "         0.0328, -0.1008,  0.0039,  0.0600,  0.1532,  0.1639,  0.0515,  0.0687,\n",
      "        -0.0522,  0.0574,  0.0135,  0.0562, -0.0455, -0.0522, -0.0772,  0.0048,\n",
      "        -0.0705, -0.0756, -0.0371, -0.0073, -0.0665,  0.0270, -0.1814,  0.0720,\n",
      "         0.0534, -0.2105,  0.0388, -0.1323,  0.1443,  0.0439, -0.0339, -0.0453])\n",
      "Gradient for layer1.1.norm2.bias:\n",
      "tensor([-0.0431, -0.0118,  0.0734,  0.0489,  0.0988, -0.0380, -0.0099,  0.0011,\n",
      "        -0.0886,  0.0826, -0.0251, -0.0826,  0.0348,  0.0024,  0.0062,  0.1023,\n",
      "         0.0279,  0.0115,  0.0660, -0.0218,  0.0078, -0.0182, -0.0148,  0.0598,\n",
      "         0.1331,  0.1054, -0.1449, -0.0397,  0.0414,  0.0063,  0.0533,  0.0087,\n",
      "         0.0677, -0.0637,  0.0522, -0.0707,  0.0651, -0.0253,  0.0589,  0.0626,\n",
      "         0.0263,  0.0496, -0.0251,  0.0527, -0.0317, -0.0734, -0.1211, -0.0535,\n",
      "         0.0122, -0.0252, -0.0157,  0.0265,  0.0132,  0.0525, -0.0885,  0.0483,\n",
      "         0.1595,  0.0062,  0.0032,  0.0048,  0.0834,  0.1536, -0.1355,  0.0293])\n",
      "Gradient for layer2.0.conv1.weight:\n",
      "tensor([[[[ 1.3415e-01,  4.7787e-02, -1.7848e-01],\n",
      "          [-1.2125e-01,  9.2319e-02,  2.5776e-01],\n",
      "          [ 8.4086e-03,  1.2957e-01, -1.7412e-01]],\n",
      "\n",
      "         [[ 2.6467e-01,  5.7843e-02, -1.1980e-02],\n",
      "          [ 2.5123e-02,  7.7349e-02, -9.4975e-02],\n",
      "          [-4.5624e-02,  1.2136e-02,  1.8691e-02]],\n",
      "\n",
      "         [[ 3.0403e-01,  1.7405e-01, -3.9744e-02],\n",
      "          [-7.3498e-02, -3.1916e-02, -3.1125e-01],\n",
      "          [-1.8591e-02,  1.0260e-01,  2.5487e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0196e-01, -5.2290e-02, -7.5066e-02],\n",
      "          [ 3.6691e-02, -3.2395e-02, -1.0519e-02],\n",
      "          [-2.4756e-02, -9.5234e-02, -1.5598e-01]],\n",
      "\n",
      "         [[ 3.4238e-01,  2.2391e-02,  2.1173e-03],\n",
      "          [-3.2078e-02,  1.5007e-01,  4.2890e-02],\n",
      "          [-1.3543e-01, -1.3638e-01, -1.7515e-01]],\n",
      "\n",
      "         [[-2.7737e-01, -4.1873e-02,  1.4813e-01],\n",
      "          [-4.5626e-02, -7.0034e-03, -4.2871e-02],\n",
      "          [ 1.2647e-01, -7.3493e-02,  1.8317e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.9920e-02, -1.9288e-01,  3.9449e-02],\n",
      "          [-2.7246e-02,  1.4470e-01, -4.0768e-02],\n",
      "          [-1.5813e-02, -4.9491e-02,  3.6883e-02]],\n",
      "\n",
      "         [[-4.5019e-02, -6.6020e-02, -4.1288e-02],\n",
      "          [-2.7607e-02, -1.1362e-01,  8.1102e-02],\n",
      "          [ 6.1795e-02,  1.5687e-01, -9.1346e-02]],\n",
      "\n",
      "         [[ 1.2331e-02, -2.9701e-02, -4.7057e-02],\n",
      "          [ 9.8687e-03,  3.0542e-02, -5.2491e-02],\n",
      "          [ 1.0329e-01,  1.6340e-01, -7.9309e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7038e-02,  6.7482e-03,  1.4548e-01],\n",
      "          [ 3.4971e-02,  1.2398e-01,  5.6521e-02],\n",
      "          [ 6.6201e-02,  6.5329e-02, -7.2410e-03]],\n",
      "\n",
      "         [[ 8.5590e-03, -9.5942e-02,  7.7073e-02],\n",
      "          [-9.9018e-02, -5.3877e-02,  4.4278e-03],\n",
      "          [-1.3688e-01,  1.9407e-01,  3.0399e-01]],\n",
      "\n",
      "         [[-8.3679e-03, -5.9399e-02, -4.4396e-02],\n",
      "          [ 2.7868e-02,  4.5441e-02, -7.0571e-02],\n",
      "          [ 9.6970e-02, -1.0524e-01, -1.5431e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2451e-02,  2.6510e-01,  1.5248e-01],\n",
      "          [-3.1069e-03, -4.4395e-02,  3.5766e-02],\n",
      "          [ 9.9192e-02,  3.5523e-02,  1.0749e-01]],\n",
      "\n",
      "         [[ 1.1875e-01,  2.3108e-02,  5.3222e-02],\n",
      "          [ 2.4298e-01,  2.2001e-01,  1.9184e-01],\n",
      "          [ 1.6434e-01,  2.4357e-01,  1.2662e-01]],\n",
      "\n",
      "         [[ 1.9835e-01,  7.9336e-03,  2.6018e-01],\n",
      "          [-5.4940e-02, -2.3916e-02,  2.1491e-02],\n",
      "          [ 7.4648e-03, -1.1897e-01, -1.8068e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.4499e-02, -1.1369e-01,  1.2322e-01],\n",
      "          [ 7.0247e-02,  2.3070e-02,  9.7089e-02],\n",
      "          [ 1.5526e-01,  4.9616e-02,  1.1126e-02]],\n",
      "\n",
      "         [[-1.7801e-02,  7.4325e-02,  1.4805e-01],\n",
      "          [ 1.9384e-01,  9.4403e-03, -9.2429e-02],\n",
      "          [-3.3727e-02,  2.4780e-01,  2.9143e-01]],\n",
      "\n",
      "         [[ 9.9397e-02, -3.4453e-02,  2.6105e-02],\n",
      "          [-2.0624e-01, -3.1502e-02,  1.2086e-01],\n",
      "          [-8.1428e-02,  6.0769e-02,  7.6298e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.9243e-01,  7.2645e-02, -4.2005e-02],\n",
      "          [ 1.6904e-01, -1.0481e-01,  4.3898e-02],\n",
      "          [ 2.0239e-01,  6.0242e-02, -2.6653e-01]],\n",
      "\n",
      "         [[ 4.3229e-02,  1.2374e-01,  4.3183e-02],\n",
      "          [ 6.8343e-02, -6.8230e-02, -9.5809e-02],\n",
      "          [-8.2085e-02, -8.8807e-02, -4.8274e-02]],\n",
      "\n",
      "         [[ 3.9423e-02,  1.9995e-01,  5.5811e-02],\n",
      "          [-3.5152e-02,  6.0399e-02, -2.2756e-01],\n",
      "          [ 1.8635e-01,  9.6341e-02, -2.8509e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1375e-01, -1.2262e-01, -7.7158e-02],\n",
      "          [ 5.3219e-02, -1.3836e-01,  1.3706e-02],\n",
      "          [ 5.3173e-02, -2.9247e-01, -3.3385e-02]],\n",
      "\n",
      "         [[ 2.5343e-01,  3.0678e-02,  3.7223e-02],\n",
      "          [-1.2936e-02,  2.4560e-01,  9.7810e-03],\n",
      "          [ 1.1634e-01, -1.9207e-02,  1.0254e-01]],\n",
      "\n",
      "         [[-3.3816e-03,  6.0951e-04, -1.4624e-01],\n",
      "          [-1.6652e-01,  1.7275e-01,  6.3973e-02],\n",
      "          [-1.7205e-01,  1.8676e-01,  8.4361e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4784e-02,  2.2072e-01,  1.8379e-01],\n",
      "          [ 2.1747e-02,  1.5960e-01, -6.5017e-02],\n",
      "          [-2.0246e-01, -1.8500e-01, -2.3631e-01]],\n",
      "\n",
      "         [[ 1.0233e-01,  2.7605e-01,  2.0995e-01],\n",
      "          [-1.5313e-01, -1.4076e-02,  5.0629e-02],\n",
      "          [-1.6680e-01, -8.7863e-03,  8.0906e-02]],\n",
      "\n",
      "         [[ 1.8312e-01, -1.0475e-01,  1.4106e-01],\n",
      "          [ 1.9109e-01,  3.6861e-02,  7.6499e-02],\n",
      "          [ 1.7680e-01,  7.7362e-02,  1.4579e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2026e-02, -2.3275e-02,  4.7614e-03],\n",
      "          [-6.4613e-02, -8.7666e-04,  1.9394e-02],\n",
      "          [-1.0929e-01, -9.6580e-02, -2.6149e-02]],\n",
      "\n",
      "         [[ 5.0749e-01,  1.6795e-01,  9.2261e-02],\n",
      "          [ 1.4844e-01,  6.3199e-02,  3.4277e-02],\n",
      "          [-5.7817e-02, -5.1817e-02,  2.5097e-01]],\n",
      "\n",
      "         [[-2.0601e-02,  2.0564e-01,  1.7021e-01],\n",
      "          [ 2.1607e-01,  2.3300e-01,  1.0885e-01],\n",
      "          [ 1.2024e-01,  2.0949e-01, -5.9715e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.0127e-02,  7.9772e-02,  1.8424e-01],\n",
      "          [-5.9735e-02,  1.2586e-01, -9.8273e-02],\n",
      "          [-1.1768e-01, -7.1609e-02,  9.0089e-02]],\n",
      "\n",
      "         [[ 1.7054e-02, -1.0085e-01,  1.0644e-01],\n",
      "          [-1.3246e-01,  9.5757e-03,  1.1868e-01],\n",
      "          [ 2.9253e-02,  1.6614e-01,  9.9357e-02]],\n",
      "\n",
      "         [[-1.0730e-01, -1.2187e-01,  1.0623e-01],\n",
      "          [ 8.9345e-02,  1.5397e-02,  3.3216e-02],\n",
      "          [ 7.5539e-02, -2.1575e-01,  1.2043e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.8681e-04,  1.1524e-01,  3.6858e-02],\n",
      "          [ 2.4331e-01, -2.1274e-02, -8.0311e-02],\n",
      "          [ 4.5439e-02,  8.8160e-02, -2.6639e-02]],\n",
      "\n",
      "         [[-1.4509e-02, -1.5525e-01,  9.0194e-02],\n",
      "          [ 4.2002e-02, -1.6767e-01,  7.6349e-03],\n",
      "          [ 6.2211e-02,  2.5918e-02,  5.0670e-02]],\n",
      "\n",
      "         [[ 2.9302e-01,  2.1443e-01,  7.1266e-02],\n",
      "          [-4.1357e-02, -2.4388e-02,  5.8596e-02],\n",
      "          [ 1.3200e-01,  9.6040e-02,  5.1333e-02]]]])\n",
      "Gradient for layer2.0.norm1.weight:\n",
      "tensor([-0.0152, -0.0731,  0.0233, -0.0271,  0.0763, -0.0628,  0.0484, -0.0148,\n",
      "        -0.0593, -0.0296, -0.0567, -0.1115, -0.0581,  0.0380, -0.0297,  0.0586,\n",
      "         0.0072, -0.0674,  0.0136,  0.0741, -0.0336,  0.0899,  0.0221,  0.0712,\n",
      "        -0.0004, -0.0214,  0.0195, -0.0697, -0.0425,  0.1334, -0.0213, -0.0288,\n",
      "        -0.0117, -0.0275,  0.0096,  0.0300, -0.1083,  0.0675,  0.1190, -0.1159,\n",
      "        -0.0214, -0.0693, -0.0593, -0.0035,  0.1528,  0.0778,  0.0292,  0.0620,\n",
      "        -0.0685, -0.0052,  0.0593,  0.0053,  0.0433, -0.0306,  0.0380,  0.0261,\n",
      "         0.0175, -0.0033,  0.0156,  0.0697,  0.0265,  0.0029, -0.0273,  0.0955,\n",
      "         0.0837, -0.0383, -0.0076, -0.0060, -0.0237,  0.0727,  0.0769, -0.0472,\n",
      "         0.0188, -0.0936, -0.0851,  0.0211,  0.1511,  0.0978,  0.0431, -0.0015,\n",
      "        -0.0432, -0.0752, -0.0686, -0.0640, -0.0611,  0.0437, -0.0527,  0.0964,\n",
      "         0.0705, -0.0209,  0.1119, -0.0467,  0.0391, -0.0704, -0.0555,  0.0318,\n",
      "        -0.0204, -0.0239, -0.0236, -0.0241, -0.0385,  0.0050,  0.0373, -0.1710,\n",
      "        -0.0440, -0.0003,  0.0145,  0.0102,  0.0472, -0.0375, -0.0501, -0.0052,\n",
      "         0.0299, -0.0060, -0.0106,  0.0830,  0.0309, -0.0176, -0.0098,  0.0314,\n",
      "         0.0040, -0.0258,  0.0072,  0.0363, -0.1246, -0.0702,  0.0303, -0.0094])\n",
      "Gradient for layer2.0.norm1.bias:\n",
      "tensor([ 0.0146, -0.0563, -0.0023, -0.0285,  0.0474, -0.0765,  0.0433,  0.0638,\n",
      "        -0.0506, -0.0387, -0.1033, -0.1018, -0.0296,  0.0063, -0.0455,  0.0535,\n",
      "        -0.0268, -0.0256,  0.0868,  0.0674,  0.0270,  0.0506, -0.0265,  0.1148,\n",
      "         0.0059,  0.0163, -0.0454, -0.0521, -0.0185,  0.0762, -0.0496, -0.0455,\n",
      "        -0.0413,  0.0341,  0.0236, -0.0338, -0.0353,  0.0417,  0.0461, -0.0360,\n",
      "        -0.0454, -0.0763, -0.0850, -0.0326,  0.0738,  0.0511,  0.0781,  0.0205,\n",
      "        -0.0458, -0.0116,  0.0490,  0.0723,  0.0458, -0.0087, -0.0509,  0.0280,\n",
      "        -0.0238, -0.0383,  0.0915,  0.0815,  0.0115, -0.0181, -0.0402,  0.0035,\n",
      "        -0.0019,  0.0050, -0.0441,  0.0182, -0.0384,  0.0060,  0.0548, -0.0456,\n",
      "        -0.0401, -0.0858, -0.0667,  0.0050,  0.0436,  0.0252,  0.0449,  0.0013,\n",
      "         0.0146, -0.0259, -0.0150, -0.0505, -0.0704,  0.0406,  0.0258,  0.0347,\n",
      "         0.0187,  0.0053,  0.0905, -0.0718,  0.0153, -0.0629, -0.0520,  0.0389,\n",
      "        -0.0189, -0.0446,  0.0322, -0.0409, -0.0327,  0.0447,  0.0114, -0.0306,\n",
      "        -0.0152, -0.0858, -0.0325,  0.0494, -0.0668, -0.0122, -0.0271, -0.0068,\n",
      "         0.0565,  0.0365,  0.0282,  0.0336, -0.0022, -0.0294, -0.0318, -0.0484,\n",
      "        -0.0136,  0.0288, -0.0234,  0.0187, -0.0891, -0.0523,  0.0445, -0.0676])\n",
      "Gradient for layer2.0.conv2.weight:\n",
      "tensor([[[[ 0.1529, -0.2620, -0.1949],\n",
      "          [-0.2222,  0.1661, -0.0116],\n",
      "          [-0.2332, -0.3202,  0.1043]],\n",
      "\n",
      "         [[-0.0995,  0.0742, -0.0037],\n",
      "          [ 0.0960,  0.1267, -0.0560],\n",
      "          [ 0.0488, -0.2331,  0.1633]],\n",
      "\n",
      "         [[-0.0742, -0.1047, -0.0409],\n",
      "          [ 0.1456, -0.0220,  0.0479],\n",
      "          [-0.1173,  0.1438, -0.2196]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0735, -0.0658, -0.1007],\n",
      "          [ 0.0270,  0.0261,  0.1826],\n",
      "          [ 0.0373,  0.1029,  0.0173]],\n",
      "\n",
      "         [[-0.0069,  0.1756,  0.2973],\n",
      "          [-0.0484, -0.2597, -0.2431],\n",
      "          [ 0.0568,  0.1431, -0.0024]],\n",
      "\n",
      "         [[ 0.0279,  0.0194, -0.1113],\n",
      "          [-0.0380,  0.0671, -0.0412],\n",
      "          [-0.1615, -0.0415,  0.1722]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0011,  0.1091, -0.0092],\n",
      "          [-0.0315,  0.0062, -0.1523],\n",
      "          [ 0.1662, -0.0603, -0.0724]],\n",
      "\n",
      "         [[-0.0076, -0.1326, -0.0006],\n",
      "          [ 0.0204, -0.1454, -0.0403],\n",
      "          [ 0.0146,  0.1206,  0.1099]],\n",
      "\n",
      "         [[ 0.0789, -0.0541, -0.1018],\n",
      "          [-0.1061,  0.1590, -0.0760],\n",
      "          [-0.0207,  0.1001,  0.1475]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0958, -0.0791,  0.0948],\n",
      "          [-0.0639, -0.0833,  0.1254],\n",
      "          [ 0.0303,  0.0916,  0.1175]],\n",
      "\n",
      "         [[ 0.0495, -0.0547,  0.0725],\n",
      "          [ 0.1255, -0.0689,  0.0304],\n",
      "          [ 0.0244,  0.1523,  0.0892]],\n",
      "\n",
      "         [[ 0.1292,  0.1231,  0.1460],\n",
      "          [-0.0044,  0.0966, -0.0461],\n",
      "          [ 0.0990,  0.1364, -0.0380]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1601,  0.2232,  0.1645],\n",
      "          [ 0.1919, -0.0346,  0.1075],\n",
      "          [ 0.1344, -0.1134,  0.2303]],\n",
      "\n",
      "         [[ 0.1238,  0.0006,  0.0167],\n",
      "          [-0.0320,  0.1574, -0.1112],\n",
      "          [-0.0349,  0.0819, -0.0164]],\n",
      "\n",
      "         [[ 0.0206, -0.1140,  0.2135],\n",
      "          [-0.1101, -0.0645, -0.0491],\n",
      "          [-0.0106,  0.0634, -0.1033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0991, -0.0781,  0.0333],\n",
      "          [-0.0693,  0.0908, -0.0876],\n",
      "          [-0.1801, -0.0915, -0.1452]],\n",
      "\n",
      "         [[ 0.0368, -0.0432,  0.0803],\n",
      "          [-0.1757,  0.1053, -0.1119],\n",
      "          [-0.0719, -0.0804, -0.0663]],\n",
      "\n",
      "         [[ 0.1898, -0.0147, -0.0655],\n",
      "          [-0.0920,  0.0467,  0.0895],\n",
      "          [ 0.1224,  0.0688,  0.1152]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1171, -0.0323, -0.0066],\n",
      "          [ 0.1166,  0.0271, -0.0963],\n",
      "          [-0.0576, -0.1901,  0.1717]],\n",
      "\n",
      "         [[-0.0898, -0.1437,  0.0282],\n",
      "          [ 0.0697,  0.0114, -0.0167],\n",
      "          [-0.1170, -0.0909, -0.0407]],\n",
      "\n",
      "         [[-0.1205,  0.0200, -0.0244],\n",
      "          [ 0.0177,  0.0161, -0.1222],\n",
      "          [ 0.0297, -0.1701,  0.0369]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0222, -0.0242, -0.0333],\n",
      "          [ 0.0811,  0.0180, -0.1551],\n",
      "          [-0.0008, -0.2086, -0.0236]],\n",
      "\n",
      "         [[ 0.0103,  0.0089,  0.0335],\n",
      "          [ 0.0067, -0.0478,  0.0844],\n",
      "          [-0.0784, -0.0377,  0.1191]],\n",
      "\n",
      "         [[-0.0081,  0.0296, -0.0976],\n",
      "          [-0.0900, -0.1084,  0.0348],\n",
      "          [ 0.0674, -0.1052, -0.0416]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0376, -0.0203,  0.2330],\n",
      "          [ 0.0925,  0.0718, -0.0285],\n",
      "          [-0.1811, -0.1558,  0.1855]],\n",
      "\n",
      "         [[ 0.0599, -0.1337,  0.1164],\n",
      "          [-0.0567, -0.0776,  0.1565],\n",
      "          [-0.1641, -0.0542,  0.3472]],\n",
      "\n",
      "         [[-0.0280, -0.0118,  0.0535],\n",
      "          [ 0.0059, -0.0602,  0.1010],\n",
      "          [-0.1138, -0.1113, -0.0938]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0181,  0.0076,  0.0993],\n",
      "          [ 0.1206,  0.0817,  0.0228],\n",
      "          [ 0.1747, -0.1433, -0.0978]],\n",
      "\n",
      "         [[-0.0775,  0.2955,  0.2088],\n",
      "          [-0.2286, -0.1090, -0.0881],\n",
      "          [-0.1961, -0.0587,  0.0120]],\n",
      "\n",
      "         [[ 0.0753, -0.0062,  0.1314],\n",
      "          [ 0.0725,  0.0304, -0.2016],\n",
      "          [ 0.1967,  0.1259,  0.0704]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1304, -0.0531, -0.0329],\n",
      "          [-0.0372,  0.1403, -0.0322],\n",
      "          [-0.0392, -0.0288,  0.2168]],\n",
      "\n",
      "         [[ 0.0510,  0.0809,  0.1743],\n",
      "          [ 0.0246, -0.1013, -0.1383],\n",
      "          [ 0.0087,  0.0107,  0.0363]],\n",
      "\n",
      "         [[-0.0437, -0.0614,  0.0767],\n",
      "          [-0.0241, -0.0291, -0.0858],\n",
      "          [-0.1263, -0.0066, -0.0856]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0958,  0.0322,  0.0291],\n",
      "          [ 0.0346, -0.0040,  0.1201],\n",
      "          [-0.0276, -0.0466, -0.0982]],\n",
      "\n",
      "         [[ 0.0169,  0.1383, -0.0848],\n",
      "          [-0.0468, -0.1440,  0.0111],\n",
      "          [ 0.0633,  0.0133, -0.0205]],\n",
      "\n",
      "         [[-0.1853, -0.1254,  0.1402],\n",
      "          [-0.0225,  0.0719,  0.0599],\n",
      "          [ 0.1668,  0.1774, -0.0087]]]])\n",
      "Gradient for layer2.0.norm2.weight:\n",
      "tensor([-3.8032e-02,  5.0107e-02,  4.2623e-02, -2.8866e-02,  4.5022e-02,\n",
      "         7.8132e-03, -2.4212e-02, -2.7775e-02, -4.1483e-03, -6.1146e-02,\n",
      "        -5.9451e-03,  4.3344e-02,  5.3778e-02,  2.6852e-02,  2.5018e-02,\n",
      "         3.4544e-02, -4.7258e-02,  3.6260e-02,  1.5612e-02, -9.8075e-02,\n",
      "        -7.2943e-03,  1.0378e-01,  9.7534e-02,  3.2932e-03, -2.0296e-02,\n",
      "        -9.7736e-02, -1.5036e-02, -6.0999e-02, -3.8844e-02, -4.8328e-02,\n",
      "         7.5631e-03, -2.2352e-02, -6.8068e-02,  7.6228e-02, -2.9595e-02,\n",
      "         3.4009e-02,  3.6494e-02, -4.3866e-03, -2.4681e-02,  9.6708e-03,\n",
      "        -9.7766e-02, -3.5305e-02,  9.4625e-02,  1.8609e-02,  6.4670e-02,\n",
      "        -5.4716e-02, -1.4061e-03,  1.3802e-02, -7.4762e-02, -2.2565e-02,\n",
      "         2.5492e-02,  1.9266e-02,  3.9510e-02,  2.9677e-02, -2.3395e-02,\n",
      "         3.3461e-02,  3.0236e-02,  1.4706e-02,  2.8049e-02,  7.2899e-03,\n",
      "         5.9973e-03, -7.9470e-02, -2.3904e-02, -5.8539e-02,  5.3655e-02,\n",
      "        -1.3745e-02, -8.6261e-02,  3.7398e-02,  4.1881e-02, -5.2764e-02,\n",
      "        -1.9892e-02,  2.5952e-02, -1.7011e-02,  3.4167e-02,  3.7314e-03,\n",
      "         1.4760e-02,  2.8566e-02, -3.6911e-02, -4.7477e-02,  1.2750e-02,\n",
      "         2.5199e-03,  8.8337e-02,  4.8197e-02, -4.5499e-02, -1.9809e-02,\n",
      "        -9.2797e-02, -3.4264e-02,  4.7415e-02, -1.3056e-01, -3.8279e-02,\n",
      "         3.6647e-02, -2.6179e-02,  8.6792e-03, -1.7192e-02,  1.1288e-02,\n",
      "        -1.0269e-01,  5.1904e-02, -5.3470e-02, -1.3207e-02, -1.0619e-01,\n",
      "        -3.1253e-02, -9.9033e-02, -3.6674e-03,  3.4815e-02, -7.8253e-04,\n",
      "        -2.8140e-02,  4.7736e-02, -1.1663e-04,  1.3710e-01,  1.0702e-02,\n",
      "        -4.2577e-02,  8.0988e-02,  9.9792e-02,  7.5185e-02, -1.4191e-03,\n",
      "         1.2645e-02,  5.0512e-02,  5.8947e-02,  1.9162e-02, -2.0871e-02,\n",
      "         8.2726e-03, -9.1717e-02,  3.4259e-02, -5.4776e-02,  1.3608e-02,\n",
      "         4.8336e-02,  6.6784e-03,  1.5645e-01])\n",
      "Gradient for layer2.0.norm2.bias:\n",
      "tensor([-0.0397,  0.0239,  0.0569,  0.0065,  0.0578,  0.0993, -0.0159,  0.0142,\n",
      "         0.0226, -0.0240, -0.0303,  0.0188, -0.0177, -0.0067,  0.0477,  0.0087,\n",
      "        -0.0470,  0.0169, -0.0005, -0.0617, -0.0656,  0.0917, -0.0156, -0.0249,\n",
      "         0.0793, -0.0503,  0.0176, -0.0778,  0.0014, -0.0075,  0.0054,  0.0261,\n",
      "         0.0185,  0.0652, -0.0179, -0.0127,  0.0334,  0.0742,  0.0097,  0.0195,\n",
      "        -0.0432, -0.0541,  0.0369, -0.0350,  0.0663,  0.0122,  0.0074, -0.0059,\n",
      "        -0.1041,  0.0411, -0.0162,  0.0197,  0.0295,  0.0691, -0.0966, -0.0123,\n",
      "         0.0210,  0.0267,  0.0272,  0.0026, -0.0398, -0.0609, -0.0069,  0.0674,\n",
      "         0.0631,  0.0381,  0.0280,  0.0540,  0.0450, -0.0929,  0.0007, -0.0077,\n",
      "         0.0352, -0.0128, -0.0290,  0.0496,  0.0260, -0.0593, -0.0123,  0.0094,\n",
      "        -0.0065,  0.0192,  0.0317, -0.0478,  0.0110, -0.0523,  0.0093, -0.0106,\n",
      "        -0.0571,  0.0084, -0.0572,  0.0174, -0.0035, -0.0256, -0.0648, -0.0244,\n",
      "         0.0270, -0.0039, -0.0025, -0.0705,  0.0250, -0.0270, -0.0119,  0.0389,\n",
      "         0.0231, -0.0200, -0.0124,  0.0305,  0.0810, -0.0339,  0.0374,  0.0664,\n",
      "         0.1391,  0.0089, -0.0433,  0.0972,  0.0289,  0.0632,  0.0725,  0.0115,\n",
      "         0.0212, -0.0488,  0.0043, -0.0342,  0.0205, -0.0136,  0.0668,  0.0764])\n",
      "Gradient for layer2.0.shortcut.0.weight:\n",
      "tensor([[[[ 0.0450]],\n",
      "\n",
      "         [[-0.1034]],\n",
      "\n",
      "         [[-0.1492]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[ 0.0507]],\n",
      "\n",
      "         [[ 0.1398]]],\n",
      "\n",
      "\n",
      "        [[[-0.0662]],\n",
      "\n",
      "         [[ 0.1006]],\n",
      "\n",
      "         [[-0.1503]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0246]],\n",
      "\n",
      "         [[ 0.1825]],\n",
      "\n",
      "         [[-0.0406]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0246]],\n",
      "\n",
      "         [[ 0.0702]],\n",
      "\n",
      "         [[-0.0722]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1950]],\n",
      "\n",
      "         [[ 0.0678]],\n",
      "\n",
      "         [[-0.1456]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0918]],\n",
      "\n",
      "         [[-0.0866]],\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0613]],\n",
      "\n",
      "         [[-0.1160]],\n",
      "\n",
      "         [[-0.1593]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1868]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[ 0.0718]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1821]],\n",
      "\n",
      "         [[ 0.1478]],\n",
      "\n",
      "         [[ 0.1841]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0282]],\n",
      "\n",
      "         [[ 0.0221]],\n",
      "\n",
      "         [[ 0.0776]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0573]],\n",
      "\n",
      "         [[ 0.0426]],\n",
      "\n",
      "         [[ 0.0704]]]])\n",
      "Gradient for layer2.0.shortcut.1.weight:\n",
      "tensor([-0.0453, -0.0531, -0.0006, -0.0783,  0.1123,  0.1394,  0.0034,  0.0254,\n",
      "         0.0313,  0.0225,  0.0182,  0.0035, -0.0330, -0.0437,  0.0398, -0.0314,\n",
      "        -0.1237, -0.0569,  0.0425, -0.0610, -0.1102,  0.0168,  0.0136, -0.0952,\n",
      "         0.1115,  0.0548,  0.0543, -0.0258,  0.0060,  0.0864, -0.0218,  0.0653,\n",
      "         0.1109,  0.0098,  0.0394, -0.1020,  0.0239,  0.1087,  0.0277, -0.0295,\n",
      "         0.0192, -0.1227,  0.0332, -0.0761,  0.0032,  0.1012,  0.0177, -0.0415,\n",
      "        -0.0396,  0.0290, -0.0646,  0.0008,  0.0477, -0.0195, -0.1004, -0.0910,\n",
      "         0.0216, -0.0350, -0.0286, -0.0161, -0.0268, -0.0423,  0.0664,  0.1396,\n",
      "        -0.0167,  0.0476,  0.0350,  0.0452, -0.0161, -0.0096,  0.0303, -0.0398,\n",
      "         0.0382, -0.0110, -0.0280, -0.0022,  0.0148,  0.0102,  0.0656, -0.0121,\n",
      "         0.0037, -0.1107, -0.0324,  0.0021, -0.0084, -0.0102, -0.0141, -0.0499,\n",
      "        -0.0360, -0.0074, -0.0953, -0.0094, -0.0484,  0.0336, -0.0758, -0.0049,\n",
      "        -0.0301, -0.0505,  0.0324, -0.0547,  0.0121, -0.0008, -0.0124, -0.0063,\n",
      "         0.1039,  0.0262, -0.1329,  0.0089, -0.0351, -0.0720,  0.0769, -0.0111,\n",
      "         0.0533,  0.0241, -0.0499,  0.0578, -0.0226,  0.0440,  0.0911, -0.0587,\n",
      "        -0.0152, -0.0348, -0.0507, -0.0425,  0.0268, -0.0775,  0.0652, -0.0336])\n",
      "Gradient for layer2.0.shortcut.1.bias:\n",
      "tensor([-0.0397,  0.0239,  0.0569,  0.0065,  0.0578,  0.0993, -0.0159,  0.0142,\n",
      "         0.0226, -0.0240, -0.0303,  0.0188, -0.0177, -0.0067,  0.0477,  0.0087,\n",
      "        -0.0470,  0.0169, -0.0005, -0.0617, -0.0656,  0.0917, -0.0156, -0.0249,\n",
      "         0.0793, -0.0503,  0.0176, -0.0778,  0.0014, -0.0075,  0.0054,  0.0261,\n",
      "         0.0185,  0.0652, -0.0179, -0.0127,  0.0334,  0.0742,  0.0097,  0.0195,\n",
      "        -0.0432, -0.0541,  0.0369, -0.0350,  0.0663,  0.0122,  0.0074, -0.0059,\n",
      "        -0.1041,  0.0411, -0.0162,  0.0197,  0.0295,  0.0691, -0.0966, -0.0123,\n",
      "         0.0210,  0.0267,  0.0272,  0.0026, -0.0398, -0.0609, -0.0069,  0.0674,\n",
      "         0.0631,  0.0381,  0.0280,  0.0540,  0.0450, -0.0929,  0.0007, -0.0077,\n",
      "         0.0352, -0.0128, -0.0290,  0.0496,  0.0260, -0.0593, -0.0123,  0.0094,\n",
      "        -0.0065,  0.0192,  0.0317, -0.0478,  0.0110, -0.0523,  0.0093, -0.0106,\n",
      "        -0.0571,  0.0084, -0.0572,  0.0174, -0.0035, -0.0256, -0.0648, -0.0244,\n",
      "         0.0270, -0.0039, -0.0025, -0.0705,  0.0250, -0.0270, -0.0119,  0.0389,\n",
      "         0.0231, -0.0200, -0.0124,  0.0305,  0.0810, -0.0339,  0.0374,  0.0664,\n",
      "         0.1391,  0.0089, -0.0433,  0.0972,  0.0289,  0.0632,  0.0725,  0.0115,\n",
      "         0.0212, -0.0488,  0.0043, -0.0342,  0.0205, -0.0136,  0.0668,  0.0764])\n",
      "Gradient for layer2.1.conv1.weight:\n",
      "tensor([[[[ 1.0920e-01, -1.1738e-01, -8.8725e-02],\n",
      "          [ 1.9627e-01, -3.6231e-02,  4.7803e-02],\n",
      "          [-9.0101e-02,  5.2128e-02,  9.4671e-02]],\n",
      "\n",
      "         [[-9.5831e-02,  1.6967e-01, -3.4405e-03],\n",
      "          [ 1.3073e-02,  4.8100e-02,  1.6770e-01],\n",
      "          [-5.5449e-03, -2.5701e-01,  6.8933e-02]],\n",
      "\n",
      "         [[-1.4257e-01,  5.6093e-02,  4.5892e-02],\n",
      "          [-6.1979e-02, -8.9936e-02,  1.9120e-01],\n",
      "          [ 3.0693e-02,  2.9279e-02, -1.5836e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.8194e-02,  3.3071e-02, -2.4419e-02],\n",
      "          [-1.3846e-01, -9.4117e-03,  1.5229e-01],\n",
      "          [-2.0349e-02, -6.7629e-02, -1.8318e-02]],\n",
      "\n",
      "         [[-1.0322e-01, -1.1255e-01,  1.4131e-01],\n",
      "          [-9.6937e-02, -1.1384e-02,  7.8029e-02],\n",
      "          [ 1.8863e-02, -4.8975e-02, -1.2823e-01]],\n",
      "\n",
      "         [[-7.4507e-02, -6.2937e-02,  7.6218e-02],\n",
      "          [-9.9446e-03, -1.4617e-01, -2.1408e-02],\n",
      "          [ 1.5104e-02, -1.5373e-01,  1.0875e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4321e-04, -6.8363e-02, -1.0630e-01],\n",
      "          [ 5.0124e-02,  7.1953e-03, -1.8426e-01],\n",
      "          [-1.7179e-02, -3.2250e-02, -2.3843e-02]],\n",
      "\n",
      "         [[-5.3837e-02,  1.5760e-01, -1.2455e-01],\n",
      "          [ 3.1711e-02, -1.6538e-02, -1.1503e-01],\n",
      "          [-1.5398e-01,  6.8788e-02, -6.7587e-02]],\n",
      "\n",
      "         [[ 3.3583e-02, -1.2940e-01, -1.6231e-01],\n",
      "          [ 1.2570e-01,  5.7456e-03, -1.2490e-01],\n",
      "          [-2.2509e-01,  7.7874e-02,  4.9995e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.1505e-02,  9.5671e-04,  1.5170e-01],\n",
      "          [ 7.8442e-02, -1.1112e-01,  8.5709e-02],\n",
      "          [-5.3703e-02, -1.3431e-01, -6.3792e-02]],\n",
      "\n",
      "         [[-4.1068e-02,  2.7419e-02, -1.5150e-01],\n",
      "          [-2.1053e-01,  1.1018e-02, -3.1622e-02],\n",
      "          [-3.2383e-02, -2.0223e-01,  2.2632e-01]],\n",
      "\n",
      "         [[-2.2611e-01, -3.0205e-02, -1.1602e-01],\n",
      "          [-2.4247e-02,  5.8462e-02, -7.0587e-02],\n",
      "          [ 9.9706e-02,  3.1581e-02,  6.8663e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5030e-02,  1.0731e-01,  6.5289e-02],\n",
      "          [-1.0220e-01,  4.4750e-02, -8.6266e-02],\n",
      "          [ 8.9101e-02, -1.4447e-01,  2.4685e-02]],\n",
      "\n",
      "         [[-1.2811e-02,  1.2332e-01, -5.4256e-02],\n",
      "          [ 1.7512e-01, -1.7833e-01,  2.0475e-02],\n",
      "          [ 7.9389e-02, -1.4417e-01, -7.8336e-02]],\n",
      "\n",
      "         [[ 1.0292e-01,  1.2671e-02, -2.3135e-02],\n",
      "          [-5.0490e-02,  1.0247e-01,  1.2380e-02],\n",
      "          [ 2.0784e-02, -1.4550e-02,  1.0726e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6155e-02,  1.9807e-02,  3.8302e-02],\n",
      "          [-8.3025e-02,  8.7241e-02, -1.1651e-01],\n",
      "          [ 6.1492e-02, -1.9935e-02, -1.1584e-01]],\n",
      "\n",
      "         [[ 9.6778e-02,  1.9437e-02,  5.0799e-02],\n",
      "          [-8.7122e-02,  1.9648e-01, -1.2956e-01],\n",
      "          [ 3.1519e-02,  3.7771e-02,  2.2447e-02]],\n",
      "\n",
      "         [[-3.8405e-02,  2.7444e-02, -3.8454e-02],\n",
      "          [-4.1944e-02,  8.5848e-02,  8.5831e-02],\n",
      "          [-4.0251e-02,  7.1297e-02, -3.7540e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.3684e-02, -1.1929e-01, -8.1384e-03],\n",
      "          [ 1.2901e-01, -5.1424e-02, -1.4070e-02],\n",
      "          [-8.1520e-02, -1.1495e-01,  3.1752e-02]],\n",
      "\n",
      "         [[ 2.3571e-02, -3.2334e-02,  9.4159e-02],\n",
      "          [-8.6354e-02, -2.2390e-02, -1.2338e-01],\n",
      "          [-2.8354e-02, -1.5516e-01, -8.7876e-03]],\n",
      "\n",
      "         [[-3.8063e-02, -1.1380e-01, -6.4385e-02],\n",
      "          [-3.9253e-02,  6.4086e-02,  1.6790e-01],\n",
      "          [-3.0563e-02,  1.5332e-02,  5.8308e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.8627e-02, -6.9656e-02, -6.9747e-02],\n",
      "          [ 3.3915e-02,  2.8186e-03, -3.3208e-02],\n",
      "          [ 1.3329e-01,  6.3165e-02, -1.4447e-01]],\n",
      "\n",
      "         [[-1.0317e-02, -6.5995e-02, -7.7160e-02],\n",
      "          [ 7.4139e-02, -7.1715e-02, -3.0049e-02],\n",
      "          [ 7.2922e-02,  2.8341e-02,  4.0702e-02]],\n",
      "\n",
      "         [[ 7.6578e-02, -1.3912e-01, -2.0192e-01],\n",
      "          [ 3.0965e-02,  9.1012e-02,  6.4239e-02],\n",
      "          [ 6.1630e-02, -4.4693e-02, -2.4262e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.8702e-02, -9.3582e-02, -2.2876e-02],\n",
      "          [ 5.6818e-02, -5.4346e-02, -1.5875e-01],\n",
      "          [ 5.5711e-02, -3.5696e-02, -9.6522e-02]],\n",
      "\n",
      "         [[ 1.7166e-01,  6.7227e-02, -4.6160e-02],\n",
      "          [ 1.6356e-01, -4.0636e-03, -6.2901e-03],\n",
      "          [-1.7920e-01,  9.7875e-02,  8.9948e-02]],\n",
      "\n",
      "         [[-7.7032e-02, -1.8673e-01, -4.5950e-02],\n",
      "          [-3.7640e-02,  8.7000e-02, -1.1387e-01],\n",
      "          [ 7.1466e-02, -1.5579e-01, -5.3968e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7951e-02, -3.5644e-02, -1.2693e-01],\n",
      "          [-9.8125e-02, -4.6473e-02, -1.0657e-01],\n",
      "          [-1.2540e-01, -1.6626e-01, -1.3547e-02]],\n",
      "\n",
      "         [[-7.8662e-02, -1.2979e-01, -1.8463e-01],\n",
      "          [ 2.9285e-02, -6.5546e-02, -9.6175e-02],\n",
      "          [ 2.4652e-02, -1.1925e-01, -1.3015e-01]],\n",
      "\n",
      "         [[ 1.1107e-03,  7.4361e-02, -1.9400e-01],\n",
      "          [ 9.1535e-02, -3.3765e-02,  2.5884e-02],\n",
      "          [-5.1185e-02,  1.7964e-04, -2.1215e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7535e-02,  1.1116e-01,  6.3820e-02],\n",
      "          [ 9.2352e-02,  3.9924e-02,  4.3630e-02],\n",
      "          [-7.8254e-02,  7.8897e-02, -5.3417e-02]],\n",
      "\n",
      "         [[ 5.4055e-02, -2.7961e-02,  1.1423e-01],\n",
      "          [-6.7947e-02, -1.4784e-01,  5.4937e-02],\n",
      "          [ 1.4325e-01,  3.7032e-02, -4.1276e-02]],\n",
      "\n",
      "         [[-7.0221e-02,  1.0810e-01, -3.6477e-02],\n",
      "          [ 6.3078e-02,  6.1310e-02, -5.4024e-02],\n",
      "          [ 8.1813e-02,  1.1509e-01,  5.3895e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8685e-02, -1.3081e-02,  5.8490e-02],\n",
      "          [-9.1422e-02,  8.2220e-02, -7.3424e-02],\n",
      "          [ 6.6383e-02,  3.5675e-02,  4.5850e-03]],\n",
      "\n",
      "         [[ 1.1798e-02,  9.2797e-02, -9.1622e-02],\n",
      "          [ 1.3052e-02,  1.1727e-01,  3.6538e-03],\n",
      "          [ 1.4865e-02,  1.4902e-01, -4.7381e-02]],\n",
      "\n",
      "         [[-1.1370e-01, -1.2301e-02,  3.9242e-02],\n",
      "          [-7.6580e-02,  2.0077e-01,  1.0227e-01],\n",
      "          [ 1.1676e-02, -2.8134e-02,  1.2284e-01]]]])\n",
      "Gradient for layer2.1.norm1.weight:\n",
      "tensor([-0.0522, -0.0187,  0.0190, -0.0197,  0.0617,  0.0512, -0.0345, -0.0142,\n",
      "        -0.0198, -0.0283,  0.1105, -0.0357, -0.0179, -0.0209, -0.0575, -0.0414,\n",
      "         0.0334, -0.1036,  0.0417,  0.0375, -0.0148, -0.0424, -0.0312, -0.0157,\n",
      "        -0.0947,  0.0087,  0.0416,  0.0594, -0.0254, -0.0016,  0.0419, -0.0371,\n",
      "        -0.0075,  0.0246, -0.0332, -0.0106, -0.0337,  0.1142, -0.0700,  0.1190,\n",
      "        -0.0372, -0.0309,  0.0272,  0.0026, -0.0047,  0.0350,  0.0070,  0.0106,\n",
      "         0.0365, -0.0394, -0.0296, -0.0390,  0.0170,  0.0251, -0.0230, -0.0338,\n",
      "        -0.0332,  0.0301, -0.0118,  0.0481,  0.0239,  0.0770, -0.0138, -0.0206,\n",
      "         0.0086, -0.0373, -0.0487, -0.0019,  0.0287, -0.0564,  0.0210,  0.0662,\n",
      "        -0.0545,  0.0362,  0.0302, -0.0237, -0.0106, -0.0096, -0.0103,  0.0439,\n",
      "        -0.0075, -0.0305, -0.0186, -0.0173,  0.0021, -0.0025, -0.0085,  0.0247,\n",
      "         0.1027, -0.0965,  0.0324,  0.0221, -0.0383,  0.0201,  0.0209,  0.0459,\n",
      "        -0.0117,  0.1023, -0.0117, -0.0727,  0.0442,  0.0078, -0.0475, -0.0301,\n",
      "        -0.0029, -0.0504,  0.0206,  0.0242,  0.0166, -0.0123, -0.0788,  0.1393,\n",
      "         0.0323, -0.0230,  0.0300, -0.0271,  0.0192, -0.0730,  0.0051, -0.0773,\n",
      "         0.0125,  0.0824,  0.0216,  0.0812, -0.0162, -0.0028,  0.0468, -0.0871])\n",
      "Gradient for layer2.1.norm1.bias:\n",
      "tensor([-0.0118, -0.0124,  0.0092,  0.0318,  0.0068, -0.0287, -0.0030, -0.0026,\n",
      "        -0.0252, -0.0116,  0.0834,  0.0313,  0.0464,  0.0242, -0.0438,  0.0237,\n",
      "         0.0332, -0.0756,  0.0012,  0.0486, -0.0301, -0.0618, -0.0166, -0.0290,\n",
      "        -0.0545,  0.0459,  0.0227,  0.0366,  0.0074, -0.0060,  0.0766, -0.0180,\n",
      "        -0.0707, -0.0036, -0.0307, -0.0393, -0.0856,  0.0535, -0.0674,  0.0051,\n",
      "         0.0030,  0.0172, -0.0229, -0.0159, -0.0440,  0.0433, -0.0401, -0.0174,\n",
      "         0.0440,  0.0073,  0.0047, -0.0347,  0.0218,  0.0501,  0.0210, -0.0391,\n",
      "        -0.0539, -0.0152, -0.0291,  0.0443,  0.0219, -0.0231, -0.0097, -0.0008,\n",
      "        -0.0427,  0.0071, -0.0477, -0.0115,  0.0609, -0.0477, -0.0048,  0.0067,\n",
      "        -0.0042, -0.0076,  0.0809, -0.0269, -0.0370, -0.0280, -0.0226,  0.0224,\n",
      "        -0.0212, -0.0559,  0.0265, -0.0467,  0.0551, -0.0440, -0.0014, -0.0072,\n",
      "         0.1076, -0.0480,  0.0480,  0.0214, -0.0537,  0.0290, -0.0296,  0.0238,\n",
      "        -0.0362,  0.0466,  0.0114, -0.0224, -0.0003,  0.0167,  0.0046, -0.0261,\n",
      "         0.0158,  0.0025,  0.0459,  0.0284,  0.0777,  0.0292, -0.0373,  0.0763,\n",
      "        -0.0043, -0.0166,  0.0267, -0.0686, -0.0054, -0.0242,  0.0150, -0.0504,\n",
      "         0.0255,  0.0570,  0.0286,  0.0381,  0.0136, -0.0011, -0.0004, -0.0451])\n",
      "Gradient for layer2.1.conv2.weight:\n",
      "tensor([[[[ 0.0374,  0.0239,  0.1300],\n",
      "          [-0.0174, -0.1093, -0.0879],\n",
      "          [ 0.0519,  0.1038,  0.1274]],\n",
      "\n",
      "         [[ 0.0119, -0.0213,  0.0399],\n",
      "          [-0.0269, -0.0166, -0.0655],\n",
      "          [-0.0338, -0.1094,  0.0462]],\n",
      "\n",
      "         [[ 0.0170,  0.1565, -0.1186],\n",
      "          [-0.1975,  0.0420,  0.0269],\n",
      "          [ 0.0834, -0.0314,  0.0655]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0548,  0.0944, -0.0474],\n",
      "          [ 0.0560, -0.0136,  0.0728],\n",
      "          [-0.0859,  0.1134,  0.0138]],\n",
      "\n",
      "         [[ 0.0532,  0.0983,  0.1916],\n",
      "          [-0.0284,  0.0259,  0.1521],\n",
      "          [ 0.0258, -0.0918, -0.0554]],\n",
      "\n",
      "         [[-0.0758,  0.0258,  0.1068],\n",
      "          [ 0.1287,  0.1207,  0.0016],\n",
      "          [ 0.0656,  0.0286, -0.0195]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0788, -0.1027, -0.0535],\n",
      "          [ 0.1017,  0.1371,  0.0762],\n",
      "          [ 0.0309, -0.0459, -0.0075]],\n",
      "\n",
      "         [[ 0.0317,  0.0216, -0.0092],\n",
      "          [-0.0400,  0.1835,  0.0715],\n",
      "          [ 0.0482, -0.0116, -0.0704]],\n",
      "\n",
      "         [[-0.0982, -0.0557, -0.0215],\n",
      "          [ 0.1581, -0.0063, -0.0025],\n",
      "          [-0.0009, -0.1687,  0.0708]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1755,  0.0203,  0.0652],\n",
      "          [-0.2689, -0.0965, -0.0056],\n",
      "          [ 0.0598,  0.2436,  0.2282]],\n",
      "\n",
      "         [[ 0.0569, -0.0005, -0.1489],\n",
      "          [-0.0175, -0.0122,  0.0411],\n",
      "          [-0.0120,  0.1567,  0.0186]],\n",
      "\n",
      "         [[ 0.1110, -0.0597, -0.0527],\n",
      "          [-0.0416, -0.1716, -0.0763],\n",
      "          [-0.0550,  0.1495,  0.0472]]],\n",
      "\n",
      "\n",
      "        [[[-0.0968,  0.0170, -0.0082],\n",
      "          [-0.0457,  0.0183,  0.0445],\n",
      "          [-0.0748, -0.0599, -0.1271]],\n",
      "\n",
      "         [[-0.0767,  0.1366, -0.1169],\n",
      "          [-0.1028,  0.0811, -0.0421],\n",
      "          [-0.0393, -0.0314,  0.1231]],\n",
      "\n",
      "         [[-0.0844, -0.0083,  0.0315],\n",
      "          [-0.0707,  0.0104, -0.0124],\n",
      "          [-0.0672, -0.0838,  0.0116]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0322, -0.0044, -0.0452],\n",
      "          [-0.1070, -0.0598,  0.0530],\n",
      "          [-0.0077,  0.0099, -0.0065]],\n",
      "\n",
      "         [[ 0.1341,  0.0138, -0.0308],\n",
      "          [ 0.0622,  0.2197,  0.0535],\n",
      "          [-0.0165, -0.1378, -0.1632]],\n",
      "\n",
      "         [[ 0.0803,  0.1561,  0.0404],\n",
      "          [ 0.0620, -0.0522, -0.0789],\n",
      "          [-0.1002,  0.0377,  0.0485]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0049, -0.0711, -0.0592],\n",
      "          [-0.0911, -0.0341,  0.0631],\n",
      "          [ 0.1122, -0.1418, -0.0530]],\n",
      "\n",
      "         [[ 0.1003, -0.0534,  0.0471],\n",
      "          [ 0.0141, -0.0907,  0.0007],\n",
      "          [-0.0607, -0.0207, -0.0052]],\n",
      "\n",
      "         [[ 0.0124,  0.0236,  0.0954],\n",
      "          [ 0.0494, -0.0270,  0.0117],\n",
      "          [ 0.0015,  0.0210,  0.1525]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0083, -0.0185, -0.1723],\n",
      "          [ 0.0883,  0.0134, -0.1110],\n",
      "          [-0.0049, -0.0133, -0.0801]],\n",
      "\n",
      "         [[ 0.0333, -0.1457, -0.1286],\n",
      "          [-0.0497, -0.0131,  0.0345],\n",
      "          [ 0.0894,  0.1240, -0.0735]],\n",
      "\n",
      "         [[-0.1066,  0.0328,  0.0175],\n",
      "          [ 0.0803,  0.0299, -0.1420],\n",
      "          [ 0.0706,  0.0571,  0.0086]]],\n",
      "\n",
      "\n",
      "        [[[-0.0086, -0.0805, -0.0099],\n",
      "          [-0.1204, -0.0930, -0.3321],\n",
      "          [-0.0868, -0.1418, -0.0286]],\n",
      "\n",
      "         [[ 0.0047,  0.0205,  0.0379],\n",
      "          [-0.1000,  0.0349,  0.0267],\n",
      "          [ 0.0120, -0.0106, -0.1506]],\n",
      "\n",
      "         [[ 0.0672,  0.0290, -0.0567],\n",
      "          [ 0.0218, -0.1666,  0.2130],\n",
      "          [-0.0032, -0.0831, -0.1147]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0673,  0.0576, -0.0478],\n",
      "          [-0.0209, -0.0136, -0.0307],\n",
      "          [-0.1027, -0.0242, -0.0606]],\n",
      "\n",
      "         [[ 0.1497, -0.0559,  0.1523],\n",
      "          [ 0.0992,  0.0573, -0.0875],\n",
      "          [-0.0467,  0.0573, -0.0043]],\n",
      "\n",
      "         [[ 0.0025,  0.0125, -0.0890],\n",
      "          [-0.0633, -0.1079,  0.0297],\n",
      "          [-0.0149, -0.0323, -0.0407]]],\n",
      "\n",
      "\n",
      "        [[[-0.0490, -0.0519, -0.0735],\n",
      "          [-0.0505,  0.0272,  0.0168],\n",
      "          [-0.0208, -0.1362, -0.0286]],\n",
      "\n",
      "         [[-0.0638, -0.1735,  0.0540],\n",
      "          [ 0.0177, -0.0460,  0.0674],\n",
      "          [-0.0451,  0.1882, -0.0277]],\n",
      "\n",
      "         [[-0.0551,  0.0532,  0.0375],\n",
      "          [ 0.1536, -0.1231,  0.0004],\n",
      "          [-0.0613, -0.0338,  0.1200]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0248,  0.0406, -0.0222],\n",
      "          [-0.0903, -0.0156, -0.1326],\n",
      "          [-0.0162, -0.0849,  0.0402]],\n",
      "\n",
      "         [[-0.0992, -0.0592,  0.0836],\n",
      "          [-0.0106, -0.0845, -0.0240],\n",
      "          [ 0.1205,  0.0203,  0.0644]],\n",
      "\n",
      "         [[-0.0583,  0.0389, -0.0049],\n",
      "          [-0.0289,  0.0355, -0.0431],\n",
      "          [-0.0216,  0.0237, -0.0900]]]])\n",
      "Gradient for layer2.1.norm2.weight:\n",
      "tensor([ 0.0220,  0.0297,  0.0092,  0.0187, -0.0433, -0.0650,  0.0167, -0.0190,\n",
      "        -0.0119,  0.0363,  0.0412,  0.0191,  0.0088, -0.0213,  0.0047,  0.0155,\n",
      "         0.0155, -0.0717, -0.0400,  0.0421,  0.0279, -0.0065,  0.0053,  0.0153,\n",
      "        -0.0005,  0.0122, -0.0140,  0.0037, -0.0004, -0.0448,  0.0205, -0.0041,\n",
      "        -0.0538,  0.0105,  0.0048,  0.0236,  0.0897,  0.0067, -0.0025, -0.0087,\n",
      "         0.0450,  0.0024, -0.0533, -0.0463, -0.0757,  0.0144, -0.0037, -0.0689,\n",
      "         0.0293, -0.0040,  0.0589, -0.0218, -0.0086, -0.0063, -0.0505, -0.0318,\n",
      "        -0.0177,  0.0408,  0.0041, -0.0812,  0.0638,  0.0411,  0.0707,  0.0146,\n",
      "        -0.0702,  0.0096,  0.0477, -0.0020,  0.0546, -0.0652,  0.0362,  0.0210,\n",
      "         0.0083, -0.0253, -0.0064, -0.0347, -0.0154, -0.0497, -0.0035,  0.0051,\n",
      "         0.0222, -0.0185,  0.0131,  0.0535,  0.0106,  0.0003, -0.0116, -0.0311,\n",
      "        -0.0051, -0.0048,  0.0250,  0.0431,  0.0430, -0.0245,  0.0516,  0.0292,\n",
      "         0.0455, -0.0328, -0.0347,  0.0558,  0.0936,  0.0091,  0.0207, -0.0862,\n",
      "         0.0165, -0.0568,  0.0698,  0.0396,  0.0226, -0.0373, -0.0019,  0.0080,\n",
      "        -0.0100,  0.0307,  0.0152,  0.0127,  0.0088, -0.0172,  0.0878,  0.0072,\n",
      "        -0.0054,  0.0627,  0.0256, -0.0064,  0.0043, -0.0149, -0.0663,  0.0243])\n",
      "Gradient for layer2.1.norm2.bias:\n",
      "tensor([-0.0399,  0.0082,  0.0060,  0.0175, -0.0096,  0.0067, -0.0116,  0.0271,\n",
      "        -0.0081,  0.0018,  0.0457,  0.0257, -0.0067, -0.0225,  0.0343,  0.0209,\n",
      "        -0.0106, -0.0157, -0.0167, -0.0155, -0.0434,  0.0175, -0.0084, -0.0006,\n",
      "         0.0183, -0.0134,  0.0374, -0.0144, -0.0373, -0.0334, -0.0416,  0.0605,\n",
      "        -0.0036,  0.0119,  0.0120,  0.0373,  0.0361,  0.0077,  0.0267, -0.0300,\n",
      "        -0.0148, -0.0184,  0.0098,  0.0022, -0.0002, -0.0435,  0.0084, -0.0017,\n",
      "        -0.0777,  0.0042,  0.0173, -0.0051,  0.0361, -0.0089,  0.0239,  0.0077,\n",
      "         0.0195,  0.0519, -0.0434, -0.0499,  0.0485, -0.0128,  0.0247,  0.0020,\n",
      "         0.0199,  0.0243,  0.0115,  0.0305,  0.0253, -0.0398,  0.0151,  0.0482,\n",
      "         0.0216, -0.0106, -0.0561, -0.0192,  0.0231, -0.0123,  0.0113,  0.0179,\n",
      "        -0.0040, -0.0005,  0.0172, -0.0168, -0.0362, -0.0139, -0.0092,  0.0067,\n",
      "        -0.0298,  0.0071, -0.0141,  0.0058, -0.0025, -0.0149, -0.0082, -0.0251,\n",
      "        -0.0025, -0.0179, -0.0092,  0.0182, -0.0130, -0.0228, -0.0097, -0.0297,\n",
      "         0.0085, -0.0244, -0.0121, -0.0102,  0.0042, -0.0315, -0.0146,  0.0558,\n",
      "         0.0082, -0.0186,  0.0326, -0.0147, -0.0149,  0.0203,  0.0232, -0.0277,\n",
      "        -0.0132,  0.0388,  0.0139, -0.0129, -0.0087, -0.0471, -0.0168,  0.0269])\n",
      "Gradient for layer3.0.conv1.weight:\n",
      "tensor([[[[-1.1423e-01, -4.1064e-05, -6.1734e-02],\n",
      "          [ 6.6066e-02, -5.9351e-02,  7.3015e-02],\n",
      "          [-7.8011e-02,  7.4601e-02, -1.3191e-01]],\n",
      "\n",
      "         [[-1.1819e-01,  3.1587e-02, -2.6014e-02],\n",
      "          [-3.3810e-02, -4.2565e-02, -2.5123e-02],\n",
      "          [ 4.0233e-02, -7.9836e-02,  5.3449e-03]],\n",
      "\n",
      "         [[ 1.3927e-02,  3.1774e-03, -1.6786e-01],\n",
      "          [ 1.3109e-02, -7.0607e-02, -5.9663e-02],\n",
      "          [ 1.3353e-01,  8.1918e-02, -7.0759e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2202e-01, -5.4702e-02, -4.5646e-02],\n",
      "          [-2.1961e-02, -5.2543e-02, -6.6067e-03],\n",
      "          [-9.4596e-02, -4.7727e-02,  2.0088e-02]],\n",
      "\n",
      "         [[-9.2020e-02, -3.1624e-02,  2.1121e-02],\n",
      "          [ 1.7424e-02, -2.4164e-02,  1.8585e-02],\n",
      "          [-2.4235e-02, -5.5138e-02, -4.6112e-02]],\n",
      "\n",
      "         [[-5.2081e-02, -9.8042e-02, -6.2397e-02],\n",
      "          [-9.2828e-02, -4.3562e-02, -1.8989e-02],\n",
      "          [-1.0242e-02, -6.5945e-03,  3.2006e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.2918e-03,  2.9061e-02,  1.3339e-02],\n",
      "          [ 2.4526e-02, -2.8127e-02, -1.6477e-02],\n",
      "          [-3.1802e-02,  1.2898e-02, -1.5793e-02]],\n",
      "\n",
      "         [[-4.7388e-02, -4.0653e-02,  8.3090e-03],\n",
      "          [-8.0743e-02, -1.9198e-02, -2.5534e-02],\n",
      "          [-1.4496e-02,  7.3681e-02, -3.3059e-02]],\n",
      "\n",
      "         [[ 6.3724e-02,  2.7469e-02, -5.8510e-02],\n",
      "          [-6.2038e-02,  1.6214e-02, -2.8287e-02],\n",
      "          [-2.2905e-02,  1.7269e-03,  2.7776e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7003e-02,  2.9238e-02,  2.9678e-02],\n",
      "          [ 5.5141e-02, -1.0748e-02, -4.3466e-02],\n",
      "          [ 5.6500e-02,  4.9650e-03, -8.7750e-02]],\n",
      "\n",
      "         [[-1.4313e-02, -1.0196e-02, -2.0687e-02],\n",
      "          [-4.3782e-02, -2.0838e-02,  5.8887e-02],\n",
      "          [ 8.8835e-02,  4.1296e-02, -1.4083e-02]],\n",
      "\n",
      "         [[ 5.9679e-02,  1.0640e-02, -1.0017e-02],\n",
      "          [-6.7445e-02,  2.9938e-02, -4.7791e-02],\n",
      "          [ 3.4182e-02, -5.3870e-02, -2.5530e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6239e-02,  1.7119e-02, -2.0424e-02],\n",
      "          [-9.6217e-02, -1.7139e-02, -9.2237e-03],\n",
      "          [-5.2490e-02,  4.5715e-02,  9.7211e-03]],\n",
      "\n",
      "         [[-1.9399e-02, -3.1359e-05, -2.2819e-02],\n",
      "          [ 4.5447e-02,  3.5252e-02,  9.6021e-02],\n",
      "          [-8.5567e-02, -8.6264e-02, -3.7676e-03]],\n",
      "\n",
      "         [[ 1.0643e-01,  5.5000e-02, -2.6376e-03],\n",
      "          [-1.4625e-02,  5.4573e-02, -4.5937e-02],\n",
      "          [-1.1796e-01,  4.9552e-03, -7.7210e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2569e-03,  2.6243e-02,  5.5855e-03],\n",
      "          [-3.7656e-02, -4.5477e-02, -1.9064e-02],\n",
      "          [ 4.7573e-02,  9.6349e-03, -5.4198e-02]],\n",
      "\n",
      "         [[-4.0936e-02, -1.2493e-02,  1.2679e-02],\n",
      "          [ 2.0687e-02, -9.9551e-03, -2.2060e-02],\n",
      "          [-5.7732e-02, -2.6932e-02, -8.5503e-04]],\n",
      "\n",
      "         [[ 4.4877e-03,  1.1738e-01, -1.5610e-02],\n",
      "          [-6.1740e-02,  3.9825e-02, -2.2548e-02],\n",
      "          [-5.9553e-02, -4.0604e-04, -1.1326e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.1133e-02,  6.7098e-03, -1.0241e-02],\n",
      "          [-8.4632e-04, -4.8432e-02,  5.7792e-02],\n",
      "          [ 1.3157e-02,  1.3391e-03,  6.0615e-02]],\n",
      "\n",
      "         [[ 3.9243e-02, -2.6613e-02,  3.7080e-02],\n",
      "          [ 2.8706e-02, -1.1180e-02,  5.9965e-02],\n",
      "          [-3.3803e-02,  5.4593e-02, -8.4941e-02]],\n",
      "\n",
      "         [[-9.3167e-03,  1.2495e-02, -4.4054e-02],\n",
      "          [-1.4908e-02,  5.7989e-02, -1.5741e-02],\n",
      "          [ 5.5484e-02, -3.5153e-02,  6.9298e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4400e-02,  2.4662e-02,  1.8809e-02],\n",
      "          [ 2.2107e-03,  4.3661e-02, -6.5046e-02],\n",
      "          [ 1.6057e-02,  2.3885e-02, -8.8999e-03]],\n",
      "\n",
      "         [[-3.2977e-02,  6.3499e-03,  7.1096e-04],\n",
      "          [-8.0470e-03, -1.1654e-02, -5.0850e-03],\n",
      "          [ 1.2836e-02, -1.0831e-02, -4.3122e-02]],\n",
      "\n",
      "         [[ 2.5035e-02,  1.1720e-02, -1.8118e-02],\n",
      "          [ 4.2706e-02, -3.8840e-02,  1.5909e-02],\n",
      "          [-4.0289e-02, -5.3581e-02, -4.1798e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.8756e-02, -3.2136e-02, -3.1476e-02],\n",
      "          [-2.1322e-02,  6.6468e-02,  3.9146e-03],\n",
      "          [-1.4487e-02,  3.5094e-03, -3.0371e-02]],\n",
      "\n",
      "         [[-5.9390e-02, -1.6501e-02,  1.3762e-02],\n",
      "          [ 1.2279e-02,  3.3731e-02,  7.8336e-03],\n",
      "          [-4.1340e-02, -1.7761e-01, -2.6878e-03]],\n",
      "\n",
      "         [[-3.3243e-02, -5.3804e-02, -4.5840e-02],\n",
      "          [ 1.2694e-01, -1.3280e-02, -8.0841e-02],\n",
      "          [-1.4122e-01, -4.2722e-02,  2.1620e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.9329e-02,  4.1979e-02, -2.1080e-02],\n",
      "          [-4.3772e-02, -3.5944e-02, -1.9809e-02],\n",
      "          [-3.1840e-02,  6.0999e-03,  1.1542e-02]],\n",
      "\n",
      "         [[ 1.8720e-02, -2.5417e-02, -1.0650e-01],\n",
      "          [-2.8041e-02,  2.3100e-02,  8.7706e-03],\n",
      "          [-9.5173e-02, -5.1940e-02,  2.0616e-02]],\n",
      "\n",
      "         [[-4.5036e-02, -7.7010e-02, -4.4372e-02],\n",
      "          [ 3.3498e-03, -1.0401e-01,  2.8222e-03],\n",
      "          [ 4.9624e-02,  1.5826e-01,  6.1346e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.9989e-03,  2.8230e-03,  7.0425e-02],\n",
      "          [ 6.4955e-02, -2.2644e-02, -6.3376e-03],\n",
      "          [ 4.0339e-02, -3.7400e-02, -2.1261e-02]],\n",
      "\n",
      "         [[-2.1448e-02,  7.5112e-02,  5.0845e-03],\n",
      "          [ 9.4421e-04, -2.2738e-03,  6.3809e-02],\n",
      "          [ 7.8607e-02,  5.7565e-02, -4.4978e-04]],\n",
      "\n",
      "         [[-8.3701e-02, -1.2035e-02, -3.4206e-03],\n",
      "          [ 3.0422e-02, -6.2301e-02, -4.3349e-02],\n",
      "          [-2.7823e-02, -4.3760e-02,  1.8264e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1429e-02,  5.8412e-02, -1.1340e-02],\n",
      "          [ 9.8056e-03, -2.2167e-03, -7.3140e-02],\n",
      "          [ 1.0635e-02, -3.3860e-02,  2.5174e-02]],\n",
      "\n",
      "         [[ 2.3839e-02,  1.5928e-02,  2.8933e-02],\n",
      "          [ 6.6794e-02,  1.0347e-02,  4.5333e-02],\n",
      "          [ 2.6746e-02,  2.1716e-02, -1.6464e-02]],\n",
      "\n",
      "         [[-8.7474e-03,  5.1849e-03, -1.3167e-02],\n",
      "          [ 3.2215e-02, -2.4847e-04, -1.1576e-03],\n",
      "          [ 5.9969e-02,  3.6691e-02, -3.1079e-02]]]])\n",
      "Gradient for layer3.0.norm1.weight:\n",
      "tensor([ 0.0015, -0.0107,  0.0005, -0.0685,  0.0494, -0.0069, -0.0255,  0.0059,\n",
      "         0.0227,  0.0186, -0.0031,  0.0048,  0.0331, -0.0084, -0.0016, -0.0482,\n",
      "         0.0100,  0.0111, -0.0207,  0.0262,  0.0316, -0.0621,  0.0233,  0.0202,\n",
      "        -0.0309, -0.0165,  0.0135, -0.0030, -0.0067,  0.0514, -0.0204, -0.0489,\n",
      "        -0.0273,  0.0169,  0.0004, -0.0156,  0.0206, -0.0347,  0.0259,  0.0129,\n",
      "         0.0217, -0.0155, -0.0233, -0.0071, -0.0345,  0.0164,  0.0228,  0.0423,\n",
      "         0.0121,  0.0265,  0.0119, -0.0024, -0.0309, -0.0004,  0.0148,  0.0185,\n",
      "        -0.0154, -0.0023, -0.0238, -0.0406,  0.0132, -0.0204,  0.0048,  0.0307,\n",
      "        -0.0406,  0.0037,  0.0020, -0.0331,  0.0130,  0.0355,  0.0097,  0.0189,\n",
      "         0.0072, -0.0099, -0.0244,  0.0094,  0.0138,  0.0085,  0.0093,  0.0320,\n",
      "         0.0225,  0.0036, -0.0237,  0.0079, -0.0289, -0.0187,  0.0423,  0.0376,\n",
      "        -0.0422,  0.0150, -0.0359,  0.0052,  0.0171, -0.0259,  0.0206, -0.0061,\n",
      "        -0.0274, -0.0161,  0.0534,  0.0424, -0.0410, -0.0080, -0.0192, -0.0115,\n",
      "        -0.0047, -0.0040, -0.0028, -0.0205, -0.0077, -0.0178, -0.0010,  0.0096,\n",
      "        -0.0129, -0.0141, -0.0331,  0.0070,  0.0414,  0.0405,  0.0316, -0.0058,\n",
      "        -0.0047,  0.0074, -0.0095,  0.0148,  0.0288,  0.0032,  0.0204, -0.0116,\n",
      "         0.0415, -0.0376, -0.0091, -0.0521, -0.0088,  0.0062, -0.0266,  0.0427,\n",
      "         0.0345, -0.0316, -0.0079,  0.0119,  0.0051,  0.0093,  0.0097,  0.0462,\n",
      "        -0.0058, -0.0285, -0.0592, -0.0232, -0.0222, -0.0288, -0.0094,  0.0292,\n",
      "         0.0235,  0.0050,  0.0524,  0.0086,  0.0610,  0.0304, -0.0220,  0.0289,\n",
      "        -0.0263,  0.0029, -0.0601, -0.0236, -0.0098, -0.0060,  0.0304, -0.0229,\n",
      "        -0.0332,  0.0129, -0.0067,  0.0026, -0.0636, -0.0050, -0.0192,  0.0096,\n",
      "         0.0359, -0.0407, -0.0005, -0.0300,  0.0252, -0.0120,  0.0329, -0.0053,\n",
      "        -0.0003,  0.0185,  0.0090,  0.0178,  0.0424, -0.0126, -0.0258, -0.0051,\n",
      "        -0.0045,  0.0522, -0.0029,  0.0061,  0.0136, -0.0335,  0.0088,  0.0100,\n",
      "         0.0003,  0.0391, -0.0106,  0.0433,  0.0037, -0.0181,  0.0724, -0.0039,\n",
      "         0.0014, -0.0504,  0.0091, -0.0294, -0.0191,  0.0120,  0.0051, -0.0108,\n",
      "        -0.0110, -0.0080, -0.0230,  0.0223, -0.0066, -0.0290,  0.0426, -0.0188,\n",
      "         0.0126, -0.0216,  0.0357,  0.0028, -0.0042, -0.0031, -0.0043,  0.0343,\n",
      "         0.0284,  0.0006, -0.0184,  0.0187,  0.0078,  0.0097,  0.0008,  0.0308,\n",
      "        -0.0508,  0.0182,  0.0385, -0.0232,  0.0147, -0.0114, -0.0478, -0.0217,\n",
      "         0.0179,  0.0052,  0.0041,  0.0052, -0.0223, -0.0226,  0.0034, -0.0368])\n",
      "Gradient for layer3.0.norm1.bias:\n",
      "tensor([ 4.3133e-03,  5.9516e-03,  1.4245e-02, -2.1932e-02,  5.0034e-02,\n",
      "        -2.2299e-02,  2.3429e-02, -2.4146e-03,  2.5368e-02,  8.5905e-03,\n",
      "         2.4083e-02,  3.6639e-02,  1.8274e-02, -2.3142e-02, -1.2245e-02,\n",
      "        -9.7245e-03, -6.5731e-03,  1.5328e-02, -9.1369e-03,  1.8851e-02,\n",
      "        -3.4707e-04, -1.9353e-02,  1.0203e-02,  2.4080e-02, -2.4925e-02,\n",
      "         8.3522e-03,  7.6088e-04, -5.0815e-03,  5.9725e-03,  5.7800e-02,\n",
      "        -9.1207e-03, -4.9872e-02, -2.5031e-03,  1.9679e-03,  3.0165e-03,\n",
      "        -1.3908e-02, -2.3741e-02,  5.5134e-03,  4.1815e-03,  2.3375e-02,\n",
      "         1.5304e-02,  3.8037e-03, -3.7842e-02, -2.9184e-02, -3.2623e-02,\n",
      "         2.7946e-02,  1.0311e-03,  4.5261e-02,  9.8783e-03,  1.1118e-02,\n",
      "        -5.6908e-04,  5.0915e-03, -4.7960e-02, -1.6506e-03,  1.8383e-02,\n",
      "        -8.1393e-03, -2.6683e-02, -2.7461e-02, -3.5756e-03, -2.0608e-02,\n",
      "         2.2623e-02, -4.9410e-03,  2.2033e-02,  1.8497e-02, -9.1044e-03,\n",
      "        -4.0722e-05, -4.9485e-03, -2.9077e-03,  3.5911e-02,  3.6990e-02,\n",
      "        -1.6704e-02,  4.0398e-02,  1.0221e-03, -2.7352e-02, -2.9639e-02,\n",
      "         9.5995e-04,  3.5996e-02,  2.2271e-02, -1.4375e-02,  3.8323e-02,\n",
      "        -3.6800e-03,  4.3102e-03, -3.9818e-04, -1.6352e-02, -2.1923e-04,\n",
      "         6.2913e-03,  1.0257e-02,  1.2923e-02, -2.6555e-02, -3.5716e-03,\n",
      "        -2.5552e-02,  1.2798e-02, -1.3504e-02,  1.2539e-02,  7.3200e-03,\n",
      "        -2.0062e-02, -6.6445e-03, -1.4950e-02,  5.7728e-02,  8.5252e-03,\n",
      "        -1.4460e-02, -1.7964e-02, -2.6232e-02,  4.3166e-03, -1.2595e-03,\n",
      "         1.7470e-03,  1.2701e-02,  3.4760e-03, -1.4555e-03,  1.3213e-03,\n",
      "        -1.2689e-02, -1.6945e-02, -4.9637e-02,  1.4657e-02, -4.0992e-02,\n",
      "         2.3877e-02,  2.3789e-02,  1.1110e-02,  1.2123e-02, -2.1667e-02,\n",
      "        -1.8712e-02,  1.9163e-02,  4.1968e-03, -1.4632e-02,  2.8457e-02,\n",
      "         1.6618e-03,  2.0725e-02, -6.8567e-03,  5.2066e-02, -1.6650e-02,\n",
      "         3.3443e-03, -3.5531e-02,  3.5342e-03,  1.3215e-03, -1.6920e-02,\n",
      "         1.7968e-02,  3.8171e-02, -2.3582e-02, -6.4310e-03,  1.1040e-03,\n",
      "         2.5514e-02,  1.2323e-02,  6.6089e-03,  1.7546e-02, -1.1185e-02,\n",
      "        -2.3840e-02, -2.6151e-02, -1.6730e-02, -2.2182e-03, -3.4091e-02,\n",
      "         6.6901e-04,  7.8072e-03,  2.0797e-02,  1.1317e-02,  3.1336e-02,\n",
      "         1.0019e-02, -6.8042e-04,  2.7251e-02,  7.5578e-03,  2.8600e-02,\n",
      "         1.6764e-02,  6.0093e-03, -1.2554e-02, -9.0177e-03, -1.5082e-02,\n",
      "         7.0880e-03,  2.1410e-02, -6.0321e-03, -4.2996e-02,  2.4400e-02,\n",
      "         2.3089e-03, -2.0507e-03, -3.6855e-02,  4.2001e-03, -2.1295e-02,\n",
      "         1.7330e-02,  2.1831e-02, -3.6382e-03, -1.1930e-02, -3.6286e-02,\n",
      "         2.8338e-02, -1.2519e-02,  2.3582e-02,  1.3229e-02, -5.7394e-03,\n",
      "        -6.8472e-03,  4.5899e-03, -1.2746e-02,  3.5884e-02,  3.7952e-03,\n",
      "        -1.5492e-02, -8.3215e-03,  9.2024e-03,  2.3887e-02,  1.0910e-02,\n",
      "         3.5373e-03,  4.5110e-02, -3.5223e-02,  8.8125e-03, -1.3755e-02,\n",
      "        -2.2838e-02,  3.6890e-02,  3.4599e-03, -4.1526e-03, -1.0632e-02,\n",
      "         6.4970e-03,  5.3307e-02,  1.0497e-02,  3.4378e-03, -2.7916e-02,\n",
      "         1.6018e-02, -6.9643e-03,  8.3178e-03,  5.6226e-03, -1.6933e-02,\n",
      "        -1.1335e-02, -4.8983e-03, -3.7061e-03, -1.9264e-02,  2.3439e-03,\n",
      "         1.4826e-02,  7.0632e-03,  3.3814e-02, -1.3263e-02,  2.9551e-02,\n",
      "        -4.2310e-02,  2.4076e-02, -8.4494e-03,  1.3430e-02, -7.5943e-03,\n",
      "        -1.5219e-02,  1.8332e-02,  5.9534e-02, -1.3255e-02,  2.5983e-03,\n",
      "         3.3171e-02, -1.2014e-02,  9.4961e-04, -1.1383e-02,  1.1402e-02,\n",
      "        -3.1480e-02,  1.6559e-02,  9.4404e-03, -1.6973e-02,  7.1495e-03,\n",
      "        -3.7500e-02,  1.5845e-02, -4.5578e-02,  6.0642e-03, -1.4853e-02,\n",
      "         4.7134e-03,  2.3765e-02,  6.4255e-03, -2.2330e-03, -8.5848e-03,\n",
      "        -3.6349e-02])\n",
      "Gradient for layer3.0.conv2.weight:\n",
      "tensor([[[[ 2.1124e-03,  4.1509e-02, -5.1050e-02],\n",
      "          [-2.0625e-02,  3.9465e-02, -3.8133e-02],\n",
      "          [ 2.6691e-02,  3.7308e-03,  9.7441e-03]],\n",
      "\n",
      "         [[ 1.9224e-02, -3.9224e-02, -3.2540e-02],\n",
      "          [ 2.9596e-02, -5.4740e-02, -6.6516e-03],\n",
      "          [-1.0885e-02,  1.2214e-02,  3.8542e-02]],\n",
      "\n",
      "         [[ 6.2597e-02, -4.3862e-02,  3.5442e-02],\n",
      "          [ 2.3766e-02, -4.1402e-02, -1.5478e-02],\n",
      "          [ 3.5815e-02, -1.0104e-02, -8.4940e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.0698e-02,  1.0038e-02, -2.0880e-02],\n",
      "          [-2.1885e-02,  2.1301e-02, -4.2966e-02],\n",
      "          [-2.3604e-02,  1.6044e-02,  1.3401e-02]],\n",
      "\n",
      "         [[-2.9117e-02,  1.0682e-02, -5.4351e-02],\n",
      "          [ 3.8145e-02, -3.7971e-03, -2.0283e-02],\n",
      "          [ 5.9071e-02,  2.8806e-02, -3.0676e-02]],\n",
      "\n",
      "         [[-6.3861e-02, -1.3189e-02, -2.3416e-02],\n",
      "          [-7.1768e-02,  4.9594e-03,  1.8330e-02],\n",
      "          [-5.2306e-02,  5.4454e-03,  6.3298e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.2147e-02,  1.5272e-02, -1.1129e-02],\n",
      "          [-2.9805e-02, -1.6546e-02, -1.3865e-02],\n",
      "          [ 1.4247e-02,  6.8463e-03, -6.2025e-03]],\n",
      "\n",
      "         [[-6.8734e-02,  6.6163e-02,  2.0769e-02],\n",
      "          [ 1.2821e-01, -5.4216e-02,  4.6060e-03],\n",
      "          [-2.7240e-02,  5.1667e-02,  5.2624e-03]],\n",
      "\n",
      "         [[ 8.5752e-02, -8.5296e-03,  3.7628e-02],\n",
      "          [ 8.1627e-06, -4.3577e-02,  9.9494e-02],\n",
      "          [ 3.0176e-02, -9.8799e-02, -9.3124e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1996e-02, -5.4100e-02,  7.1566e-02],\n",
      "          [ 9.7167e-03,  3.3679e-03, -9.3130e-02],\n",
      "          [ 1.1036e-01, -1.6865e-02, -1.1686e-02]],\n",
      "\n",
      "         [[ 7.8834e-03, -3.1412e-02, -7.3867e-02],\n",
      "          [-9.3624e-02, -3.9664e-02,  7.4862e-03],\n",
      "          [-3.3474e-03, -1.7141e-02,  2.6300e-02]],\n",
      "\n",
      "         [[-7.7401e-03,  3.4272e-02,  2.3755e-02],\n",
      "          [ 7.0912e-02, -2.7347e-02, -5.0305e-02],\n",
      "          [-2.9953e-02,  4.6109e-02,  8.0638e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3723e-02,  2.3748e-02,  7.6195e-03],\n",
      "          [ 7.1997e-03, -2.1199e-02, -5.0522e-02],\n",
      "          [-3.7890e-02, -6.9387e-02, -1.0837e-02]],\n",
      "\n",
      "         [[-4.3764e-03,  3.1260e-02,  4.3065e-02],\n",
      "          [ 2.5336e-02, -4.0603e-02,  7.5815e-02],\n",
      "          [-4.4105e-02,  4.1754e-02, -2.9559e-02]],\n",
      "\n",
      "         [[-1.2085e-02,  1.5475e-02, -2.7181e-02],\n",
      "          [-8.2147e-02,  5.0033e-02,  5.4267e-02],\n",
      "          [-3.2788e-02,  3.5445e-02,  4.8377e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6906e-02, -4.9592e-02, -9.5508e-03],\n",
      "          [-8.3772e-02, -3.3994e-02, -5.4269e-02],\n",
      "          [ 5.2448e-03, -9.8842e-04,  1.2527e-04]],\n",
      "\n",
      "         [[ 4.2458e-02, -3.3921e-03,  7.1938e-02],\n",
      "          [ 1.0444e-02,  2.0439e-02, -2.4287e-02],\n",
      "          [-2.8640e-03, -6.0727e-02,  3.7484e-02]],\n",
      "\n",
      "         [[ 5.2174e-02,  4.3961e-02,  1.1964e-03],\n",
      "          [-2.9151e-02,  9.3317e-03, -1.6361e-02],\n",
      "          [-1.1203e-01,  1.1542e-01, -8.6214e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.0151e-02, -1.3323e-02, -3.4244e-02],\n",
      "          [-4.3114e-02,  1.4160e-02, -1.0787e-02],\n",
      "          [-3.0532e-04, -1.9766e-02,  4.6412e-02]],\n",
      "\n",
      "         [[-2.4590e-02, -2.6459e-02,  5.6232e-02],\n",
      "          [ 5.6690e-02,  1.8134e-02, -7.0231e-02],\n",
      "          [-4.7108e-03,  4.9290e-02,  2.1500e-02]],\n",
      "\n",
      "         [[-1.4952e-02, -7.6238e-03,  6.3199e-02],\n",
      "          [ 5.3959e-03, -1.7726e-04,  4.8465e-02],\n",
      "          [-2.2615e-02,  5.8947e-02,  2.0040e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.1326e-04,  2.6198e-02, -3.8736e-02],\n",
      "          [-5.7413e-02,  2.0954e-03,  6.5536e-02],\n",
      "          [ 4.1108e-02,  5.7196e-03, -3.8188e-02]],\n",
      "\n",
      "         [[-4.0399e-02, -2.7820e-02, -1.9511e-02],\n",
      "          [-4.3459e-02, -3.3380e-02,  2.2609e-02],\n",
      "          [ 5.9222e-02,  5.8234e-02, -3.2297e-02]],\n",
      "\n",
      "         [[ 1.4565e-02, -9.1172e-03, -2.1441e-02],\n",
      "          [-6.3827e-03, -3.9002e-03, -7.2798e-03],\n",
      "          [-1.5915e-02,  4.1237e-03,  5.4580e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.8481e-03,  2.8447e-02, -2.4302e-02],\n",
      "          [-7.1523e-02,  1.4164e-01,  3.3292e-02],\n",
      "          [-2.4075e-02, -2.8622e-02,  1.9594e-02]],\n",
      "\n",
      "         [[-4.1051e-02, -1.0062e-01, -1.9841e-02],\n",
      "          [ 9.0485e-02,  4.4537e-02, -8.1809e-02],\n",
      "          [ 2.7871e-02, -3.7093e-02,  4.8344e-02]],\n",
      "\n",
      "         [[-9.5932e-02,  2.8794e-02, -5.2836e-02],\n",
      "          [-5.7308e-02, -9.5309e-02,  7.0048e-02],\n",
      "          [-1.9205e-02, -5.6902e-02, -4.7975e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.4703e-02,  4.2900e-02, -3.2861e-03],\n",
      "          [-1.6008e-02, -9.1094e-02,  1.3297e-02],\n",
      "          [ 5.8519e-02,  2.4722e-03, -4.9674e-02]],\n",
      "\n",
      "         [[ 8.0591e-02, -1.1059e-01, -4.8887e-02],\n",
      "          [-6.1755e-03,  1.5240e-02,  4.5754e-02],\n",
      "          [ 2.0190e-02,  7.7503e-02,  7.4745e-02]],\n",
      "\n",
      "         [[-1.3904e-01,  3.6731e-02, -6.1098e-02],\n",
      "          [-1.0802e-02, -1.4251e-02,  5.6504e-03],\n",
      "          [ 1.7806e-04, -2.7554e-02,  4.1718e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0707e-02, -4.7041e-03, -9.5754e-03],\n",
      "          [ 1.4008e-02, -8.0447e-03,  2.1086e-02],\n",
      "          [-6.8819e-02, -2.0977e-02, -2.0861e-02]],\n",
      "\n",
      "         [[-4.3398e-02, -6.7129e-03, -3.7256e-02],\n",
      "          [-5.7721e-02,  5.6649e-02,  3.0185e-02],\n",
      "          [-2.4226e-03, -5.1977e-02, -3.5717e-02]],\n",
      "\n",
      "         [[-5.6065e-03, -1.3419e-02,  6.6867e-02],\n",
      "          [ 4.0824e-02,  1.0154e-02,  2.0827e-02],\n",
      "          [-3.1433e-02, -7.9221e-03, -8.6521e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.3529e-02,  2.8390e-02,  1.7818e-02],\n",
      "          [-2.5892e-03, -1.8257e-02, -7.8072e-03],\n",
      "          [ 3.1346e-02,  7.1480e-02,  1.6214e-02]],\n",
      "\n",
      "         [[ 1.6091e-02,  7.8151e-02,  4.3876e-02],\n",
      "          [ 1.7237e-02,  2.4618e-02, -6.6919e-02],\n",
      "          [-5.7870e-03, -1.6622e-02,  2.5975e-02]],\n",
      "\n",
      "         [[ 2.9086e-02,  7.6503e-03,  4.8479e-02],\n",
      "          [-1.2946e-02, -2.2862e-02,  1.5020e-03],\n",
      "          [ 1.6511e-02,  5.3701e-02, -7.2087e-02]]]])\n",
      "Gradient for layer3.0.norm2.weight:\n",
      "tensor([ 0.0603,  0.0345, -0.0695, -0.0365,  0.0010, -0.0102, -0.0527,  0.0246,\n",
      "         0.0133,  0.0131,  0.0078,  0.0759,  0.0227, -0.0084, -0.0290, -0.0163,\n",
      "         0.0046,  0.0240, -0.0299, -0.0145,  0.0059,  0.0071,  0.0339,  0.0091,\n",
      "         0.0384, -0.0139, -0.0092,  0.0055, -0.0064,  0.0109, -0.0182,  0.0391,\n",
      "         0.0037,  0.0074,  0.0046, -0.0330, -0.0264, -0.0075, -0.0237,  0.0307,\n",
      "        -0.0057, -0.0183,  0.0097,  0.0174, -0.0073, -0.0189, -0.0435,  0.0117,\n",
      "         0.0125, -0.0266, -0.0265,  0.0088,  0.0187,  0.0076, -0.0078, -0.0246,\n",
      "        -0.0257, -0.0281,  0.0134, -0.0005,  0.0064, -0.0070,  0.0281, -0.0061,\n",
      "        -0.0278, -0.0441, -0.0176, -0.0121,  0.0264,  0.0013, -0.0182,  0.0360,\n",
      "        -0.0146,  0.0106, -0.0060, -0.0026,  0.0525, -0.0062, -0.0347, -0.0236,\n",
      "         0.0040,  0.0063,  0.0771,  0.0147,  0.0127,  0.0369,  0.0296,  0.0313,\n",
      "        -0.0041, -0.0360, -0.0190, -0.0163,  0.0047,  0.0326,  0.0199,  0.0062,\n",
      "         0.0327,  0.0145, -0.0126,  0.0084, -0.0239,  0.0174, -0.0092,  0.0027,\n",
      "         0.0252,  0.0299,  0.0055, -0.0125,  0.0494,  0.0078,  0.0156,  0.0367,\n",
      "        -0.0063, -0.0040, -0.0404,  0.0225,  0.0214,  0.0238, -0.0256, -0.0160,\n",
      "        -0.0181,  0.0018, -0.0352, -0.0006, -0.0465, -0.0152,  0.0545, -0.0294,\n",
      "         0.0088,  0.0237, -0.0066, -0.0161,  0.0016, -0.0325,  0.0037, -0.0069,\n",
      "         0.0229,  0.0237,  0.0280, -0.0037, -0.0040,  0.0031,  0.0056,  0.0049,\n",
      "         0.0079, -0.0474, -0.0133,  0.0358, -0.0321, -0.0378, -0.0102,  0.0044,\n",
      "        -0.0209,  0.0015, -0.0050, -0.0405,  0.0138,  0.0063, -0.0091,  0.0044,\n",
      "         0.0227, -0.0245, -0.0272, -0.0080,  0.0061,  0.0019, -0.0004, -0.0172,\n",
      "        -0.0039, -0.0165, -0.0019,  0.0010, -0.0237,  0.0397,  0.0167, -0.0248,\n",
      "        -0.0226,  0.0078,  0.0214,  0.0412, -0.0185,  0.0129,  0.0502,  0.0053,\n",
      "         0.0032, -0.0196, -0.0183, -0.0286,  0.0380, -0.0137, -0.0055, -0.0222,\n",
      "         0.0117, -0.0062,  0.0474,  0.0379, -0.0232,  0.0236, -0.0228, -0.0091,\n",
      "        -0.0043, -0.0356,  0.0262, -0.0043, -0.0202, -0.0399,  0.0054, -0.0234,\n",
      "        -0.0307,  0.0116, -0.0246, -0.0246, -0.0122,  0.0110,  0.0031, -0.0546,\n",
      "         0.0073, -0.0165, -0.0138,  0.0161, -0.0225,  0.0209,  0.0190, -0.0339,\n",
      "         0.0156,  0.0157, -0.0397, -0.0029, -0.0273, -0.0131,  0.0112, -0.0043,\n",
      "        -0.0254,  0.0121,  0.0355, -0.0632,  0.0222, -0.0187,  0.0194,  0.0041,\n",
      "         0.0174, -0.0154,  0.0104, -0.0124, -0.0308,  0.0046, -0.0060,  0.0319,\n",
      "        -0.0086, -0.0091, -0.0227,  0.0032, -0.0351,  0.0156,  0.0004, -0.0180])\n",
      "Gradient for layer3.0.norm2.bias:\n",
      "tensor([ 0.0366,  0.0171, -0.0114, -0.0128, -0.0061,  0.0243, -0.0516,  0.0010,\n",
      "         0.0014,  0.0007, -0.0018,  0.0266,  0.0291,  0.0336, -0.0069, -0.0290,\n",
      "        -0.0103, -0.0159, -0.0282,  0.0046,  0.0052,  0.0088,  0.0250,  0.0179,\n",
      "         0.0174,  0.0004,  0.0030, -0.0017,  0.0071, -0.0143, -0.0133, -0.0025,\n",
      "         0.0018, -0.0102,  0.0096, -0.0195, -0.0098,  0.0123, -0.0076,  0.0234,\n",
      "         0.0162,  0.0019,  0.0184,  0.0094, -0.0159,  0.0058, -0.0447,  0.0105,\n",
      "        -0.0164, -0.0083, -0.0323,  0.0106, -0.0013,  0.0292, -0.0278, -0.0177,\n",
      "        -0.0359,  0.0074,  0.0166, -0.0090,  0.0263, -0.0254, -0.0081, -0.0335,\n",
      "        -0.0275,  0.0066, -0.0085,  0.0071,  0.0101, -0.0168,  0.0045, -0.0038,\n",
      "        -0.0210,  0.0010, -0.0081,  0.0150,  0.0445,  0.0129, -0.0222, -0.0245,\n",
      "         0.0191,  0.0046,  0.0126,  0.0216,  0.0080,  0.0089,  0.0085,  0.0109,\n",
      "        -0.0291, -0.0240,  0.0083, -0.0220,  0.0280,  0.0107, -0.0015,  0.0110,\n",
      "         0.0126,  0.0178,  0.0053, -0.0250, -0.0228,  0.0189, -0.0023,  0.0009,\n",
      "        -0.0090,  0.0313,  0.0007,  0.0193,  0.0356, -0.0023, -0.0028,  0.0332,\n",
      "        -0.0295,  0.0059, -0.0227,  0.0149, -0.0280,  0.0229, -0.0171, -0.0143,\n",
      "         0.0021, -0.0146, -0.0177,  0.0138, -0.0278,  0.0045,  0.0203, -0.0204,\n",
      "        -0.0015,  0.0258,  0.0092,  0.0220,  0.0262, -0.0187,  0.0092,  0.0208,\n",
      "         0.0143,  0.0158,  0.0063,  0.0146,  0.0254,  0.0011,  0.0390, -0.0067,\n",
      "         0.0218, -0.0177, -0.0168, -0.0177, -0.0060, -0.0217,  0.0075, -0.0211,\n",
      "        -0.0015,  0.0119,  0.0079, -0.0154,  0.0081,  0.0325, -0.0130,  0.0122,\n",
      "         0.0377, -0.0126, -0.0183, -0.0125,  0.0066, -0.0014, -0.0045, -0.0258,\n",
      "        -0.0043, -0.0054,  0.0217,  0.0271, -0.0092,  0.0554,  0.0296, -0.0015,\n",
      "         0.0174,  0.0211, -0.0111,  0.0267, -0.0027, -0.0013,  0.0209, -0.0030,\n",
      "        -0.0079,  0.0086, -0.0174, -0.0219,  0.0316, -0.0030,  0.0066, -0.0061,\n",
      "         0.0058,  0.0065, -0.0154,  0.0028, -0.0272, -0.0156,  0.0044,  0.0080,\n",
      "         0.0157, -0.0167,  0.0143,  0.0065,  0.0088, -0.0248, -0.0117,  0.0012,\n",
      "        -0.0228,  0.0198, -0.0245, -0.0082, -0.0165, -0.0323,  0.0176, -0.0109,\n",
      "        -0.0008,  0.0091,  0.0192, -0.0156,  0.0017,  0.0159,  0.0081,  0.0206,\n",
      "        -0.0172,  0.0251,  0.0051,  0.0180, -0.0023,  0.0141, -0.0040,  0.0007,\n",
      "        -0.0106,  0.0079,  0.0139, -0.0067,  0.0113, -0.0200,  0.0273, -0.0020,\n",
      "        -0.0264, -0.0315, -0.0100, -0.0135,  0.0294, -0.0013,  0.0163,  0.0251,\n",
      "        -0.0168,  0.0154, -0.0179, -0.0007, -0.0272,  0.0157,  0.0233, -0.0237])\n",
      "Gradient for layer3.0.shortcut.0.weight:\n",
      "tensor([[[[-0.0004]],\n",
      "\n",
      "         [[ 0.0226]],\n",
      "\n",
      "         [[-0.0088]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0451]],\n",
      "\n",
      "         [[ 0.0440]],\n",
      "\n",
      "         [[ 0.0571]]],\n",
      "\n",
      "\n",
      "        [[[-0.0320]],\n",
      "\n",
      "         [[-0.0413]],\n",
      "\n",
      "         [[-0.0096]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0219]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         [[ 0.1172]]],\n",
      "\n",
      "\n",
      "        [[[-0.0437]],\n",
      "\n",
      "         [[-0.0271]],\n",
      "\n",
      "         [[-0.0355]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0665]],\n",
      "\n",
      "         [[ 0.0411]],\n",
      "\n",
      "         [[ 0.0405]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0173]],\n",
      "\n",
      "         [[ 0.0173]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0199]],\n",
      "\n",
      "         [[ 0.0638]],\n",
      "\n",
      "         [[-0.0191]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0123]],\n",
      "\n",
      "         [[ 0.0243]],\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0573]],\n",
      "\n",
      "         [[ 0.0076]],\n",
      "\n",
      "         [[-0.0355]]],\n",
      "\n",
      "\n",
      "        [[[-0.0254]],\n",
      "\n",
      "         [[ 0.0372]],\n",
      "\n",
      "         [[-0.1148]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0251]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[-0.0244]]]])\n",
      "Gradient for layer3.0.shortcut.1.weight:\n",
      "tensor([ 1.9646e-02, -8.5869e-03,  2.5671e-02,  8.8611e-03,  1.2020e-03,\n",
      "         2.3359e-02, -9.2952e-03, -1.6278e-02, -5.6026e-03,  2.4272e-02,\n",
      "        -2.8295e-02,  4.9240e-02,  4.5801e-02,  5.2150e-02, -1.7394e-02,\n",
      "        -3.8198e-02,  2.0334e-02, -2.2744e-02, -1.4074e-02,  7.7789e-03,\n",
      "         1.7109e-02, -5.5216e-02, -1.1753e-02,  1.7322e-02,  1.9560e-03,\n",
      "        -8.7639e-03,  1.2257e-02,  1.7888e-02,  1.8105e-02, -1.1657e-02,\n",
      "         2.9861e-02,  9.1524e-03, -7.3029e-03, -6.6990e-02,  5.0549e-02,\n",
      "         5.8663e-03,  2.5237e-02, -8.2543e-03,  5.1803e-03,  1.8278e-03,\n",
      "        -2.1821e-03, -5.1292e-02,  1.3971e-02,  6.4738e-03,  4.6995e-04,\n",
      "        -7.2849e-03, -1.6240e-02,  1.1314e-02, -2.7874e-02, -5.4131e-03,\n",
      "        -1.7848e-02,  8.3705e-03, -4.6463e-02,  3.7699e-03, -4.4937e-02,\n",
      "         4.0625e-02, -5.5070e-02,  3.0314e-02,  1.2546e-02, -2.4670e-03,\n",
      "         1.5220e-02,  7.6501e-03,  1.9961e-02,  2.1365e-02,  1.5037e-02,\n",
      "         6.2266e-03, -9.2707e-03,  1.6046e-02, -8.3631e-03, -1.0995e-03,\n",
      "         2.9030e-03, -1.4522e-02,  3.3615e-03,  1.7143e-03, -3.8560e-03,\n",
      "         1.9391e-02,  1.1403e-02,  3.7775e-02,  8.9383e-03, -3.1865e-02,\n",
      "         2.4993e-02, -1.5570e-02, -2.3599e-02, -5.5742e-03,  1.6051e-02,\n",
      "         5.0777e-02,  3.4897e-02, -1.6849e-02, -1.3177e-02, -3.5061e-03,\n",
      "         3.4998e-02, -7.0998e-03,  5.0986e-02, -3.2586e-02, -2.8987e-02,\n",
      "         4.1659e-03,  1.6422e-03,  4.5717e-03,  3.1074e-02, -1.1241e-02,\n",
      "        -1.5230e-02, -5.1043e-03,  1.4724e-02, -4.7175e-03, -3.2656e-02,\n",
      "         4.8099e-03, -1.7499e-02,  2.2959e-02,  2.0789e-02, -2.7108e-02,\n",
      "         6.0832e-03,  4.0063e-04, -2.6250e-02,  3.8242e-02,  7.6538e-03,\n",
      "        -8.4043e-03, -1.7999e-02,  3.0735e-03,  1.0165e-02,  8.0351e-03,\n",
      "         1.3675e-02, -3.2624e-02,  2.3804e-02,  4.6553e-02,  1.2224e-02,\n",
      "         3.6184e-02,  6.5442e-03, -2.6217e-02,  2.0596e-02,  5.6965e-03,\n",
      "         9.9594e-03, -2.1833e-04,  3.2120e-02,  5.1701e-03, -9.3929e-05,\n",
      "         2.1474e-02, -2.1684e-03,  8.6443e-03,  9.9673e-03,  9.5694e-03,\n",
      "         1.8903e-02,  1.0897e-02,  1.5635e-03,  1.3295e-02,  1.7054e-02,\n",
      "         7.8631e-04, -2.2571e-02, -1.0258e-02,  1.5764e-02, -8.5672e-03,\n",
      "        -8.1142e-03, -1.1817e-02,  2.7355e-02,  6.8768e-03,  2.4703e-02,\n",
      "        -1.2687e-02,  3.7741e-03,  4.6222e-02, -6.9403e-03, -4.4552e-02,\n",
      "        -2.0370e-03, -1.1739e-02, -9.0070e-03,  4.0198e-03,  9.2938e-03,\n",
      "         1.0528e-03,  5.8714e-03, -2.1671e-02, -2.5767e-02,  1.6854e-02,\n",
      "         2.8832e-02,  1.3668e-02,  3.2772e-04,  3.2334e-02,  5.0823e-03,\n",
      "        -1.7612e-02,  4.4484e-02, -4.4409e-03, -9.5751e-03, -1.7727e-03,\n",
      "        -1.2313e-02, -3.1384e-03, -4.1246e-03,  1.0230e-02,  1.7770e-03,\n",
      "        -4.6911e-03, -9.2949e-03, -2.9051e-02,  2.2960e-02, -1.4249e-02,\n",
      "         6.7832e-03,  1.9288e-02,  1.4736e-02, -1.3030e-02, -5.5539e-03,\n",
      "        -1.9052e-02, -2.7054e-02, -4.0474e-03,  1.8805e-02,  7.0829e-04,\n",
      "        -5.1240e-03, -6.2067e-02, -2.0673e-02,  1.2379e-03,  1.1333e-02,\n",
      "         7.6058e-03, -1.7786e-02,  1.9615e-02,  1.5550e-02, -3.7511e-03,\n",
      "         6.3668e-04, -1.0741e-02,  7.1603e-03, -3.0133e-02,  1.2245e-02,\n",
      "         2.7892e-03,  3.9626e-03,  2.1727e-02,  3.5931e-03, -1.3867e-02,\n",
      "         1.8219e-02, -1.4788e-03, -1.2176e-02,  1.6070e-02, -1.7927e-02,\n",
      "        -1.0928e-02,  9.3072e-03,  3.1638e-02,  1.2419e-03,  3.0877e-02,\n",
      "        -1.2093e-02,  5.8200e-03,  6.0685e-03,  3.7103e-02, -1.3527e-02,\n",
      "         1.1305e-02,  2.3622e-03, -2.2389e-02,  1.4092e-02, -5.7118e-03,\n",
      "        -3.6925e-02, -1.3238e-02,  1.5059e-03,  1.3529e-02,  3.2741e-02,\n",
      "        -4.1600e-02,  5.9279e-03, -2.1730e-03, -1.2666e-02, -1.4747e-02,\n",
      "        -1.2661e-02, -2.6597e-02, -1.5910e-02, -7.8396e-03,  1.3385e-02,\n",
      "        -1.0016e-02])\n",
      "Gradient for layer3.0.shortcut.1.bias:\n",
      "tensor([ 0.0366,  0.0171, -0.0114, -0.0128, -0.0061,  0.0243, -0.0516,  0.0010,\n",
      "         0.0014,  0.0007, -0.0018,  0.0266,  0.0291,  0.0336, -0.0069, -0.0290,\n",
      "        -0.0103, -0.0159, -0.0282,  0.0046,  0.0052,  0.0088,  0.0250,  0.0179,\n",
      "         0.0174,  0.0004,  0.0030, -0.0017,  0.0071, -0.0143, -0.0133, -0.0025,\n",
      "         0.0018, -0.0102,  0.0096, -0.0195, -0.0098,  0.0123, -0.0076,  0.0234,\n",
      "         0.0162,  0.0019,  0.0184,  0.0094, -0.0159,  0.0058, -0.0447,  0.0105,\n",
      "        -0.0164, -0.0083, -0.0323,  0.0106, -0.0013,  0.0292, -0.0278, -0.0177,\n",
      "        -0.0359,  0.0074,  0.0166, -0.0090,  0.0263, -0.0254, -0.0081, -0.0335,\n",
      "        -0.0275,  0.0066, -0.0085,  0.0071,  0.0101, -0.0168,  0.0045, -0.0038,\n",
      "        -0.0210,  0.0010, -0.0081,  0.0150,  0.0445,  0.0129, -0.0222, -0.0245,\n",
      "         0.0191,  0.0046,  0.0126,  0.0216,  0.0080,  0.0089,  0.0085,  0.0109,\n",
      "        -0.0291, -0.0240,  0.0083, -0.0220,  0.0280,  0.0107, -0.0015,  0.0110,\n",
      "         0.0126,  0.0178,  0.0053, -0.0250, -0.0228,  0.0189, -0.0023,  0.0009,\n",
      "        -0.0090,  0.0313,  0.0007,  0.0193,  0.0356, -0.0023, -0.0028,  0.0332,\n",
      "        -0.0295,  0.0059, -0.0227,  0.0149, -0.0280,  0.0229, -0.0171, -0.0143,\n",
      "         0.0021, -0.0146, -0.0177,  0.0138, -0.0278,  0.0045,  0.0203, -0.0204,\n",
      "        -0.0015,  0.0258,  0.0092,  0.0220,  0.0262, -0.0187,  0.0092,  0.0208,\n",
      "         0.0143,  0.0158,  0.0063,  0.0146,  0.0254,  0.0011,  0.0390, -0.0067,\n",
      "         0.0218, -0.0177, -0.0168, -0.0177, -0.0060, -0.0217,  0.0075, -0.0211,\n",
      "        -0.0015,  0.0119,  0.0079, -0.0154,  0.0081,  0.0325, -0.0130,  0.0122,\n",
      "         0.0377, -0.0126, -0.0183, -0.0125,  0.0066, -0.0014, -0.0045, -0.0258,\n",
      "        -0.0043, -0.0054,  0.0217,  0.0271, -0.0092,  0.0554,  0.0296, -0.0015,\n",
      "         0.0174,  0.0211, -0.0111,  0.0267, -0.0027, -0.0013,  0.0209, -0.0030,\n",
      "        -0.0079,  0.0086, -0.0174, -0.0219,  0.0316, -0.0030,  0.0066, -0.0061,\n",
      "         0.0058,  0.0065, -0.0154,  0.0028, -0.0272, -0.0156,  0.0044,  0.0080,\n",
      "         0.0157, -0.0167,  0.0143,  0.0065,  0.0088, -0.0248, -0.0117,  0.0012,\n",
      "        -0.0228,  0.0198, -0.0245, -0.0082, -0.0165, -0.0323,  0.0176, -0.0109,\n",
      "        -0.0008,  0.0091,  0.0192, -0.0156,  0.0017,  0.0159,  0.0081,  0.0206,\n",
      "        -0.0172,  0.0251,  0.0051,  0.0180, -0.0023,  0.0141, -0.0040,  0.0007,\n",
      "        -0.0106,  0.0079,  0.0139, -0.0067,  0.0113, -0.0200,  0.0273, -0.0020,\n",
      "        -0.0264, -0.0315, -0.0100, -0.0135,  0.0294, -0.0013,  0.0163,  0.0251,\n",
      "        -0.0168,  0.0154, -0.0179, -0.0007, -0.0272,  0.0157,  0.0233, -0.0237])\n",
      "Gradient for layer3.1.conv1.weight:\n",
      "tensor([[[[ 3.1194e-04,  5.1075e-02, -1.5662e-02],\n",
      "          [ 4.3789e-03, -3.1455e-02, -3.9876e-02],\n",
      "          [-2.9305e-02,  2.5495e-02,  6.0713e-02]],\n",
      "\n",
      "         [[-5.2614e-02, -5.7409e-03, -1.9575e-03],\n",
      "          [-2.4370e-02, -1.0916e-02, -2.9635e-03],\n",
      "          [-8.0054e-03, -1.1973e-02, -5.4955e-03]],\n",
      "\n",
      "         [[-7.4788e-02,  7.7005e-02,  5.0781e-02],\n",
      "          [-3.0960e-02,  3.1840e-02, -2.0129e-02],\n",
      "          [-4.5198e-02,  1.2105e-02,  7.6439e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.3939e-03, -1.3991e-02,  6.8887e-02],\n",
      "          [-1.9347e-02, -1.1340e-02,  5.7694e-02],\n",
      "          [ 7.6362e-02, -4.9869e-03, -8.1919e-02]],\n",
      "\n",
      "         [[-2.1595e-02,  1.1901e-02,  1.7101e-02],\n",
      "          [ 2.8124e-02, -3.8909e-02, -6.6055e-02],\n",
      "          [-2.5088e-02, -8.3348e-03, -8.9406e-02]],\n",
      "\n",
      "         [[ 4.4264e-03,  2.0246e-02, -5.8780e-02],\n",
      "          [-3.9572e-02, -4.7126e-02,  5.9677e-02],\n",
      "          [ 9.7368e-02,  8.3998e-02, -1.4145e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.9870e-02, -4.1296e-02, -3.5856e-02],\n",
      "          [-2.3151e-02, -1.1771e-03,  2.3129e-02],\n",
      "          [-1.4331e-01, -1.7335e-02,  5.5060e-02]],\n",
      "\n",
      "         [[-2.3513e-02, -1.5641e-02, -2.0964e-02],\n",
      "          [ 5.8335e-02,  7.9497e-02, -3.1651e-02],\n",
      "          [-3.7475e-02,  1.5756e-02,  5.6472e-02]],\n",
      "\n",
      "         [[-4.6297e-02, -1.6524e-02, -1.0691e-01],\n",
      "          [-4.1786e-02, -1.6086e-02, -2.2283e-02],\n",
      "          [-1.7913e-02,  5.7283e-02, -4.4456e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.2506e-02, -3.3248e-02, -3.3880e-02],\n",
      "          [-5.4626e-02, -6.5076e-02,  2.6069e-02],\n",
      "          [ 9.3552e-03,  8.0031e-02, -2.0205e-02]],\n",
      "\n",
      "         [[ 2.7886e-02,  4.9632e-02,  6.6819e-02],\n",
      "          [ 3.9250e-02, -4.3302e-03, -3.2654e-02],\n",
      "          [-9.4631e-03, -3.1848e-02,  4.4766e-02]],\n",
      "\n",
      "         [[-4.4788e-02, -2.6813e-02,  3.9524e-02],\n",
      "          [-8.7031e-02, -8.5030e-03, -4.1240e-03],\n",
      "          [ 7.7846e-02,  4.2414e-02, -2.7937e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4617e-03, -6.0847e-02,  7.3825e-03],\n",
      "          [ 6.1581e-02,  1.4946e-02,  7.8000e-03],\n",
      "          [-1.4790e-02,  8.9647e-02, -2.2466e-02]],\n",
      "\n",
      "         [[ 1.1900e-02, -4.2151e-03, -3.4486e-02],\n",
      "          [ 2.8595e-02,  2.1579e-02, -2.7700e-02],\n",
      "          [ 1.2506e-02,  2.2235e-02,  7.4382e-02]],\n",
      "\n",
      "         [[-6.3619e-04,  5.0196e-02,  3.3598e-02],\n",
      "          [-3.1281e-03, -1.4136e-02,  1.4460e-02],\n",
      "          [ 3.1525e-02,  2.4884e-02,  1.3873e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6550e-02, -4.1613e-02,  6.7347e-02],\n",
      "          [-1.8530e-02, -5.2992e-02,  2.4938e-02],\n",
      "          [-2.6068e-02,  1.0471e-01,  7.5379e-03]],\n",
      "\n",
      "         [[ 1.9678e-02, -6.9329e-02, -1.7173e-02],\n",
      "          [-1.8644e-02, -3.0969e-02,  1.5536e-02],\n",
      "          [ 5.1927e-02,  8.7221e-03,  3.5375e-03]],\n",
      "\n",
      "         [[-4.4955e-02, -3.4149e-02, -2.2163e-02],\n",
      "          [ 1.6176e-02,  1.4293e-02,  1.5605e-02],\n",
      "          [ 3.7025e-02,  7.8559e-02, -1.4368e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.4679e-03, -6.6086e-02,  6.5918e-02],\n",
      "          [-5.1284e-02,  7.0929e-02,  5.6412e-02],\n",
      "          [ 4.8199e-02,  7.5138e-02, -2.7366e-02]],\n",
      "\n",
      "         [[ 1.2317e-02, -5.8939e-02,  2.1296e-05],\n",
      "          [-1.0903e-02, -8.9873e-03,  1.3723e-02],\n",
      "          [ 2.4590e-02,  3.0033e-02, -1.1881e-02]],\n",
      "\n",
      "         [[ 2.1141e-02, -5.7173e-02,  6.5978e-02],\n",
      "          [-5.8839e-02,  8.9908e-03,  5.3758e-02],\n",
      "          [ 2.5464e-02,  1.4959e-02, -4.6853e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3251e-02, -6.7979e-02,  5.3242e-02],\n",
      "          [ 1.8793e-02,  5.6923e-02, -4.3380e-02],\n",
      "          [-3.6629e-02,  4.2884e-02, -9.8473e-04]],\n",
      "\n",
      "         [[ 4.6781e-02,  2.3838e-02, -9.6327e-02],\n",
      "          [ 5.2613e-02,  3.6182e-02, -3.1559e-02],\n",
      "          [ 3.9158e-02,  3.9519e-02, -2.3812e-02]],\n",
      "\n",
      "         [[ 1.7350e-02,  2.3306e-02,  7.8482e-04],\n",
      "          [ 3.8785e-02, -2.2175e-02, -1.8915e-02],\n",
      "          [-4.9467e-02, -1.1110e-03,  5.5280e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6055e-02, -7.0576e-02, -3.7327e-02],\n",
      "          [ 2.7569e-02,  4.4521e-03,  4.8944e-02],\n",
      "          [ 3.4695e-02, -3.6562e-02,  5.9366e-02]],\n",
      "\n",
      "         [[ 2.6612e-02, -4.8098e-02, -3.0827e-02],\n",
      "          [-3.7526e-03,  3.2016e-02, -2.8021e-02],\n",
      "          [-2.1032e-02, -5.9122e-02,  1.4078e-02]],\n",
      "\n",
      "         [[-2.9812e-02,  3.1644e-02, -4.4071e-02],\n",
      "          [ 3.6343e-02,  5.5494e-04,  3.6122e-02],\n",
      "          [ 6.9494e-02, -3.2829e-02,  4.2190e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5476e-02, -6.1930e-02,  6.1196e-04],\n",
      "          [-1.4751e-02, -6.2123e-03, -3.4005e-02],\n",
      "          [ 6.9825e-02,  3.0854e-02,  2.8831e-02]],\n",
      "\n",
      "         [[-3.4069e-03, -9.6495e-03,  1.8391e-02],\n",
      "          [ 3.0620e-02, -6.9145e-02,  2.4681e-02],\n",
      "          [-2.2095e-02,  3.2961e-03, -1.8873e-02]],\n",
      "\n",
      "         [[ 1.7282e-02,  6.4966e-02, -2.8875e-02],\n",
      "          [-3.0678e-02, -1.0684e-01, -7.7422e-02],\n",
      "          [ 8.7142e-03, -3.5292e-03, -5.5766e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5999e-02, -2.6375e-02, -3.0675e-02],\n",
      "          [ 7.3219e-02,  2.4307e-02, -4.1951e-02],\n",
      "          [-1.6396e-02, -9.2815e-02,  5.5920e-02]],\n",
      "\n",
      "         [[ 2.8766e-02,  2.4987e-02,  7.6241e-03],\n",
      "          [ 2.6689e-02,  7.3955e-03, -7.5349e-03],\n",
      "          [-1.7259e-02,  3.7960e-02,  1.8693e-02]],\n",
      "\n",
      "         [[ 3.2472e-02,  1.6178e-03,  6.7495e-02],\n",
      "          [ 3.8708e-02, -1.0797e-02,  3.7162e-02],\n",
      "          [-4.0202e-02, -9.2284e-03,  6.3187e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1392e-02,  1.0487e-02, -3.0056e-02],\n",
      "          [ 6.5608e-02,  2.2609e-02,  1.8916e-05],\n",
      "          [-2.1551e-02,  3.1401e-04, -1.3513e-02]],\n",
      "\n",
      "         [[ 6.6453e-03,  4.8765e-02, -9.9127e-03],\n",
      "          [ 2.1144e-02,  5.3697e-02, -1.9877e-02],\n",
      "          [-5.2204e-02, -4.5362e-02, -9.2442e-03]],\n",
      "\n",
      "         [[-4.0988e-02, -9.0119e-02, -5.3525e-03],\n",
      "          [ 2.4440e-02, -3.6087e-02, -2.6678e-02],\n",
      "          [-2.2514e-02, -1.7040e-02,  4.0241e-02]]]])\n",
      "Gradient for layer3.1.norm1.weight:\n",
      "tensor([-2.3097e-02, -1.9060e-02, -1.2294e-02, -2.9651e-02, -9.8996e-04,\n",
      "         1.7667e-02, -7.3071e-03, -3.4682e-02, -1.1040e-02,  1.3191e-02,\n",
      "         4.0791e-02,  1.2655e-02, -1.5839e-02,  1.4242e-02,  7.3386e-03,\n",
      "        -1.6534e-02, -7.1545e-03,  2.6883e-03,  5.4568e-03,  4.2674e-03,\n",
      "        -1.7623e-04,  3.4427e-02, -3.0537e-02, -2.1279e-02, -1.3876e-02,\n",
      "         4.1703e-02, -1.8881e-02,  2.5732e-02,  3.6279e-02,  1.1003e-03,\n",
      "        -5.7316e-02, -2.2269e-02,  1.3678e-02,  7.3204e-03, -2.3795e-02,\n",
      "        -3.7176e-03,  2.9120e-02, -1.8130e-02,  3.4363e-03,  3.2513e-02,\n",
      "        -3.7449e-02, -3.5051e-03, -2.7463e-02,  1.4504e-02, -3.5445e-03,\n",
      "         8.0261e-03,  1.5188e-02,  2.9513e-02, -1.7380e-02,  9.4432e-04,\n",
      "        -5.0389e-03,  3.0191e-03,  2.7846e-02, -1.1432e-04,  1.1308e-02,\n",
      "         9.1737e-03, -1.0721e-02, -6.0623e-04,  6.9818e-03, -9.3870e-04,\n",
      "         2.3335e-02, -1.1816e-02, -9.8982e-03,  2.0842e-02,  2.9232e-03,\n",
      "         2.3113e-02, -1.1753e-02, -1.0218e-02,  5.7574e-03, -9.1648e-03,\n",
      "         1.5964e-02,  6.0593e-03, -2.0708e-02,  1.6632e-02,  3.0351e-02,\n",
      "        -1.4064e-02,  5.7033e-03,  1.3695e-02,  9.1028e-03, -2.8240e-03,\n",
      "        -1.9956e-02,  3.5192e-02,  1.0863e-02,  1.5377e-02, -2.7700e-02,\n",
      "         1.9413e-03, -3.7652e-02,  1.6083e-02, -4.4348e-02, -4.0962e-03,\n",
      "         1.7744e-02, -6.0466e-03, -1.1372e-02,  7.0423e-04,  1.5993e-03,\n",
      "        -8.4722e-03,  9.3191e-03, -2.8442e-02,  3.1745e-02,  2.3206e-02,\n",
      "         4.3940e-03, -2.1905e-02,  2.0886e-02,  1.4132e-02,  8.7879e-03,\n",
      "        -1.2536e-03,  9.6886e-03,  1.1542e-02,  1.4670e-02, -7.8778e-03,\n",
      "         1.1555e-03, -1.4413e-02,  7.9659e-03, -5.5842e-04,  1.4097e-03,\n",
      "        -1.7986e-02,  2.3189e-02, -8.9068e-03, -9.5646e-03, -2.0651e-02,\n",
      "         3.6165e-02, -7.5900e-03,  1.5549e-02,  1.8402e-02, -8.5657e-03,\n",
      "        -5.5058e-03, -7.5885e-02, -1.2973e-02, -2.1144e-02, -2.7375e-03,\n",
      "        -4.2010e-02,  1.3307e-02,  1.6403e-02, -9.6562e-03, -6.9317e-03,\n",
      "        -1.0541e-02,  1.8145e-02, -1.9890e-02,  2.3769e-02, -9.3186e-03,\n",
      "        -2.3341e-02,  1.1783e-02,  2.2479e-02, -1.6492e-02,  5.7186e-03,\n",
      "         7.2724e-03, -8.3251e-03, -1.6296e-02,  2.6647e-02, -9.3776e-03,\n",
      "         4.8396e-02, -3.5015e-02,  3.2540e-02,  6.8887e-03, -1.0396e-02,\n",
      "         2.4606e-02,  1.1370e-02, -9.8917e-03,  3.2106e-03, -1.2708e-02,\n",
      "        -4.3710e-02,  1.0263e-02,  3.1096e-03, -4.6224e-03,  1.9182e-02,\n",
      "         2.7911e-02,  2.6657e-02,  1.8595e-02, -2.8578e-03, -1.8447e-02,\n",
      "        -1.9692e-02,  6.4975e-03, -2.4277e-02, -4.4881e-02, -2.5454e-02,\n",
      "        -1.8794e-02,  4.2520e-03,  1.8601e-02,  2.2095e-02, -1.3777e-02,\n",
      "         1.4308e-03,  2.5244e-02, -2.9691e-02, -2.6550e-03, -1.8017e-02,\n",
      "         2.4241e-02,  2.6853e-02, -2.8819e-02, -2.2649e-02, -8.1131e-03,\n",
      "         2.4707e-02,  2.1963e-02,  2.9319e-02,  2.9016e-02, -7.3410e-03,\n",
      "         9.7346e-03,  2.5875e-02, -2.1778e-02,  3.4251e-02,  1.9981e-02,\n",
      "         7.5055e-05,  9.4114e-06,  2.1456e-02, -2.9829e-02,  9.2184e-03,\n",
      "        -1.5512e-02,  5.2636e-02, -6.6131e-03,  2.1918e-02, -2.9444e-02,\n",
      "        -1.4351e-02,  1.4001e-03,  1.8490e-02,  7.5237e-05, -1.5255e-02,\n",
      "        -1.2822e-02,  7.7217e-03, -5.8620e-03,  1.1280e-02,  1.7562e-02,\n",
      "         1.2987e-02, -1.7122e-02,  5.0986e-03,  3.3380e-02,  8.0098e-03,\n",
      "        -3.3709e-02, -2.2353e-02, -1.5407e-02, -4.8361e-03, -9.7137e-03,\n",
      "         3.3938e-02, -1.5321e-03,  7.1469e-03,  2.6010e-03, -3.6090e-02,\n",
      "         1.5977e-03, -1.3230e-02, -6.0303e-03, -4.5713e-03,  1.3367e-02,\n",
      "        -1.7945e-02, -2.3289e-02,  1.1859e-02,  2.5607e-02, -1.4289e-02,\n",
      "         1.9629e-02,  4.2124e-03, -2.3787e-02,  6.2194e-03,  4.5022e-03,\n",
      "        -6.4075e-02, -3.0664e-03, -3.8472e-02,  1.4904e-02,  3.4863e-02,\n",
      "        -1.3988e-02])\n",
      "Gradient for layer3.1.norm1.bias:\n",
      "tensor([-1.0716e-03, -1.2248e-02, -1.8799e-02, -1.0208e-02,  2.9790e-03,\n",
      "         1.9066e-02, -2.1722e-02, -1.6203e-02, -3.0900e-04,  4.0750e-02,\n",
      "         4.2993e-02,  9.1752e-03, -1.0167e-02, -6.3671e-03, -2.3259e-04,\n",
      "        -5.2863e-03, -2.1351e-02, -2.9346e-02,  7.8338e-03, -1.7370e-03,\n",
      "        -4.0219e-04,  2.5304e-02, -2.0692e-02, -8.6719e-03, -8.7297e-03,\n",
      "         4.5582e-02, -1.5787e-02,  6.8101e-03,  2.8466e-02, -3.6314e-03,\n",
      "        -2.7639e-02, -3.4506e-02, -8.7014e-03,  1.4756e-02, -2.7533e-02,\n",
      "         1.7521e-02,  2.4571e-02, -3.5529e-03,  1.8447e-02,  3.2861e-02,\n",
      "        -5.9722e-03, -1.7672e-02, -3.7670e-02,  1.9736e-03, -3.4724e-02,\n",
      "         2.5508e-03, -4.3718e-03,  1.7823e-02, -1.1475e-02, -4.1633e-03,\n",
      "        -1.6589e-02, -5.4308e-05,  1.5184e-02,  4.0834e-03,  3.2497e-03,\n",
      "         1.3517e-03, -1.6904e-02,  1.7986e-03,  1.4301e-02,  1.1132e-02,\n",
      "         1.6914e-02, -1.4472e-02, -1.0218e-03,  1.6636e-03, -1.2523e-02,\n",
      "         1.2455e-02, -5.0619e-03, -2.1782e-02, -1.0178e-02, -2.1770e-02,\n",
      "         1.9726e-02,  1.7484e-02, -7.4269e-03,  2.5030e-02,  1.2527e-03,\n",
      "        -1.2007e-02, -3.3111e-02,  1.7095e-02,  1.4302e-02,  4.1914e-03,\n",
      "        -3.2586e-03,  3.1297e-02, -1.3374e-02,  2.0939e-02, -1.0547e-02,\n",
      "         1.6246e-02, -3.8050e-02,  2.0166e-02, -1.5203e-02, -8.2903e-03,\n",
      "         1.4774e-02,  1.2405e-02,  8.6712e-04,  2.1819e-02,  6.7483e-03,\n",
      "        -7.4715e-04, -2.9645e-03, -1.6813e-02,  5.2678e-02,  1.8931e-02,\n",
      "         1.0470e-02,  2.3437e-03,  1.0019e-02,  5.7111e-03,  2.8461e-03,\n",
      "         9.7753e-03, -9.2204e-04,  3.6718e-03,  2.6616e-02, -7.7501e-03,\n",
      "        -2.5257e-02, -1.3556e-02,  1.2795e-02,  1.2268e-03, -1.2447e-02,\n",
      "        -2.1127e-02,  3.0087e-02,  5.5474e-03, -1.2861e-02, -1.9373e-02,\n",
      "         1.2489e-02, -2.3916e-02,  1.1150e-02,  7.1299e-03, -1.2770e-02,\n",
      "        -1.9841e-03, -3.4248e-02,  5.6402e-03, -1.6642e-02,  1.9475e-02,\n",
      "        -2.5116e-02,  7.2797e-03,  6.3540e-03,  8.2238e-03, -8.3247e-03,\n",
      "        -1.9690e-03,  2.0212e-02, -1.5811e-02,  1.7860e-02, -9.2184e-03,\n",
      "        -1.1721e-02, -6.6221e-03,  2.3401e-02, -2.7981e-03,  7.2048e-03,\n",
      "         1.2886e-02, -8.7371e-05, -1.0041e-02,  3.4990e-03, -2.6124e-02,\n",
      "         2.8345e-02, -2.6837e-02,  4.1794e-02,  6.2996e-03, -6.6670e-04,\n",
      "         7.9401e-03,  8.6967e-03, -4.8266e-04, -3.0046e-02, -2.5116e-03,\n",
      "        -3.1770e-02,  1.4089e-02,  9.0377e-03, -2.3817e-03,  1.6977e-02,\n",
      "         2.5401e-02,  1.1843e-02, -2.9443e-03,  3.3941e-03, -1.2405e-02,\n",
      "        -8.2107e-03,  2.4581e-02, -2.4705e-02, -3.0894e-02, -1.5999e-02,\n",
      "        -2.5910e-03, -1.4907e-02, -4.2318e-03,  1.2670e-02, -3.3153e-02,\n",
      "         4.4914e-03,  1.8855e-03, -2.9695e-02,  1.0662e-02, -1.8486e-02,\n",
      "         2.2206e-02,  2.5528e-02, -1.8841e-02, -1.5389e-02, -9.4433e-03,\n",
      "         3.3515e-02,  7.7530e-03,  2.7809e-02,  1.8315e-02, -3.5615e-03,\n",
      "         3.6401e-02,  2.1156e-02,  2.3229e-03,  1.2467e-02,  9.9861e-03,\n",
      "        -1.1695e-02, -1.4169e-03,  9.8574e-03,  4.0183e-03, -6.8240e-03,\n",
      "        -9.4968e-03,  1.8986e-02, -8.8018e-03,  3.0018e-02,  4.1349e-04,\n",
      "        -2.5592e-02, -8.4745e-04,  1.8308e-02,  1.1319e-03, -1.9425e-02,\n",
      "        -1.6330e-03,  2.3373e-02, -4.3725e-04,  3.1040e-02,  9.0002e-03,\n",
      "        -1.3693e-02, -2.4904e-02,  2.9358e-03,  1.8456e-02, -5.9778e-03,\n",
      "        -9.5761e-03, -4.9916e-03, -3.8920e-02, -5.1312e-03, -5.4062e-03,\n",
      "         3.3956e-02, -1.3000e-02,  6.9542e-03,  3.4193e-04, -2.3652e-02,\n",
      "         8.5427e-03, -1.9845e-03, -1.5348e-02, -6.8437e-03,  1.8749e-02,\n",
      "        -1.0960e-02, -2.9404e-02,  2.8954e-03,  9.0636e-03, -1.7647e-02,\n",
      "         2.2016e-02,  1.8577e-02, -1.6571e-02, -1.2386e-02, -3.1336e-03,\n",
      "        -3.6484e-02, -6.7758e-04, -2.2421e-02,  6.4539e-03,  2.2474e-02,\n",
      "        -1.1912e-02])\n",
      "Gradient for layer3.1.conv2.weight:\n",
      "tensor([[[[ 4.1744e-02,  3.7489e-03, -5.9042e-04],\n",
      "          [-7.1320e-03, -2.3228e-02,  4.5822e-02],\n",
      "          [-3.5718e-02, -1.2806e-02, -3.1646e-02]],\n",
      "\n",
      "         [[ 2.3750e-02, -1.2436e-02, -3.4559e-02],\n",
      "          [ 6.1914e-03, -2.3521e-02, -1.6097e-02],\n",
      "          [ 1.8442e-02,  1.8420e-02,  2.4926e-02]],\n",
      "\n",
      "         [[ 4.8064e-03, -6.9838e-03,  3.5379e-02],\n",
      "          [-1.0824e-02,  8.5332e-03, -3.5006e-03],\n",
      "          [ 7.1524e-03, -1.8247e-02, -3.6050e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4759e-02, -7.5421e-03, -3.0800e-02],\n",
      "          [-1.3835e-02, -1.3124e-02,  6.6885e-03],\n",
      "          [-4.0339e-03,  1.5311e-02,  2.8465e-02]],\n",
      "\n",
      "         [[ 7.7988e-04, -5.7720e-03, -3.7282e-03],\n",
      "          [-1.7523e-02, -4.3359e-02,  2.6181e-02],\n",
      "          [-7.7597e-03,  5.6050e-02, -4.4102e-02]],\n",
      "\n",
      "         [[-3.2229e-02, -3.4294e-02, -2.6156e-03],\n",
      "          [-3.4855e-02, -2.6628e-02,  2.2505e-02],\n",
      "          [-1.8222e-02, -5.4273e-02,  3.5220e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1265e-02,  5.2967e-02,  7.4213e-02],\n",
      "          [-5.3593e-03, -7.6230e-03,  8.6842e-03],\n",
      "          [ 3.8359e-03,  5.4362e-03, -5.5404e-03]],\n",
      "\n",
      "         [[-9.6676e-03,  7.5322e-03,  4.1262e-02],\n",
      "          [ 1.4497e-02, -1.4064e-02,  7.2319e-03],\n",
      "          [ 3.0013e-03, -3.9948e-02,  5.5498e-02]],\n",
      "\n",
      "         [[-3.2072e-02,  4.2767e-02, -4.9483e-03],\n",
      "          [ 7.1354e-04,  7.4802e-03, -1.1958e-02],\n",
      "          [-4.5857e-03,  8.6977e-03,  3.3355e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5401e-02,  1.3839e-03, -6.3645e-03],\n",
      "          [ 3.1963e-02,  3.7429e-02, -1.7724e-02],\n",
      "          [-1.6249e-02, -5.0479e-02,  2.4324e-02]],\n",
      "\n",
      "         [[-1.3829e-02,  2.6702e-02,  2.5485e-02],\n",
      "          [ 6.0975e-03, -1.8568e-02,  5.1701e-03],\n",
      "          [ 3.8935e-02,  1.6283e-02,  4.2670e-02]],\n",
      "\n",
      "         [[ 1.6967e-04,  2.6040e-02, -1.9346e-02],\n",
      "          [ 1.4822e-03,  6.3190e-02, -1.2815e-02],\n",
      "          [ 7.2305e-03,  1.7692e-03,  3.0320e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.6214e-02, -2.1604e-03, -1.9378e-03],\n",
      "          [-3.4037e-02,  5.9254e-02,  3.9789e-02],\n",
      "          [ 2.2023e-02,  1.6148e-02, -1.5134e-03]],\n",
      "\n",
      "         [[-2.6102e-02, -1.3151e-02,  9.4842e-02],\n",
      "          [ 2.1833e-02, -3.2111e-02, -6.2908e-03],\n",
      "          [ 4.0859e-02, -1.8429e-02, -7.1925e-02]],\n",
      "\n",
      "         [[ 6.5785e-02,  8.6383e-03, -3.1954e-02],\n",
      "          [-2.4181e-02,  6.6235e-03,  3.6103e-02],\n",
      "          [-3.4045e-02, -3.5226e-02,  5.4992e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.5858e-03,  5.3925e-05,  1.3763e-05],\n",
      "          [ 7.2388e-02, -6.7774e-03,  3.8282e-02],\n",
      "          [ 2.5416e-02,  6.4820e-02, -5.3152e-02]],\n",
      "\n",
      "         [[ 8.8969e-02, -4.3734e-02,  1.5985e-02],\n",
      "          [ 2.6277e-02, -3.2053e-02,  1.5999e-02],\n",
      "          [ 7.1286e-02,  2.1809e-02, -2.1225e-02]],\n",
      "\n",
      "         [[-1.2099e-02,  2.8368e-02, -9.7579e-03],\n",
      "          [-1.6061e-02,  4.9006e-02,  3.5030e-02],\n",
      "          [-2.8003e-02,  4.7519e-02, -2.2864e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.2878e-02,  1.8259e-02,  4.9937e-03],\n",
      "          [-2.1433e-02,  3.3405e-03,  1.2256e-02],\n",
      "          [-1.7378e-02, -1.9628e-02,  8.9450e-03]],\n",
      "\n",
      "         [[ 1.5242e-02,  1.4724e-02,  2.4447e-02],\n",
      "          [-2.5817e-02, -3.2817e-02, -3.2146e-03],\n",
      "          [ 1.7015e-02, -3.0659e-02, -1.2483e-02]],\n",
      "\n",
      "         [[ 1.6464e-03,  1.3261e-02,  3.3306e-02],\n",
      "          [ 1.2031e-02,  1.6422e-02, -3.5388e-02],\n",
      "          [-3.0476e-02,  1.9046e-02, -1.9189e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9282e-03,  1.1149e-02, -2.5462e-03],\n",
      "          [ 5.3790e-02,  4.2096e-02, -5.5251e-02],\n",
      "          [-2.0141e-02,  3.4589e-03, -1.5998e-03]],\n",
      "\n",
      "         [[ 4.5868e-02, -4.0053e-02, -3.8935e-02],\n",
      "          [-6.9931e-02, -5.9962e-02, -3.3933e-02],\n",
      "          [ 7.1900e-03,  1.1761e-02,  1.3726e-02]],\n",
      "\n",
      "         [[-1.5916e-02, -1.7324e-03,  3.0753e-03],\n",
      "          [-3.6682e-03,  2.7558e-02,  2.8850e-03],\n",
      "          [ 1.1100e-02, -5.2993e-03,  1.5456e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.4271e-02, -4.9517e-02, -4.3509e-03],\n",
      "          [-3.4033e-03, -3.6320e-03,  4.2514e-02],\n",
      "          [-1.2212e-02,  1.2356e-02,  1.6751e-02]],\n",
      "\n",
      "         [[ 2.1542e-02, -3.5819e-02,  4.5196e-02],\n",
      "          [-4.4111e-02, -7.4011e-02, -4.5572e-02],\n",
      "          [-1.1449e-02,  1.4746e-02, -5.0490e-02]],\n",
      "\n",
      "         [[-3.0505e-02, -3.6166e-02, -6.0876e-02],\n",
      "          [-3.1504e-02,  1.3521e-02,  3.0422e-02],\n",
      "          [-6.6367e-02, -7.7578e-03,  4.4985e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9267e-02, -7.2944e-02, -4.7717e-02],\n",
      "          [ 3.7872e-02, -1.9263e-03,  1.0497e-02],\n",
      "          [-4.5080e-02,  2.3741e-02, -5.5229e-02]],\n",
      "\n",
      "         [[-5.4539e-02, -3.0280e-02, -3.9759e-02],\n",
      "          [-1.5944e-02, -3.8461e-02, -4.9438e-02],\n",
      "          [ 6.0372e-02, -1.6844e-02,  6.7704e-02]],\n",
      "\n",
      "         [[-5.2685e-02, -2.9507e-02, -2.2574e-02],\n",
      "          [-7.5512e-03,  3.0951e-02, -8.1312e-03],\n",
      "          [-2.3096e-02, -4.8436e-03, -3.4404e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2187e-02,  3.0747e-02,  2.5502e-02],\n",
      "          [-4.9159e-03, -1.6538e-02, -1.7837e-02],\n",
      "          [ 7.7392e-02, -1.8834e-02, -4.9553e-02]],\n",
      "\n",
      "         [[-1.0595e-03,  1.4704e-02,  8.2428e-02],\n",
      "          [-8.2310e-03, -2.4632e-02, -3.4091e-02],\n",
      "          [ 7.0236e-03,  7.2323e-03, -1.4275e-02]],\n",
      "\n",
      "         [[-2.1811e-02,  1.1066e-02,  5.9762e-02],\n",
      "          [ 2.9409e-02,  2.5634e-02,  5.0943e-02],\n",
      "          [ 2.8840e-02,  3.6309e-02,  3.5177e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3020e-02, -2.7300e-02, -1.7918e-03],\n",
      "          [ 7.2017e-03,  4.6533e-03,  3.0825e-02],\n",
      "          [-1.2488e-02,  5.0722e-02,  5.4609e-02]],\n",
      "\n",
      "         [[-3.6527e-03, -1.9893e-02,  1.9379e-02],\n",
      "          [ 3.9450e-02,  3.4809e-02,  2.9082e-02],\n",
      "          [ 3.3200e-02,  5.3999e-02, -2.4836e-04]],\n",
      "\n",
      "         [[ 3.5620e-02,  3.6755e-02, -2.3974e-02],\n",
      "          [ 3.0608e-02,  6.4258e-02,  3.2206e-02],\n",
      "          [-1.0585e-02, -3.0209e-02,  4.0148e-02]]]])\n",
      "Gradient for layer3.1.norm2.weight:\n",
      "tensor([ 7.9059e-04,  1.8799e-02,  9.0413e-04, -2.6580e-02, -1.2813e-02,\n",
      "         9.7544e-06, -1.9377e-02, -4.1110e-02,  6.8999e-03, -2.6403e-02,\n",
      "         2.0558e-03, -7.8922e-03, -4.3764e-04,  1.4480e-02, -1.2596e-02,\n",
      "        -1.2524e-02,  1.3974e-02,  1.7498e-02, -1.6495e-02, -7.2692e-03,\n",
      "        -6.1392e-03,  3.6656e-02,  3.9661e-03, -1.5336e-02, -7.0110e-03,\n",
      "         5.0721e-03, -1.6021e-02, -1.6337e-02,  3.3647e-03,  7.8409e-03,\n",
      "         1.9682e-02,  1.5196e-02, -4.5950e-03, -4.3093e-03, -3.6344e-02,\n",
      "         7.1189e-04, -2.4439e-02,  1.4105e-02,  3.6730e-03, -4.0627e-02,\n",
      "        -1.1242e-02,  5.2344e-03, -6.4103e-04,  6.4624e-03, -1.6666e-02,\n",
      "         4.6042e-02,  1.8175e-02,  4.1686e-03, -4.2748e-03,  1.8403e-02,\n",
      "         2.5839e-02, -1.1370e-02,  9.2648e-03,  2.6760e-02, -1.2054e-02,\n",
      "        -2.7542e-04,  1.0885e-02, -2.7420e-02, -6.2086e-03, -4.5786e-03,\n",
      "        -5.8984e-03,  1.2663e-02,  5.1555e-03, -1.4150e-02, -2.7384e-03,\n",
      "         1.3537e-02, -4.9566e-03,  1.6499e-02,  3.1805e-02,  1.2892e-02,\n",
      "        -3.9405e-03, -1.3009e-02,  1.8149e-02,  1.3685e-02,  1.5771e-02,\n",
      "         1.4103e-02, -1.2291e-02, -1.1901e-02, -5.9047e-03, -1.9283e-02,\n",
      "        -1.8250e-02,  6.7206e-03,  2.9287e-03,  9.5898e-03,  1.3572e-02,\n",
      "        -1.2308e-02, -1.2271e-02,  8.8951e-03, -9.0814e-03,  5.8892e-03,\n",
      "        -9.2700e-03,  4.6537e-03, -1.1929e-02, -1.9588e-02, -1.8358e-02,\n",
      "         1.4856e-03, -3.3902e-02,  1.2107e-02,  5.5466e-03, -6.5611e-03,\n",
      "         5.3663e-03, -5.1894e-04, -3.6255e-02,  1.7918e-02,  8.0822e-03,\n",
      "        -3.3288e-02,  2.9769e-02, -4.9386e-03,  3.5242e-02,  1.9022e-02,\n",
      "         1.0962e-02, -1.3031e-02, -1.3996e-02,  1.0851e-02,  8.5132e-03,\n",
      "        -2.3951e-03, -2.5179e-03, -1.8616e-02, -2.5179e-02,  1.4107e-02,\n",
      "         3.1219e-03, -1.0736e-02,  1.3625e-02, -3.5383e-02,  1.2265e-02,\n",
      "         6.6513e-04, -2.3709e-02, -2.2770e-02, -2.4988e-02,  7.9690e-04,\n",
      "        -3.4826e-02,  5.5294e-03, -1.0285e-02, -3.1105e-02,  1.4336e-02,\n",
      "         3.0649e-02,  3.9185e-03,  9.0110e-03, -1.6736e-02, -5.4639e-03,\n",
      "         3.1278e-02,  3.2526e-02, -1.8305e-02,  8.7385e-03, -7.1704e-03,\n",
      "         8.4082e-03,  3.8133e-02, -4.9876e-03, -1.0409e-02,  1.7056e-02,\n",
      "         1.4919e-02, -1.8971e-02,  5.4424e-03,  2.3415e-02, -2.0605e-02,\n",
      "         1.8614e-02, -1.6569e-02,  1.6198e-03, -1.7595e-03, -6.2973e-03,\n",
      "         3.4805e-03,  2.7726e-02,  7.0612e-03,  3.1047e-03, -2.1430e-03,\n",
      "        -8.9375e-05, -1.1658e-02, -4.3533e-03, -6.8409e-03,  1.4423e-02,\n",
      "         1.3299e-02, -4.9241e-03, -8.2734e-03,  2.6747e-02, -1.1440e-02,\n",
      "         7.1258e-03,  2.9611e-02, -1.5900e-02,  8.4830e-03,  3.9098e-03,\n",
      "        -2.3065e-02,  1.1510e-02, -1.1863e-02, -1.2574e-02, -1.8507e-02,\n",
      "        -1.3852e-03,  9.1268e-04,  2.0741e-02,  1.8912e-02,  2.0504e-02,\n",
      "         9.7314e-03,  1.3423e-02, -4.1238e-03, -6.7499e-03, -1.5887e-02,\n",
      "         2.2931e-02,  1.7296e-02, -2.1018e-02,  1.3953e-02, -2.5066e-02,\n",
      "         1.3531e-03,  1.2901e-02, -5.2868e-03,  4.4023e-03,  3.1050e-02,\n",
      "         6.0046e-03, -4.0975e-03,  3.2500e-02, -1.8461e-02,  9.2714e-04,\n",
      "        -8.9842e-03, -8.8210e-03, -6.6649e-03, -7.1289e-03, -1.2566e-02,\n",
      "         5.1256e-03, -2.2941e-02, -1.8399e-02, -7.2672e-03, -3.6603e-02,\n",
      "        -1.5515e-02, -6.0219e-03,  3.6957e-04,  1.4682e-03, -3.2818e-04,\n",
      "         1.9339e-02,  1.1264e-02,  3.1858e-02, -1.4920e-02, -1.8752e-02,\n",
      "        -1.6353e-02,  1.2218e-02, -1.1996e-02, -1.2104e-02, -3.0723e-03,\n",
      "         2.9467e-03, -6.1229e-03,  1.4933e-02, -2.7803e-02,  1.9628e-02,\n",
      "         2.3742e-02, -3.3994e-02,  1.7420e-02, -3.3279e-02,  3.0776e-02,\n",
      "        -1.8349e-03,  1.3122e-03,  4.5378e-03,  3.2825e-03, -3.1516e-02,\n",
      "        -3.6575e-05,  2.3332e-02,  7.9227e-03, -4.7209e-03,  2.8117e-02,\n",
      "        -1.8665e-02])\n",
      "Gradient for layer3.1.norm2.bias:\n",
      "tensor([ 1.9010e-02,  1.6568e-02,  4.4142e-03, -2.3483e-03,  1.3394e-03,\n",
      "        -6.5554e-03, -2.6164e-02,  2.0276e-02, -7.6900e-03,  1.2872e-02,\n",
      "        -9.8043e-04,  7.3557e-03, -1.3097e-02,  3.7477e-03, -6.9136e-03,\n",
      "        -6.1381e-03,  2.2836e-02,  9.7146e-04, -1.3758e-04, -5.0691e-03,\n",
      "         1.0827e-02,  4.1126e-03, -6.5336e-03,  3.4537e-03, -8.8757e-03,\n",
      "        -1.2063e-02,  2.0122e-03, -2.4929e-02, -4.6438e-03,  3.0555e-04,\n",
      "         2.5902e-03,  8.0058e-03,  8.1747e-03, -6.3841e-03,  5.3866e-03,\n",
      "        -2.5595e-02,  1.6189e-02,  1.1067e-03, -3.6946e-03, -1.4748e-02,\n",
      "         2.2322e-03,  1.5817e-03, -2.0216e-03, -8.1880e-03, -1.1631e-02,\n",
      "         8.7154e-03,  4.6202e-03, -7.3134e-04, -1.1682e-02,  1.8357e-03,\n",
      "        -6.5646e-04,  8.5962e-03,  6.4634e-03,  1.8897e-03,  2.3334e-03,\n",
      "        -4.7764e-03, -8.2946e-03,  1.2176e-02, -1.3258e-04, -1.0395e-02,\n",
      "         1.2670e-02,  5.3326e-03, -1.4635e-02, -2.4397e-02, -2.1526e-02,\n",
      "         9.2073e-03, -9.2356e-04,  2.0275e-02,  3.5304e-03,  5.2580e-03,\n",
      "        -8.5685e-03, -1.8462e-02, -2.0391e-03,  8.9784e-03,  3.8610e-03,\n",
      "         1.4062e-02,  5.3011e-03,  1.4864e-02, -5.9514e-03, -7.9024e-03,\n",
      "         1.0934e-02,  1.0587e-02,  2.9359e-03, -1.5593e-03,  1.0306e-02,\n",
      "        -2.2084e-02,  4.3902e-03, -1.2305e-03, -1.1940e-02,  1.9487e-03,\n",
      "         6.6756e-03, -2.2242e-03,  1.5426e-02, -1.5151e-02, -6.5598e-03,\n",
      "         1.0498e-02,  1.4429e-04,  1.2610e-02,  4.6675e-03,  1.3714e-02,\n",
      "         1.7062e-02, -8.5652e-03, -4.5696e-03, -3.2549e-05, -1.1375e-03,\n",
      "        -3.9733e-03,  1.6797e-02, -9.8352e-03,  1.6725e-02, -4.2913e-03,\n",
      "         2.1870e-02,  1.4799e-02, -6.3763e-03,  4.8283e-03, -1.5666e-02,\n",
      "         1.4860e-02,  3.3775e-04, -4.5078e-03,  1.2542e-03, -2.4569e-03,\n",
      "         3.4532e-03, -1.7752e-02, -6.7147e-03, -1.0622e-03, -2.1298e-03,\n",
      "        -4.7611e-03, -5.5284e-03, -1.7481e-02, -2.4992e-02, -1.5394e-02,\n",
      "        -1.5535e-02,  3.9492e-03, -1.2192e-02, -5.8335e-03,  4.0650e-03,\n",
      "         1.0605e-02,  1.4332e-02,  2.5597e-02, -1.1218e-02,  8.9246e-03,\n",
      "         7.6629e-03,  2.1323e-02, -1.1147e-02, -1.7980e-02,  1.9372e-02,\n",
      "         1.1868e-02, -4.3943e-03, -3.4092e-03,  1.3640e-03, -7.9142e-03,\n",
      "        -1.3168e-02, -1.7159e-03,  6.0650e-04, -1.3562e-03, -1.1976e-02,\n",
      "        -2.6059e-04, -5.1628e-03,  1.1000e-03,  1.3397e-02, -8.4143e-03,\n",
      "         2.1420e-02,  2.1461e-03, -5.4379e-03, -6.5869e-03, -1.0188e-02,\n",
      "        -2.8009e-02, -4.7537e-03, -9.4524e-03, -1.8186e-02, -9.4022e-03,\n",
      "        -4.4591e-03,  2.2259e-02, -4.9911e-03,  1.4501e-02, -7.6340e-03,\n",
      "        -1.3776e-02,  3.6318e-03, -1.3299e-02, -3.2681e-03,  1.3719e-02,\n",
      "        -1.6402e-02, -5.5331e-03,  3.0720e-03,  1.2917e-03, -1.0856e-02,\n",
      "         7.5839e-03, -1.0323e-02,  2.2008e-02,  8.7767e-04,  1.0495e-02,\n",
      "         1.9657e-02,  3.1034e-02, -8.3368e-03, -1.7067e-03, -3.3630e-03,\n",
      "         3.6143e-03,  9.0971e-03,  4.0313e-03,  2.1814e-02, -6.8367e-03,\n",
      "         5.9120e-03, -2.5326e-03,  2.6102e-05,  1.0638e-02,  3.2053e-03,\n",
      "        -1.3631e-02,  7.5685e-03,  2.2162e-02, -2.0165e-02,  1.6180e-02,\n",
      "        -2.1096e-02, -1.7114e-02, -3.9086e-03, -7.7873e-03, -2.7819e-03,\n",
      "         1.9193e-02, -1.1506e-02,  2.7769e-03,  2.1326e-03, -2.9421e-02,\n",
      "        -6.0017e-03,  1.6075e-02, -2.0310e-03, -8.7907e-04,  1.3563e-02,\n",
      "         1.6763e-02,  7.7164e-03,  1.8885e-02, -1.2663e-02,  1.1944e-02,\n",
      "         2.5545e-03,  1.2896e-02,  8.9841e-03, -1.0074e-02, -1.2730e-02,\n",
      "        -1.1491e-02, -7.6486e-03, -9.7887e-03,  9.6030e-03, -1.1150e-02,\n",
      "        -3.1155e-04, -1.2588e-02, -7.2633e-03, -6.0438e-03,  2.2514e-02,\n",
      "        -2.8641e-03,  4.5937e-03, -8.8942e-03, -1.8148e-02, -2.3514e-03,\n",
      "        -1.1877e-02,  2.1323e-02, -2.7141e-03, -5.8100e-03,  6.8452e-03,\n",
      "        -1.3329e-02])\n",
      "Gradient for layer4.0.conv1.weight:\n",
      "tensor([[[[-1.4137e-02,  8.7742e-04, -5.9359e-03],\n",
      "          [ 6.5620e-03,  2.4367e-04, -2.7506e-03],\n",
      "          [ 3.5116e-03, -1.1524e-02, -3.5233e-03]],\n",
      "\n",
      "         [[-2.0227e-02,  1.4116e-03,  8.9312e-03],\n",
      "          [-7.7742e-03, -1.8488e-02, -3.4243e-03],\n",
      "          [-1.7030e-02,  5.0279e-03,  6.0019e-03]],\n",
      "\n",
      "         [[ 7.1015e-03, -1.1281e-02, -1.4100e-02],\n",
      "          [-1.2747e-02,  1.8609e-02, -6.8292e-03],\n",
      "          [ 1.3746e-02, -2.9917e-02, -3.3350e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8597e-03,  1.1928e-02, -9.2853e-03],\n",
      "          [ 4.7539e-04,  3.8597e-03, -1.7562e-02],\n",
      "          [ 1.5100e-02, -1.8100e-02, -2.2432e-02]],\n",
      "\n",
      "         [[-1.0654e-02, -4.7206e-03,  1.0565e-02],\n",
      "          [-4.9274e-03, -2.0180e-02,  1.8814e-02],\n",
      "          [-3.3308e-03,  2.8936e-03,  1.5138e-03]],\n",
      "\n",
      "         [[ 3.5342e-03,  4.9171e-03, -4.8471e-03],\n",
      "          [-7.2475e-03,  1.4083e-02,  3.8106e-03],\n",
      "          [-9.9547e-03,  1.9103e-02,  6.6003e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.5992e-02,  1.8659e-02,  2.7495e-02],\n",
      "          [-1.8081e-02, -1.6621e-02, -4.6380e-02],\n",
      "          [-2.2135e-02,  1.1918e-03, -5.6659e-03]],\n",
      "\n",
      "         [[ 1.1895e-02,  3.1415e-03,  7.8436e-03],\n",
      "          [-2.5611e-02,  2.7116e-02,  9.4373e-04],\n",
      "          [ 9.2106e-03,  2.6150e-03, -6.5956e-02]],\n",
      "\n",
      "         [[-2.6734e-03,  6.4290e-03,  8.0562e-03],\n",
      "          [ 5.0367e-03,  3.4015e-03,  3.3228e-03],\n",
      "          [-4.0827e-03,  1.0208e-02,  1.8413e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0450e-02, -1.2577e-02,  2.0054e-02],\n",
      "          [-4.1842e-03,  7.1964e-03, -6.9171e-02],\n",
      "          [-3.3346e-02, -7.4073e-02, -2.3161e-02]],\n",
      "\n",
      "         [[-2.3624e-02,  1.5630e-02,  2.8455e-02],\n",
      "          [ 4.2204e-03, -2.9118e-02, -8.9375e-03],\n",
      "          [-1.6575e-02,  1.8474e-03, -1.4240e-02]],\n",
      "\n",
      "         [[ 1.4150e-03,  4.5108e-02, -1.8401e-04],\n",
      "          [ 6.3875e-03,  2.1173e-02, -1.7982e-02],\n",
      "          [-2.7033e-02,  2.3891e-02,  2.7763e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.9473e-04, -1.4219e-02,  6.0687e-03],\n",
      "          [-2.3459e-02,  2.1625e-02, -3.7365e-02],\n",
      "          [ 1.5634e-02,  4.6138e-03,  9.9005e-03]],\n",
      "\n",
      "         [[-6.4777e-05, -1.3805e-02,  2.7377e-02],\n",
      "          [-8.3292e-03,  2.6475e-02, -7.6459e-03],\n",
      "          [-1.8173e-02,  6.4943e-03, -9.3582e-03]],\n",
      "\n",
      "         [[-1.7505e-02, -2.6380e-02, -3.1971e-02],\n",
      "          [ 7.3187e-03,  3.1137e-02,  4.8579e-03],\n",
      "          [ 2.9499e-05,  5.8449e-03, -1.0354e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5971e-02, -1.1504e-03,  2.0448e-02],\n",
      "          [-3.4423e-02, -5.8236e-03, -2.7494e-02],\n",
      "          [-3.5945e-02, -1.0455e-02, -1.5760e-02]],\n",
      "\n",
      "         [[-6.2742e-03,  7.4218e-03,  1.0565e-02],\n",
      "          [ 6.7652e-03, -1.6130e-02,  1.5515e-02],\n",
      "          [ 2.0500e-02,  3.0148e-03, -1.5441e-03]],\n",
      "\n",
      "         [[ 6.5910e-03,  7.4212e-03, -1.4016e-03],\n",
      "          [ 1.7484e-02, -1.2162e-02,  2.3987e-02],\n",
      "          [-3.6633e-03,  1.0422e-02,  6.4511e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.8231e-02,  4.6591e-03,  5.5407e-03],\n",
      "          [-1.3457e-02, -1.1645e-02, -4.1201e-02],\n",
      "          [-1.3150e-02, -9.1906e-03,  4.1742e-03]],\n",
      "\n",
      "         [[ 2.1055e-02, -2.5891e-02, -8.0003e-03],\n",
      "          [-2.4548e-02, -5.1524e-03,  2.2895e-02],\n",
      "          [ 1.2252e-03,  1.3560e-02,  1.1332e-02]],\n",
      "\n",
      "         [[-1.1290e-02,  8.2454e-03, -4.4531e-03],\n",
      "          [ 1.0180e-02, -2.2111e-02,  2.7880e-02],\n",
      "          [ 2.1923e-02, -4.7161e-03,  1.0621e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4480e-02, -4.3051e-02,  2.4786e-03],\n",
      "          [ 3.9524e-03,  2.3244e-02, -1.5450e-02],\n",
      "          [-2.2639e-02,  1.2107e-02, -4.4942e-02]],\n",
      "\n",
      "         [[-1.3954e-02,  3.7810e-03, -1.4391e-02],\n",
      "          [ 3.9354e-03, -1.8084e-02,  3.2774e-03],\n",
      "          [-1.5930e-03, -1.3900e-02,  4.3479e-03]],\n",
      "\n",
      "         [[-6.2118e-03, -3.7441e-03, -6.7763e-03],\n",
      "          [ 2.0457e-03,  1.0686e-02,  4.6591e-03],\n",
      "          [-6.1392e-03, -3.9111e-02,  4.4568e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.1397e-03,  7.7049e-03,  7.2732e-03],\n",
      "          [ 8.3583e-03,  6.2854e-03,  8.9291e-04],\n",
      "          [ 1.4921e-02, -1.1597e-03, -1.8122e-02]],\n",
      "\n",
      "         [[ 4.5238e-03, -5.5208e-03,  2.7392e-02],\n",
      "          [-1.5216e-02, -4.7946e-02, -2.9766e-02],\n",
      "          [-4.3375e-02, -1.8606e-02, -8.3712e-03]],\n",
      "\n",
      "         [[-4.4789e-03,  6.1784e-03, -5.0659e-04],\n",
      "          [-5.2009e-03,  2.9195e-02,  6.6514e-03],\n",
      "          [ 1.8783e-02, -1.8802e-02, -4.3891e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0483e-02,  1.8333e-02,  2.6172e-02],\n",
      "          [-2.7087e-02, -1.5882e-02, -7.9561e-04],\n",
      "          [ 1.7346e-02, -1.7362e-02, -3.1480e-02]],\n",
      "\n",
      "         [[ 3.9855e-03,  1.6394e-02,  4.7658e-03],\n",
      "          [ 1.2437e-03, -4.1550e-02,  2.6479e-02],\n",
      "          [ 4.5537e-04,  6.6762e-03, -7.7242e-03]],\n",
      "\n",
      "         [[ 1.2754e-02,  1.4584e-02,  6.8347e-03],\n",
      "          [ 9.8828e-04, -4.9818e-03,  2.0178e-02],\n",
      "          [ 6.6080e-03,  5.6414e-03,  1.8200e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8161e-02,  6.2747e-02,  5.6023e-02],\n",
      "          [ 2.9908e-02, -3.0378e-02,  6.2933e-03],\n",
      "          [-4.0373e-02,  2.2392e-02, -6.7073e-02]],\n",
      "\n",
      "         [[ 8.4102e-02,  1.2458e-02, -3.4239e-03],\n",
      "          [-5.3502e-02, -1.5384e-01, -1.6059e-02],\n",
      "          [-5.0948e-02, -3.2210e-02, -1.2645e-02]],\n",
      "\n",
      "         [[ 1.4370e-02,  1.0332e-01,  1.0702e-01],\n",
      "          [ 6.0979e-03, -9.7043e-04,  4.9968e-02],\n",
      "          [ 3.3664e-02,  1.8935e-02,  5.2351e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8869e-03,  1.3596e-02,  6.3904e-02],\n",
      "          [ 5.9199e-03, -5.0166e-03,  4.2105e-02],\n",
      "          [ 2.5416e-02,  3.5089e-02, -3.9515e-02]],\n",
      "\n",
      "         [[ 3.1585e-02,  4.8269e-02, -8.9280e-03],\n",
      "          [-8.2867e-05, -7.0323e-02, -3.2482e-02],\n",
      "          [-2.1295e-02, -2.8930e-02, -1.3654e-02]],\n",
      "\n",
      "         [[ 1.7122e-02,  2.3574e-02,  2.1238e-03],\n",
      "          [-1.6988e-02,  1.6066e-03, -8.8540e-03],\n",
      "          [ 1.4272e-02, -3.0003e-02, -2.5414e-02]]]])\n",
      "Gradient for layer4.0.norm1.weight:\n",
      "tensor([-3.4603e-02,  2.5684e-02, -1.4199e-02,  1.3212e-02,  2.4387e-02,\n",
      "         1.9034e-02, -4.3138e-03,  1.1731e-02,  1.0973e-02,  1.1291e-02,\n",
      "        -1.2011e-02,  4.1504e-03,  1.9224e-03,  3.9104e-03, -6.2293e-03,\n",
      "         4.9362e-03, -4.4454e-03, -6.4186e-03,  1.1411e-02,  1.2075e-02,\n",
      "        -1.0366e-02,  1.1583e-02,  6.9046e-03,  1.7983e-02, -2.6005e-02,\n",
      "        -1.6567e-03, -2.3664e-03,  1.1173e-03, -3.9058e-03, -2.2353e-03,\n",
      "         2.2727e-02, -7.2518e-03, -4.6800e-04, -5.7145e-03, -3.0493e-02,\n",
      "        -3.6014e-03, -1.4590e-02,  2.3099e-03, -1.0149e-02, -7.1473e-03,\n",
      "        -1.4738e-02, -2.1193e-03,  1.2008e-03,  1.6278e-02, -8.1052e-03,\n",
      "         1.9387e-02, -2.1443e-02, -3.2722e-03,  3.3985e-03, -7.7690e-03,\n",
      "         1.0759e-02,  2.0098e-02, -7.0464e-03, -2.5298e-02, -2.6878e-02,\n",
      "        -2.3350e-04,  1.6348e-02, -1.8369e-02,  1.1795e-03,  3.1932e-02,\n",
      "         1.5554e-03, -2.2882e-03, -2.4631e-02, -2.0679e-02, -2.2751e-02,\n",
      "        -2.8829e-02,  1.0591e-03, -4.7556e-03, -2.3052e-02,  8.4073e-03,\n",
      "         2.2307e-02,  1.3461e-02, -1.6778e-02, -1.6561e-02,  2.6979e-02,\n",
      "         1.6457e-02,  1.5141e-02,  8.7606e-03, -8.2965e-03,  1.4243e-02,\n",
      "        -7.5654e-03,  2.7133e-03, -1.2608e-02, -7.7770e-03, -7.0309e-03,\n",
      "        -4.8952e-03,  7.1890e-03, -1.1333e-02,  4.6786e-03, -1.5319e-03,\n",
      "         1.1078e-02,  9.9760e-03, -5.1159e-04,  1.4797e-03,  2.0926e-02,\n",
      "         2.1996e-03,  7.1199e-03, -1.3779e-02, -1.0546e-03, -5.7345e-03,\n",
      "        -7.6717e-03,  7.2644e-03,  4.6974e-03, -8.1491e-03, -3.4666e-04,\n",
      "         5.2364e-03, -4.7522e-03,  5.5001e-03, -5.7479e-03,  1.1494e-02,\n",
      "         3.7503e-03,  1.0927e-02,  6.2530e-03, -7.0886e-03,  1.2479e-02,\n",
      "         8.5456e-03, -4.3268e-03,  8.5363e-03, -2.1046e-02,  6.6520e-05,\n",
      "         1.5813e-02,  1.3049e-02, -1.4907e-02,  2.7932e-02, -6.3067e-04,\n",
      "        -1.1106e-02,  1.4902e-02, -8.6848e-03,  5.8689e-03,  3.8272e-02,\n",
      "         3.2045e-03, -2.4139e-03,  3.1707e-03, -1.3391e-02, -5.0070e-03,\n",
      "        -5.7460e-03, -2.8282e-02, -8.1088e-03, -7.8963e-03,  5.2596e-03,\n",
      "         3.3009e-03,  4.1685e-02,  5.4792e-03,  7.3935e-03,  3.0228e-02,\n",
      "        -9.5543e-03,  8.6873e-03,  1.6095e-02, -6.1887e-03, -9.9843e-03,\n",
      "         2.2091e-02, -6.5519e-03,  1.1301e-03, -9.9634e-03, -8.3673e-03,\n",
      "         1.1565e-02,  2.0267e-02,  8.3451e-03, -1.2529e-02,  1.9884e-02,\n",
      "        -1.2159e-02, -1.7063e-02, -2.1918e-02,  1.5842e-02,  9.0259e-03,\n",
      "        -7.2320e-03,  1.8363e-03, -2.5548e-02, -1.0363e-02, -1.3509e-02,\n",
      "        -6.8503e-03, -3.6743e-03,  6.8070e-03, -1.4118e-02,  6.4709e-03,\n",
      "        -2.0714e-03, -2.8471e-03, -7.9939e-03, -5.1686e-03,  2.0030e-03,\n",
      "        -2.3344e-03, -7.3301e-03,  9.4748e-03,  1.0554e-02,  1.1408e-02,\n",
      "         3.2848e-03,  1.7278e-03,  3.4459e-04, -3.2111e-02,  2.0857e-02,\n",
      "         2.2175e-02, -1.0261e-02, -9.3667e-03,  9.0742e-03, -4.9653e-03,\n",
      "         7.9360e-03, -1.1112e-03, -5.7893e-03,  3.0236e-03,  1.0932e-03,\n",
      "        -1.1024e-02,  1.7150e-02,  1.6244e-02, -8.1113e-03,  8.0178e-03,\n",
      "        -2.8661e-02, -1.1746e-02, -1.3363e-02,  4.8079e-03,  5.2835e-03,\n",
      "         2.2780e-03, -2.9460e-03, -1.4055e-03, -2.0439e-03, -6.1225e-03,\n",
      "         9.3260e-03,  1.2591e-02, -9.1515e-03,  1.7599e-02, -1.4484e-02,\n",
      "        -5.0934e-03,  6.5095e-03, -9.1240e-03,  3.2007e-03, -3.2013e-03,\n",
      "        -1.1362e-02,  1.8253e-03, -1.0943e-03, -1.2269e-02,  9.6846e-03,\n",
      "         3.7587e-03, -1.4516e-02, -3.7243e-03,  2.3839e-04,  8.8349e-04,\n",
      "        -9.5494e-03,  9.5276e-03,  2.4896e-03, -3.2391e-04,  2.5560e-03,\n",
      "        -6.2807e-03,  1.3055e-02, -9.4869e-03,  2.2916e-03,  1.2734e-02,\n",
      "        -1.4848e-02, -6.7820e-03, -1.2695e-02,  1.6920e-03,  2.4812e-03,\n",
      "        -5.8385e-03, -2.3035e-02, -1.8928e-03,  5.9131e-03, -6.0498e-03,\n",
      "         8.9046e-03, -3.8785e-04,  1.3158e-03,  3.4246e-02,  2.5774e-02,\n",
      "         1.9602e-04,  2.7314e-03, -8.7889e-03, -5.0877e-03, -5.9776e-03,\n",
      "         1.1275e-02, -8.8715e-03,  3.4028e-03, -1.2605e-02,  6.9337e-03,\n",
      "         2.3170e-03, -1.9042e-02, -4.5277e-03,  9.3413e-03,  4.0724e-03,\n",
      "         1.2161e-03, -3.7475e-03,  1.4559e-02, -1.7168e-03, -1.6834e-02,\n",
      "         4.7337e-03, -7.0157e-03,  2.1013e-02, -6.6866e-03,  3.1546e-03,\n",
      "        -1.0994e-02,  1.7319e-03,  3.2442e-04, -3.4965e-02, -5.7034e-03,\n",
      "        -5.6667e-03,  1.2977e-02, -9.7286e-03, -2.3799e-03,  1.3253e-02,\n",
      "        -1.3072e-02, -1.2434e-02, -1.0734e-02, -2.8289e-02,  1.4401e-02,\n",
      "        -1.0905e-02, -2.3394e-03, -1.3205e-03, -1.5662e-02, -8.5313e-03,\n",
      "         5.2590e-04,  3.3820e-04,  3.4392e-03,  8.1345e-03, -1.2431e-02,\n",
      "        -1.4871e-04, -7.0035e-03,  6.3557e-03, -4.2939e-03,  4.1632e-03,\n",
      "        -1.2449e-02,  1.0881e-02,  8.3807e-03,  7.9564e-04, -1.7106e-02,\n",
      "         9.3257e-03,  8.6407e-03,  2.0052e-02,  4.0895e-03,  6.5623e-03,\n",
      "         1.0940e-02, -5.6710e-04, -3.0491e-03, -1.6606e-02,  1.1717e-02,\n",
      "        -4.6807e-04,  3.2119e-02, -7.2470e-03,  1.1796e-02,  4.6300e-03,\n",
      "        -1.2570e-02,  8.4951e-03,  2.8492e-02, -8.0533e-04, -1.6343e-02,\n",
      "        -1.3552e-03,  9.8133e-03,  1.8835e-03,  1.6678e-02,  1.8583e-02,\n",
      "         1.3317e-03, -1.4344e-03,  6.4182e-03,  3.4987e-03,  7.5865e-03,\n",
      "        -4.0388e-05, -8.0150e-03,  2.4828e-03, -1.4623e-02, -2.0635e-03,\n",
      "         4.1772e-03, -9.1693e-03, -4.5215e-03, -8.6524e-03, -2.4011e-04,\n",
      "         8.9609e-04,  2.4451e-02, -6.3448e-04,  1.2267e-02, -9.9236e-03,\n",
      "         8.5235e-03, -5.7059e-04,  1.6843e-02,  1.7038e-03,  9.9088e-04,\n",
      "        -6.8447e-03, -3.9296e-03,  1.9998e-02,  1.6908e-02,  3.2406e-03,\n",
      "        -1.2234e-02, -7.9704e-03,  5.8018e-03, -4.9584e-03, -2.2727e-03,\n",
      "         1.6283e-02, -1.1207e-02,  8.3987e-03,  1.7565e-02,  1.2595e-02,\n",
      "         1.1320e-02, -7.1620e-03, -2.1288e-03, -2.3063e-02,  9.3877e-04,\n",
      "        -5.8744e-03,  6.5477e-03, -1.0435e-02, -5.5942e-03,  8.6705e-03,\n",
      "        -8.7392e-03,  4.5673e-03, -1.2032e-02,  2.4125e-03,  3.6220e-03,\n",
      "        -3.8621e-03,  2.7327e-03, -2.8444e-04, -9.8804e-03, -1.6947e-02,\n",
      "        -1.0655e-02, -4.6420e-03, -3.0339e-03,  2.9081e-03,  2.2685e-02,\n",
      "         1.7758e-02,  9.7641e-03, -6.0981e-03, -6.4396e-03,  5.6442e-03,\n",
      "        -5.6308e-03, -1.4423e-03, -2.0943e-02, -1.4023e-02, -1.5252e-02,\n",
      "        -8.1682e-03,  4.9217e-03, -1.2492e-03,  2.4406e-03, -1.1137e-02,\n",
      "         5.3960e-03,  1.7976e-02, -1.3877e-03,  9.2337e-03,  1.2108e-02,\n",
      "        -1.3557e-02,  1.6004e-02,  1.5231e-02, -3.0663e-03, -9.7451e-03,\n",
      "         1.6631e-02,  2.5035e-03, -7.0480e-03, -1.3763e-03, -7.1599e-03,\n",
      "        -3.8612e-03,  2.4853e-03,  9.8001e-04, -9.1008e-03,  2.6604e-03,\n",
      "         7.5493e-03,  5.2644e-03,  3.7556e-03,  8.1182e-03, -1.3410e-03,\n",
      "         3.1279e-03,  5.5889e-03, -7.4384e-03, -2.7718e-03,  1.3036e-03,\n",
      "         1.3698e-03, -1.6178e-04, -9.9850e-03,  2.2598e-02, -3.4892e-03,\n",
      "         5.9191e-03, -7.8939e-03,  8.8177e-04, -7.4833e-03, -1.2340e-02,\n",
      "        -1.3532e-02,  9.7861e-03, -1.3651e-03, -9.2731e-03,  1.0475e-03,\n",
      "        -1.8854e-02, -1.3841e-02,  1.0701e-02,  6.5119e-03,  7.8662e-03,\n",
      "         2.1283e-02, -1.8052e-02,  9.8938e-04,  1.2737e-02,  2.8561e-03,\n",
      "         9.2882e-03, -1.6863e-02,  2.6511e-03, -1.9473e-02, -9.5574e-03,\n",
      "        -1.3739e-02,  1.8003e-02, -9.2997e-03,  4.1196e-03, -1.6166e-02,\n",
      "         9.2219e-03,  3.3951e-03,  1.4313e-02,  1.0776e-02, -7.2803e-03,\n",
      "         7.2573e-03, -6.1564e-04,  3.1016e-03, -1.8619e-03, -5.3254e-03,\n",
      "         1.0112e-02,  1.5442e-02, -1.4492e-02,  7.2261e-03, -6.9739e-03,\n",
      "         4.0094e-03, -6.9727e-03,  8.3632e-04, -2.3513e-02,  1.8194e-03,\n",
      "        -4.5385e-03, -7.1147e-03])\n",
      "Gradient for layer4.0.norm1.bias:\n",
      "tensor([-2.3375e-02,  1.4378e-02, -1.5133e-02,  1.0337e-02,  2.2086e-02,\n",
      "         2.3850e-03,  2.3298e-03,  1.2583e-02,  1.3237e-03,  1.3369e-02,\n",
      "         6.8817e-03,  6.7560e-03,  7.1020e-04,  7.6966e-03, -4.3542e-03,\n",
      "         6.9477e-03, -2.0313e-03, -4.2451e-03, -1.0340e-02,  9.7164e-03,\n",
      "         6.2159e-04,  8.1004e-03,  1.0971e-02,  4.9120e-03, -3.0949e-02,\n",
      "        -1.3267e-02, -1.2023e-02, -1.2692e-03, -5.8855e-03, -8.0708e-03,\n",
      "         1.4442e-02, -6.7775e-03, -2.7368e-03, -1.0451e-03, -1.7467e-02,\n",
      "         3.6321e-03, -1.0261e-02, -8.4341e-03, -4.7812e-03, -4.8071e-03,\n",
      "        -1.0201e-03,  1.8319e-03,  1.8911e-03,  1.1363e-02, -8.2740e-03,\n",
      "         1.1477e-02, -1.3160e-02, -9.9839e-04, -3.3406e-03, -5.4662e-03,\n",
      "         8.7781e-03,  6.1575e-03,  1.7307e-03, -2.1041e-02, -1.3180e-02,\n",
      "         9.5889e-04,  3.4000e-03, -9.0123e-03,  5.0015e-04,  2.3447e-02,\n",
      "         1.6309e-02,  4.3641e-03, -1.6010e-02, -1.3619e-02, -3.4843e-02,\n",
      "        -1.6215e-02,  2.8679e-03,  2.5017e-03, -2.1382e-02,  1.8359e-02,\n",
      "         1.0256e-02,  8.1211e-03,  1.6738e-03, -5.8969e-03,  1.4005e-02,\n",
      "         2.3495e-02,  3.2980e-03, -1.6623e-03, -9.6278e-03,  8.8907e-03,\n",
      "        -2.0161e-02, -7.1796e-03, -4.3540e-03, -1.1521e-02, -1.4296e-02,\n",
      "        -1.3390e-03,  6.6379e-03, -6.7236e-04, -4.6290e-03,  4.2400e-03,\n",
      "         5.0630e-03,  4.5293e-03, -5.2241e-03,  1.9604e-02,  1.4949e-02,\n",
      "        -3.4138e-03,  1.1588e-02, -1.0408e-02, -9.3232e-04, -4.8069e-03,\n",
      "        -8.4800e-03,  1.4792e-03, -1.3127e-03, -9.1069e-03, -5.9697e-03,\n",
      "         1.0418e-02, -2.8559e-03,  8.0819e-03, -2.1548e-03,  6.0293e-04,\n",
      "         2.1099e-03,  2.2490e-02, -1.6191e-03, -1.9057e-02,  1.9756e-02,\n",
      "         1.0370e-02, -4.1168e-03,  6.3125e-04, -2.3505e-03, -4.3793e-03,\n",
      "         1.5651e-02,  4.1743e-03, -4.7158e-03,  2.9062e-02,  3.0722e-03,\n",
      "        -9.6853e-03,  1.5230e-02, -2.1984e-02,  1.5108e-02,  2.3143e-02,\n",
      "         1.1583e-02, -1.7765e-03,  1.3204e-03, -1.7073e-02, -7.4064e-03,\n",
      "        -1.1442e-02, -2.3747e-02, -8.5461e-03, -1.2171e-02,  1.9375e-02,\n",
      "         6.9125e-03,  1.3534e-02,  1.3252e-02,  4.1561e-03,  1.2147e-02,\n",
      "        -1.5820e-02,  8.0218e-03, -2.1049e-03, -1.0221e-02,  8.7607e-03,\n",
      "         1.6709e-02, -1.5146e-03,  1.6764e-03,  2.5515e-03,  8.4676e-03,\n",
      "         1.3283e-02,  1.9969e-02,  5.9637e-03, -5.9001e-03,  1.3333e-02,\n",
      "        -1.3171e-02, -1.2007e-02, -8.3894e-03,  3.4465e-03, -6.6756e-04,\n",
      "        -3.1955e-03, -2.4854e-03, -1.9886e-02, -1.3762e-02, -1.7185e-02,\n",
      "        -3.1703e-03, -4.8084e-03,  1.3882e-02, -7.0737e-03,  4.4757e-03,\n",
      "        -3.5374e-03,  3.0542e-03, -4.1800e-03,  5.0650e-04,  6.2910e-03,\n",
      "        -3.6288e-03,  2.3233e-04,  7.5550e-03,  1.3018e-02, -1.3436e-02,\n",
      "        -8.2546e-03, -5.2444e-04,  9.6687e-03, -1.3337e-02,  2.1737e-02,\n",
      "         2.0072e-02, -2.2020e-03, -1.4475e-02,  4.0051e-03, -4.6998e-03,\n",
      "         1.4837e-02, -7.1506e-04,  4.0778e-03,  5.2177e-03,  1.8266e-03,\n",
      "        -1.3870e-02,  1.7005e-02,  1.0303e-02, -8.8657e-04,  6.7686e-03,\n",
      "        -1.5950e-02, -1.7108e-03, -7.8398e-03,  2.8993e-03, -9.9990e-03,\n",
      "        -8.8796e-03,  2.2697e-03,  1.1038e-02, -3.0414e-03, -9.8686e-03,\n",
      "         3.4994e-03,  5.6223e-03, -1.6127e-02,  1.8582e-02, -2.0525e-02,\n",
      "        -9.0230e-04,  4.2656e-03, -1.4616e-02,  2.0002e-03, -9.2192e-03,\n",
      "        -2.9363e-04, -1.3253e-04,  4.7783e-03, -8.4909e-03,  6.0848e-04,\n",
      "         8.6758e-04, -1.3611e-02,  4.7657e-04, -2.1610e-03,  9.5582e-03,\n",
      "         2.8019e-03,  5.5659e-03, -4.4314e-03,  2.0658e-03, -1.2403e-03,\n",
      "        -1.7011e-02,  2.2592e-02, -3.5466e-03, -1.1407e-02,  9.1968e-03,\n",
      "        -6.3198e-03, -6.2415e-03, -5.3371e-03,  1.5387e-02,  9.3688e-03,\n",
      "         1.3158e-02, -1.0299e-02, -1.7089e-03,  7.0296e-03, -2.3315e-04,\n",
      "         3.0941e-03, -5.8794e-04, -3.4916e-03,  1.7221e-02,  8.3597e-03,\n",
      "        -1.1150e-02, -2.1088e-03, -1.0150e-02,  3.9841e-03, -7.8783e-03,\n",
      "         5.9758e-03, -4.1473e-03, -3.8780e-03, -3.9618e-03, -6.6017e-03,\n",
      "        -1.2300e-02, -1.5540e-02, -7.0336e-03, -4.0476e-04, -5.4823e-03,\n",
      "         9.5604e-03, -1.6648e-03,  1.3263e-02,  9.2866e-03,  9.9337e-03,\n",
      "         6.3133e-03, -4.3240e-03,  1.9633e-02, -9.1804e-03,  3.9740e-03,\n",
      "        -2.9493e-03,  3.9801e-04,  5.5509e-04, -9.4020e-03, -9.5330e-03,\n",
      "        -8.9624e-03, -7.7875e-05, -4.1715e-03, -4.6910e-03,  8.5498e-03,\n",
      "        -5.5163e-03, -9.6360e-03, -6.2433e-03, -1.9545e-02,  1.3038e-02,\n",
      "        -6.4697e-03,  2.3513e-03,  4.3086e-05, -2.0076e-02, -3.3557e-03,\n",
      "         1.9552e-03, -4.5559e-03,  3.5075e-03, -4.6616e-04, -8.4098e-03,\n",
      "        -4.2994e-03, -2.0584e-02,  1.2529e-02, -4.4266e-03, -5.2325e-03,\n",
      "        -1.7498e-02, -3.0542e-03,  5.7221e-03, -2.6913e-03, -4.4586e-03,\n",
      "         6.1097e-03, -5.2863e-03,  7.7718e-03,  2.3702e-03,  3.5082e-03,\n",
      "         1.3996e-02,  5.4968e-03, -9.5386e-03, -1.2784e-02,  2.5201e-03,\n",
      "        -1.1237e-03,  2.3890e-02, -4.6838e-03,  1.8323e-02,  6.7352e-03,\n",
      "         1.1664e-02,  9.6320e-03,  7.7485e-03,  3.8787e-03, -8.6140e-03,\n",
      "        -1.4774e-02,  1.4040e-02, -5.0525e-03,  1.7921e-02,  1.3284e-02,\n",
      "         4.0514e-03, -6.0809e-03,  9.4912e-03,  1.6657e-03,  4.4996e-03,\n",
      "         1.0778e-02, -1.2699e-02,  4.9449e-03,  3.8507e-03, -4.1883e-03,\n",
      "        -2.8393e-03, -4.8162e-03,  2.2311e-03, -1.3877e-02,  1.0454e-02,\n",
      "         4.5263e-03,  1.5996e-02,  6.0615e-04,  8.5027e-03,  5.6160e-03,\n",
      "         4.0392e-03, -4.9131e-03,  1.4827e-02, -1.1552e-02,  6.8274e-03,\n",
      "        -2.6619e-03,  2.7307e-03,  1.9858e-02,  1.1931e-02,  4.4964e-04,\n",
      "        -7.3811e-03, -5.7087e-03,  5.9962e-04, -1.0443e-02, -9.3947e-05,\n",
      "         8.2249e-03, -1.0029e-02,  7.0742e-03,  1.1824e-02,  2.8803e-03,\n",
      "         1.5887e-02, -7.2416e-04, -9.3735e-04, -1.0448e-02, -9.3865e-03,\n",
      "         1.4512e-04, -1.5948e-03, -9.2025e-03,  1.3077e-04, -1.8761e-02,\n",
      "         1.0913e-02, -7.9541e-03, -1.2264e-02, -8.2484e-04,  1.0180e-03,\n",
      "        -6.1015e-03,  3.4034e-03, -1.6698e-03, -8.1528e-03, -2.4110e-02,\n",
      "        -1.2763e-02, -9.3645e-03,  1.8195e-03, -1.9769e-03,  1.2877e-02,\n",
      "        -3.0418e-03,  1.3402e-02,  1.1698e-03, -7.0711e-03,  2.6957e-03,\n",
      "        -1.2396e-02,  9.2166e-03, -2.9087e-02, -4.3173e-03, -1.3097e-02,\n",
      "        -1.6233e-02, -1.9228e-03,  3.0162e-03, -1.2638e-03,  3.6696e-03,\n",
      "        -1.2121e-02, -5.2888e-03,  3.4384e-04,  6.6350e-03, -5.9471e-04,\n",
      "        -1.7230e-02,  1.6403e-02,  1.2162e-02, -4.8156e-03, -6.5653e-03,\n",
      "         5.0515e-03, -1.0170e-02, -5.3418e-03,  6.6638e-03,  1.3799e-02,\n",
      "         4.5693e-03, -1.3328e-02, -2.2653e-03, -1.0298e-03, -4.1765e-03,\n",
      "        -2.0869e-03,  8.2162e-03,  4.7367e-03,  7.8222e-03, -1.1308e-03,\n",
      "         6.4573e-03,  1.3634e-02, -1.5089e-02, -9.6482e-03,  2.4204e-04,\n",
      "        -1.7448e-03,  2.1631e-03, -5.8334e-03,  2.5109e-02,  5.4066e-03,\n",
      "         1.3761e-03, -4.4894e-03, -1.3866e-02,  2.7183e-04, -1.8918e-02,\n",
      "        -7.2120e-03,  1.3473e-02, -5.5121e-03, -8.1274e-03,  9.1245e-03,\n",
      "        -1.1009e-02, -1.1799e-02,  9.2289e-04,  1.0159e-02,  4.6783e-05,\n",
      "         1.1586e-02,  6.2899e-04,  6.4495e-03,  1.1882e-02, -4.1402e-03,\n",
      "         1.2439e-03,  4.9933e-03, -5.7344e-03, -1.9785e-02, -9.4622e-03,\n",
      "         3.8340e-03,  3.9419e-03, -1.2530e-02,  6.9744e-03, -8.9778e-03,\n",
      "         1.7514e-02,  4.9276e-03,  1.4607e-02,  3.1699e-03, -2.2904e-03,\n",
      "         8.3177e-04, -1.6118e-03,  5.6774e-03,  3.8181e-03, -4.9811e-03,\n",
      "         9.8873e-03,  6.5834e-03, -1.5936e-02,  9.2002e-03, -1.0909e-02,\n",
      "         1.5063e-03, -1.4237e-02, -3.0285e-03, -4.8437e-03, -2.0785e-04,\n",
      "        -2.6307e-03,  3.9645e-03])\n",
      "Gradient for layer4.0.conv2.weight:\n",
      "tensor([[[[ 8.9924e-03, -2.5086e-02, -4.9108e-02],\n",
      "          [-1.7598e-02, -4.8179e-02,  4.5501e-02],\n",
      "          [ 7.0335e-03,  2.6440e-02,  2.3227e-02]],\n",
      "\n",
      "         [[-1.7689e-02, -1.0165e-02,  1.9889e-02],\n",
      "          [ 4.6449e-02, -8.7341e-03,  1.7905e-02],\n",
      "          [ 3.0658e-02,  1.9388e-02,  5.7139e-03]],\n",
      "\n",
      "         [[ 6.8302e-03,  4.4786e-03, -8.7385e-04],\n",
      "          [ 7.1640e-03, -3.3721e-02,  5.2103e-03],\n",
      "          [-7.6701e-04,  3.3160e-02,  1.3886e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5214e-02,  2.0898e-02,  2.2514e-03],\n",
      "          [ 3.7028e-02, -1.8934e-02,  1.2666e-02],\n",
      "          [-1.1295e-02,  2.0759e-02,  8.0449e-03]],\n",
      "\n",
      "         [[-3.8255e-02, -2.9979e-02,  1.2399e-02],\n",
      "          [ 1.4996e-02,  1.9909e-02,  2.0052e-02],\n",
      "          [ 2.8222e-03,  2.4499e-03, -1.7574e-02]],\n",
      "\n",
      "         [[ 5.2744e-03, -4.0389e-02,  2.6812e-02],\n",
      "          [ 1.0749e-02,  1.0936e-03, -1.5940e-02],\n",
      "          [-1.1101e-04, -2.0025e-02, -2.8380e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.4198e-03, -3.1688e-03,  8.0710e-03],\n",
      "          [-4.0861e-04,  1.8894e-02,  6.2785e-04],\n",
      "          [ 1.6486e-02,  4.9880e-03,  1.1240e-02]],\n",
      "\n",
      "         [[-9.8717e-03,  3.3589e-03,  8.8159e-03],\n",
      "          [ 1.0143e-02,  8.2796e-03,  6.9853e-03],\n",
      "          [ 1.4467e-02,  9.1534e-03,  4.8411e-03]],\n",
      "\n",
      "         [[-2.6366e-02, -1.1491e-02, -1.2219e-03],\n",
      "          [ 4.4988e-03, -2.4881e-02,  4.1086e-03],\n",
      "          [ 4.9520e-03,  1.6839e-02,  1.3094e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7816e-03,  1.0682e-03,  1.0247e-02],\n",
      "          [-8.4007e-03,  1.0137e-02,  9.5539e-04],\n",
      "          [ 1.6868e-02, -1.0164e-03,  1.1129e-02]],\n",
      "\n",
      "         [[-3.2111e-05, -1.1059e-02,  1.0213e-02],\n",
      "          [ 2.3392e-03, -7.9078e-03,  3.9171e-03],\n",
      "          [ 1.7150e-03,  1.1257e-03,  3.1485e-03]],\n",
      "\n",
      "         [[-4.0823e-03, -1.7683e-02,  2.9662e-03],\n",
      "          [-1.9406e-03, -7.5698e-03,  8.6046e-03],\n",
      "          [ 5.2696e-03,  6.1371e-03,  2.1359e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.7835e-03,  3.7531e-03,  1.1976e-02],\n",
      "          [-5.1448e-03,  5.6282e-03, -1.5803e-02],\n",
      "          [ 5.5775e-03,  4.5172e-03,  2.0626e-02]],\n",
      "\n",
      "         [[ 4.5162e-03, -1.0937e-02, -1.2819e-02],\n",
      "          [-9.1442e-03, -4.1093e-03, -3.5993e-03],\n",
      "          [ 4.8145e-03, -2.0526e-02,  7.5713e-03]],\n",
      "\n",
      "         [[ 3.9785e-03,  1.0506e-02,  7.0905e-03],\n",
      "          [ 1.5539e-02,  9.0247e-03, -2.0273e-03],\n",
      "          [-1.4022e-02, -2.1211e-02, -7.1322e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1752e-02, -1.4535e-02,  1.7254e-03],\n",
      "          [ 7.0345e-03,  2.1285e-03, -1.5193e-02],\n",
      "          [ 2.0944e-02, -1.6222e-02,  4.0904e-03]],\n",
      "\n",
      "         [[-5.3054e-07, -1.2943e-02, -1.8682e-02],\n",
      "          [ 2.6639e-03,  1.4308e-02,  1.2624e-02],\n",
      "          [ 6.2942e-03,  1.5433e-03,  4.1917e-03]],\n",
      "\n",
      "         [[-1.1588e-02, -3.5245e-03, -1.5287e-03],\n",
      "          [ 1.0679e-02, -2.9989e-03,  1.5823e-02],\n",
      "          [ 8.2785e-03, -1.5433e-04,  4.3481e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.1759e-02,  2.0952e-02,  4.3107e-03],\n",
      "          [ 8.0562e-04, -6.6302e-03, -8.9046e-03],\n",
      "          [ 7.7773e-04, -8.7110e-05,  1.1948e-02]],\n",
      "\n",
      "         [[ 8.7469e-03, -3.2736e-02, -8.2255e-03],\n",
      "          [ 1.6222e-03, -1.1432e-02,  2.4662e-04],\n",
      "          [ 6.0347e-03,  1.2193e-02,  5.5344e-03]],\n",
      "\n",
      "         [[-2.7711e-03, -1.8782e-02, -1.0363e-02],\n",
      "          [ 1.3869e-02, -9.8282e-03, -2.7531e-02],\n",
      "          [-5.0423e-03, -2.3031e-04, -1.3976e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0304e-03, -1.8189e-02, -7.9109e-03],\n",
      "          [ 3.9504e-03, -5.2974e-03, -1.2301e-02],\n",
      "          [ 1.1402e-02, -2.8918e-03, -1.3995e-03]],\n",
      "\n",
      "         [[-2.4271e-03, -7.9404e-03, -1.8148e-02],\n",
      "          [-4.3283e-03,  1.1801e-02, -1.2900e-03],\n",
      "          [ 1.3764e-02,  1.8440e-02, -5.0558e-03]],\n",
      "\n",
      "         [[ 2.5519e-03,  1.6508e-02, -7.2428e-03],\n",
      "          [ 2.2377e-03,  1.9571e-02,  3.7221e-03],\n",
      "          [ 7.4184e-03, -6.0263e-03, -8.8918e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.3805e-02,  5.6185e-03,  4.7468e-02],\n",
      "          [ 4.2344e-03,  4.6892e-02,  2.0281e-02],\n",
      "          [ 2.1611e-02,  7.5689e-03,  4.6555e-02]],\n",
      "\n",
      "         [[ 1.1407e-02, -2.1979e-02,  3.3063e-03],\n",
      "          [ 2.7867e-02,  3.0832e-02, -1.1816e-02],\n",
      "          [ 4.7651e-02, -3.7519e-02,  1.6361e-02]],\n",
      "\n",
      "         [[-2.9556e-02, -4.4462e-02, -2.3570e-02],\n",
      "          [-4.4912e-03,  6.7347e-03, -4.5351e-02],\n",
      "          [ 2.8628e-02,  8.7491e-03,  1.0249e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9213e-03, -2.6203e-03, -5.4511e-03],\n",
      "          [ 2.5220e-02, -6.1487e-03, -3.3510e-02],\n",
      "          [ 5.2816e-02, -3.0036e-02,  2.0371e-02]],\n",
      "\n",
      "         [[ 1.2050e-02,  1.9642e-02, -1.1763e-03],\n",
      "          [ 5.9874e-03,  2.2759e-02, -1.9237e-02],\n",
      "          [ 5.9940e-03, -2.6109e-03,  8.9304e-03]],\n",
      "\n",
      "         [[-2.2057e-03,  2.2574e-02, -1.9741e-02],\n",
      "          [ 9.9710e-03, -5.1028e-02,  3.3853e-02],\n",
      "          [ 1.4011e-02,  8.5622e-03,  2.5127e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4582e-02,  8.4414e-03, -2.9468e-02],\n",
      "          [ 3.7018e-03, -3.7477e-02, -2.2183e-02],\n",
      "          [-1.1061e-02, -1.5681e-02, -1.3649e-02]],\n",
      "\n",
      "         [[-5.4706e-03,  8.7526e-03, -4.4258e-03],\n",
      "          [-8.7695e-03, -6.6867e-03,  8.6303e-03],\n",
      "          [-1.0314e-02,  1.1252e-02, -2.9349e-03]],\n",
      "\n",
      "         [[ 9.3725e-03, -3.6511e-04, -2.3422e-03],\n",
      "          [ 5.2726e-03,  1.7409e-02,  1.6557e-02],\n",
      "          [-1.4936e-02, -8.9473e-03,  6.5028e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3193e-03,  7.0265e-03, -6.7353e-03],\n",
      "          [-4.1579e-03, -9.9148e-03,  1.7471e-02],\n",
      "          [-2.0030e-02,  2.2134e-02, -3.3499e-03]],\n",
      "\n",
      "         [[-9.3245e-03, -6.7754e-03, -1.0873e-02],\n",
      "          [-8.8801e-03, -8.9728e-03,  1.8300e-03],\n",
      "          [ 6.7050e-04,  7.5563e-03,  2.1365e-03]],\n",
      "\n",
      "         [[ 5.9547e-03, -3.1430e-03,  5.4175e-03],\n",
      "          [-2.9809e-03,  1.1120e-02, -1.8175e-02],\n",
      "          [-2.5056e-03,  7.3423e-03, -2.3337e-02]]]])\n",
      "Gradient for layer4.0.norm2.weight:\n",
      "tensor([-2.4582e-02, -6.7708e-03, -1.1381e-02,  1.6921e-02,  3.0970e-02,\n",
      "         9.7564e-03,  6.2647e-03, -1.9483e-02,  8.2954e-03,  1.2261e-02,\n",
      "         4.9451e-03, -1.1387e-02, -2.5903e-03, -3.4264e-02, -1.1488e-02,\n",
      "         1.3061e-02, -3.5077e-02,  9.6550e-03, -1.3122e-02, -1.5787e-02,\n",
      "         6.8528e-03,  2.4272e-02, -1.4320e-02, -4.8136e-03,  2.6804e-02,\n",
      "         7.0359e-03, -7.0960e-03, -1.8495e-02, -1.0573e-02,  4.5674e-02,\n",
      "        -1.8525e-02,  2.0135e-02,  2.4118e-02, -4.7754e-02,  1.8181e-02,\n",
      "        -2.6155e-02, -1.5150e-02,  3.9035e-02,  2.3709e-02, -1.1103e-02,\n",
      "        -4.5313e-02, -2.8804e-02, -1.9761e-02,  6.2066e-03, -1.3815e-03,\n",
      "         2.3155e-02, -1.9549e-02, -1.1632e-02,  1.9996e-02, -1.9189e-02,\n",
      "        -2.0884e-02,  2.5924e-03, -1.9389e-02,  7.3015e-03,  2.5376e-04,\n",
      "        -1.4564e-02,  3.9820e-04, -3.0486e-04,  3.4431e-02, -1.0290e-02,\n",
      "         6.7965e-02, -2.1212e-02, -2.2162e-02,  2.3189e-02, -1.6383e-02,\n",
      "         1.3578e-02,  5.4843e-03, -2.3379e-02,  4.2697e-02, -7.4583e-03,\n",
      "         3.1306e-02,  1.0888e-02,  5.5590e-03,  4.0728e-03, -5.2508e-02,\n",
      "        -1.4987e-02, -2.7533e-02,  3.7961e-03,  1.9099e-03, -6.7011e-03,\n",
      "        -3.2693e-03,  1.7876e-02,  1.1060e-04, -1.3931e-02, -2.5276e-02,\n",
      "        -3.8567e-02, -1.9561e-02,  1.2261e-02, -4.9629e-03, -4.1134e-02,\n",
      "         4.5420e-02, -7.5347e-03, -1.1400e-02, -3.2152e-02,  2.4489e-02,\n",
      "        -3.2334e-02, -6.2593e-03,  4.7741e-02,  4.4137e-02,  1.9841e-02,\n",
      "        -1.0658e-02,  3.3675e-02, -1.5134e-02, -7.7727e-03,  4.8010e-03,\n",
      "        -2.6505e-03, -2.9479e-02, -2.7479e-02, -7.7975e-02, -2.8455e-02,\n",
      "         5.2509e-03,  2.2521e-02,  1.9441e-02, -5.3445e-03, -3.3617e-02,\n",
      "        -1.7885e-02,  3.4483e-03,  1.3853e-02,  3.9516e-02, -3.6901e-03,\n",
      "         1.7517e-02, -2.0110e-02,  1.2066e-03, -1.6372e-03, -1.6721e-02,\n",
      "         6.4545e-03, -2.9190e-03, -6.5006e-03, -1.6646e-03,  5.6900e-03,\n",
      "         4.1660e-03,  8.2698e-03, -3.1275e-02, -4.2147e-02,  1.2607e-02,\n",
      "        -2.2473e-03, -7.0041e-03, -2.0809e-02,  3.4045e-02, -1.0021e-03,\n",
      "        -1.8316e-02,  4.7519e-02, -3.9312e-02,  4.0308e-02, -1.9978e-02,\n",
      "        -4.7408e-03, -1.1709e-03, -2.1169e-02, -2.7091e-02,  1.6776e-02,\n",
      "         4.3295e-02,  2.5692e-02,  1.9949e-02, -6.1317e-02,  1.0475e-02,\n",
      "         2.5914e-02, -2.0430e-02,  3.6259e-02, -2.4883e-03,  9.9085e-03,\n",
      "        -5.9256e-03, -1.6339e-02,  1.5289e-02,  8.6904e-03, -1.5316e-02,\n",
      "         1.8169e-02,  9.6637e-03, -1.3618e-03, -9.2916e-03,  9.0506e-03,\n",
      "         6.6052e-03, -1.5107e-02, -9.0152e-03, -2.0447e-02,  2.8747e-02,\n",
      "         1.4468e-02,  5.4832e-03,  4.9049e-03, -6.9108e-03, -1.1183e-02,\n",
      "        -1.9533e-02, -5.4846e-02, -5.2317e-03,  4.3657e-02, -2.6922e-02,\n",
      "        -7.7040e-03, -2.2258e-02,  7.6701e-03, -4.0166e-03,  1.6879e-02,\n",
      "         2.2400e-02,  1.2149e-02,  1.3135e-02,  5.5190e-03, -2.5772e-03,\n",
      "         1.2320e-02,  2.0310e-02, -2.2863e-02,  5.6226e-03,  4.2366e-02,\n",
      "         4.4933e-02, -2.2665e-02, -1.8457e-02, -8.5623e-04, -1.8091e-02,\n",
      "         3.3988e-02, -1.1177e-03, -4.1059e-02, -5.9951e-02, -1.2809e-02,\n",
      "        -5.4320e-03, -2.2018e-03,  4.6394e-03, -9.4710e-04,  9.0942e-03,\n",
      "         4.8491e-03,  7.3961e-03, -3.1821e-02,  7.2694e-04, -4.4692e-02,\n",
      "         1.1699e-02,  1.9944e-02,  7.9867e-03, -9.2026e-03,  1.7798e-02,\n",
      "         6.8036e-03, -9.6870e-03, -2.9170e-02,  1.0213e-02,  1.6198e-02,\n",
      "        -5.1915e-02,  1.6908e-03, -1.0839e-02,  1.9847e-02,  2.3120e-02,\n",
      "         1.1401e-02,  1.8386e-02,  4.9007e-02,  2.1228e-02,  5.1119e-03,\n",
      "         2.3484e-02,  1.7892e-02, -1.6819e-03,  2.0950e-02, -2.7358e-02,\n",
      "         4.1891e-02,  3.3096e-03, -4.0237e-04,  3.0927e-02,  2.3214e-02,\n",
      "         1.7654e-02,  1.3474e-02, -1.8828e-02, -1.9664e-03,  1.0729e-02,\n",
      "         8.9279e-03, -3.9372e-03,  1.5496e-02,  2.4038e-02,  2.7849e-02,\n",
      "         5.4059e-03,  2.1857e-02,  2.2550e-02,  2.6596e-02,  8.7833e-03,\n",
      "        -1.5050e-02,  3.2203e-02,  1.4647e-02,  3.3031e-02, -2.2312e-02,\n",
      "         4.6798e-03,  4.7878e-02, -1.7023e-03,  8.8588e-03, -2.9286e-02,\n",
      "         1.7469e-02, -4.3780e-03, -1.9366e-02, -4.0930e-02,  3.0640e-02,\n",
      "         8.5545e-03,  2.3246e-02, -1.8699e-02, -7.7544e-03, -1.9562e-02,\n",
      "         2.7835e-02, -6.2354e-03,  1.7505e-02,  2.0885e-02, -1.2819e-02,\n",
      "        -1.0628e-02, -1.8447e-02, -3.9476e-03, -1.5596e-02, -1.9417e-02,\n",
      "         3.9763e-03,  2.9528e-02, -5.0852e-03, -6.2343e-02, -9.6550e-03,\n",
      "         2.8049e-03, -6.4652e-03,  8.0514e-03, -5.9499e-03,  6.7620e-03,\n",
      "         5.2244e-02,  1.2129e-02, -9.1058e-03, -2.4162e-02, -4.8587e-02,\n",
      "         1.9653e-02,  1.4314e-02,  1.3706e-02, -8.2387e-03, -6.4215e-03,\n",
      "        -1.2207e-02,  5.3714e-03,  1.8277e-02, -1.0702e-02,  3.4753e-02,\n",
      "        -3.1814e-02,  2.1023e-02,  4.1160e-02, -3.0962e-03, -4.5176e-02,\n",
      "        -1.2712e-02,  2.0162e-02, -3.3724e-02, -1.6978e-03,  1.5238e-02,\n",
      "        -3.5027e-03,  5.1143e-02, -1.3699e-02, -2.0143e-02,  2.4999e-02,\n",
      "        -2.9515e-02, -2.3881e-02,  1.2747e-02, -2.0662e-03, -5.1962e-03,\n",
      "         9.4293e-03, -2.0980e-02, -2.6557e-02, -2.0405e-02, -9.0596e-03,\n",
      "        -9.9756e-03, -4.2555e-02, -7.9813e-04,  1.4336e-02,  3.1047e-02,\n",
      "         1.9674e-02, -4.2601e-02, -2.1313e-02,  1.5961e-02,  1.5553e-03,\n",
      "        -1.1628e-02,  3.8736e-03,  4.4900e-03, -8.1227e-03,  2.2103e-02,\n",
      "         1.7811e-02,  1.9667e-02,  2.4178e-02, -1.2432e-02,  6.9154e-03,\n",
      "        -1.0835e-02, -7.8562e-03, -1.7771e-02,  1.9831e-03,  5.5216e-03,\n",
      "        -2.4845e-02, -4.7011e-02, -2.4795e-02, -2.0783e-02, -7.3044e-04,\n",
      "        -2.0074e-02, -6.9231e-02,  5.5682e-03,  9.1539e-03, -2.1470e-02,\n",
      "        -4.6730e-03,  3.0260e-02,  1.0145e-03, -3.7136e-02,  2.7929e-03,\n",
      "         1.1825e-02,  1.3909e-03,  4.8219e-02, -6.0288e-04,  2.7197e-02,\n",
      "         5.3476e-03, -3.4538e-02, -6.7989e-03, -2.5578e-02, -5.6672e-04,\n",
      "        -1.4664e-02,  9.2260e-04,  5.2403e-03,  5.4914e-03, -6.1790e-03,\n",
      "         1.5924e-04, -1.1414e-02, -1.1677e-02,  1.7814e-03,  1.5556e-02,\n",
      "         8.6084e-04, -1.3965e-02, -1.1789e-02, -3.5414e-02, -2.5793e-02,\n",
      "        -2.1504e-02, -1.5577e-02,  1.7478e-02,  3.8403e-02, -2.6425e-02,\n",
      "        -4.0043e-04,  6.0867e-02,  1.6074e-03,  2.1487e-02,  1.0448e-04,\n",
      "         3.1333e-02,  5.8751e-03,  8.7383e-03, -6.7202e-03, -1.5597e-02,\n",
      "        -4.7692e-02,  9.4407e-03, -9.4851e-03,  2.4790e-02,  3.0415e-03,\n",
      "         6.9908e-03, -1.0313e-02,  1.2110e-02, -7.8925e-04,  3.6004e-02,\n",
      "         3.6780e-03,  4.8172e-02,  4.3007e-03, -3.8861e-02, -2.1587e-03,\n",
      "         1.1164e-02,  4.2468e-03, -1.1382e-02, -2.2103e-03,  1.5526e-02,\n",
      "        -1.6781e-02, -1.3637e-02,  1.7534e-02, -2.4417e-03,  7.7955e-03,\n",
      "         4.1163e-02, -1.9568e-02, -1.7210e-02,  8.2534e-03, -1.1791e-02,\n",
      "         1.6777e-02,  3.9011e-02, -2.1088e-02,  2.2799e-03,  1.0482e-02,\n",
      "         2.1690e-02,  1.6450e-02,  1.2842e-02, -2.6570e-02, -1.2853e-03,\n",
      "        -4.3069e-02,  1.7582e-02,  2.6679e-02, -2.2872e-02, -2.4791e-04,\n",
      "         1.8381e-03, -2.5016e-02,  2.8873e-03, -4.1491e-02, -2.7366e-02,\n",
      "        -3.3403e-02, -1.6919e-03, -3.5424e-03, -3.8296e-02, -1.5496e-02,\n",
      "        -1.1554e-02,  1.1601e-02, -2.0650e-02, -3.2521e-02, -8.1970e-03,\n",
      "         3.5682e-02,  1.6116e-03,  8.8427e-04,  2.4037e-02, -1.8163e-02,\n",
      "         1.1021e-02, -3.3981e-03,  3.9103e-02,  7.4936e-03,  8.6176e-03,\n",
      "        -1.7203e-03,  1.3689e-02,  8.8510e-03,  5.5586e-04, -2.4188e-02,\n",
      "         8.8797e-03, -2.2607e-02, -1.8529e-02, -1.4915e-02, -2.6213e-02,\n",
      "        -2.1901e-02, -5.9295e-05, -5.1817e-03,  3.0616e-03, -1.3971e-02,\n",
      "         6.3387e-02,  3.0450e-02])\n",
      "Gradient for layer4.0.norm2.bias:\n",
      "tensor([-0.0246, -0.0034, -0.0155,  0.0175,  0.0586,  0.0165,  0.0168, -0.0171,\n",
      "         0.0248,  0.0055,  0.0092, -0.0133,  0.0156, -0.0464, -0.0095, -0.0120,\n",
      "        -0.0498,  0.0130, -0.0372, -0.0231,  0.0219,  0.0317, -0.0116,  0.0281,\n",
      "         0.0426,  0.0089, -0.0050, -0.0785, -0.0155,  0.0544, -0.0272,  0.0219,\n",
      "         0.0560, -0.0414,  0.0434, -0.0402, -0.0180,  0.0477,  0.0233, -0.0090,\n",
      "        -0.0547, -0.0391, -0.0430,  0.0197, -0.0093,  0.0477, -0.0608, -0.0093,\n",
      "         0.0423, -0.0329, -0.0388,  0.0050, -0.0238,  0.0201, -0.0158, -0.0440,\n",
      "        -0.0024, -0.0021,  0.0697, -0.0093,  0.0940, -0.0105, -0.0157,  0.0172,\n",
      "        -0.0149,  0.0130,  0.0080, -0.0430,  0.0723, -0.0088,  0.0424, -0.0048,\n",
      "         0.0070,  0.0090, -0.0559, -0.0180, -0.0400,  0.0116,  0.0073, -0.0075,\n",
      "        -0.0280,  0.0217, -0.0051, -0.0465, -0.0409, -0.0418, -0.0506,  0.0153,\n",
      "         0.0038, -0.0603,  0.0542,  0.0031, -0.0228, -0.0433,  0.0404, -0.0330,\n",
      "        -0.0306,  0.0640,  0.0647,  0.0354, -0.0143,  0.0423, -0.0446, -0.0040,\n",
      "         0.0087, -0.0028, -0.0416, -0.0257, -0.0829, -0.0611,  0.0015,  0.0604,\n",
      "         0.0338, -0.0042, -0.0649, -0.0141,  0.0099,  0.0280,  0.0467, -0.0075,\n",
      "         0.0172, -0.0202,  0.0087, -0.0068, -0.0034, -0.0218, -0.0200, -0.0174,\n",
      "        -0.0041,  0.0019,  0.0024,  0.0132, -0.0429, -0.0583,  0.0232, -0.0076,\n",
      "        -0.0067, -0.0304,  0.0333,  0.0006, -0.0220,  0.0589, -0.0379,  0.0580,\n",
      "        -0.0207, -0.0079,  0.0048, -0.0327, -0.0276,  0.0439,  0.0460,  0.0250,\n",
      "         0.0192, -0.0742,  0.0278,  0.0350, -0.0101,  0.0525, -0.0033,  0.0183,\n",
      "        -0.0055, -0.0343,  0.0216,  0.0350, -0.0519,  0.0644,  0.0185,  0.0059,\n",
      "        -0.0119,  0.0011,  0.0218,  0.0016,  0.0218, -0.0206,  0.0712,  0.0157,\n",
      "         0.0187, -0.0045, -0.0040, -0.0234, -0.0322, -0.0549, -0.0122,  0.0510,\n",
      "        -0.0239, -0.0124, -0.0230,  0.0086, -0.0143,  0.0207,  0.0446,  0.0084,\n",
      "         0.0236, -0.0022, -0.0082,  0.0292,  0.0314, -0.0483,  0.0131,  0.0405,\n",
      "         0.0820, -0.0218, -0.0251, -0.0016, -0.0508,  0.0394, -0.0022, -0.0491,\n",
      "        -0.0629, -0.0404, -0.0127, -0.0443,  0.0122, -0.0031,  0.0028,  0.0069,\n",
      "         0.0281, -0.0348,  0.0266, -0.0755,  0.0025,  0.0294,  0.0160, -0.0082,\n",
      "         0.0139,  0.0154, -0.0066, -0.0299,  0.0186,  0.0255, -0.0618,  0.0016,\n",
      "        -0.0101,  0.0167,  0.0610,  0.0249,  0.0309,  0.0758,  0.0528,  0.0075,\n",
      "         0.0397,  0.0327, -0.0018,  0.0527, -0.0492,  0.0494, -0.0038,  0.0108,\n",
      "         0.0211,  0.0464,  0.0126, -0.0007, -0.0237, -0.0057,  0.0091,  0.0048,\n",
      "         0.0041,  0.0398,  0.0242,  0.0414, -0.0043,  0.0245,  0.0479,  0.0189,\n",
      "         0.0166, -0.0095,  0.0426,  0.0373,  0.0531, -0.0268,  0.0044,  0.0593,\n",
      "        -0.0014,  0.0111, -0.0360,  0.0207, -0.0054, -0.0237, -0.0572,  0.0355,\n",
      "         0.0181,  0.0274, -0.0153, -0.0052, -0.0257,  0.0738, -0.0114,  0.0245,\n",
      "         0.0304, -0.0118, -0.0204, -0.0263, -0.0243, -0.0140, -0.0198,  0.0079,\n",
      "         0.0519, -0.0218, -0.0759, -0.0081,  0.0052, -0.0162,  0.0163, -0.0045,\n",
      "         0.0420,  0.0626,  0.0114, -0.0121, -0.0278, -0.0419,  0.0137,  0.0357,\n",
      "         0.0198, -0.0279, -0.0099, -0.0095, -0.0158,  0.0236, -0.0269,  0.0371,\n",
      "        -0.0431,  0.0364,  0.0774, -0.0059, -0.0352, -0.0249,  0.0466, -0.0224,\n",
      "        -0.0046,  0.0193, -0.0288,  0.0648, -0.0167, -0.0271,  0.0328, -0.0725,\n",
      "        -0.0422,  0.0404,  0.0078, -0.0186,  0.0071, -0.0457, -0.0383, -0.0645,\n",
      "        -0.0117, -0.0018, -0.0599, -0.0023,  0.0154,  0.0710,  0.0265, -0.1079,\n",
      "        -0.0117,  0.0547, -0.0184, -0.0348, -0.0046, -0.0147,  0.0096,  0.0211,\n",
      "         0.0114,  0.0445,  0.0450, -0.0211,  0.0035,  0.0023, -0.0164, -0.0138,\n",
      "        -0.0108,  0.0138, -0.0411, -0.0661, -0.0314, -0.0256,  0.0059, -0.0251,\n",
      "        -0.0886,  0.0046,  0.0156, -0.0155, -0.0023,  0.0355, -0.0066, -0.0460,\n",
      "        -0.0089,  0.0044,  0.0039,  0.0543, -0.0078,  0.0290,  0.0168, -0.0460,\n",
      "        -0.0094, -0.0384, -0.0017, -0.0297,  0.0180,  0.0170, -0.0053, -0.0128,\n",
      "         0.0123,  0.0134, -0.0165,  0.0088,  0.0216, -0.0054, -0.0205, -0.0183,\n",
      "        -0.0584, -0.0444, -0.0382, -0.0144,  0.0307,  0.0597, -0.0439, -0.0095,\n",
      "         0.0795, -0.0144,  0.0372,  0.0027,  0.0475,  0.0141,  0.0060, -0.0205,\n",
      "         0.0002, -0.0965,  0.0218, -0.0103,  0.0235,  0.0157,  0.0256, -0.0168,\n",
      "         0.0316, -0.0074,  0.0347, -0.0037,  0.0762,  0.0076, -0.0559, -0.0146,\n",
      "         0.0234,  0.0034, -0.0077,  0.0482,  0.0246, -0.0188, -0.0296,  0.0166,\n",
      "         0.0110,  0.0094,  0.0410, -0.0318, -0.0274,  0.0167, -0.0011,  0.0042,\n",
      "         0.0614, -0.0444,  0.0027,  0.0142,  0.0392,  0.0374,  0.0249, -0.0367,\n",
      "        -0.0145, -0.0573,  0.0355,  0.0311, -0.0186,  0.0028, -0.0061, -0.0602,\n",
      "         0.0082, -0.0488, -0.0475, -0.0508,  0.0023,  0.0031, -0.0603, -0.0280,\n",
      "        -0.0133,  0.0281, -0.0252, -0.0367, -0.0157,  0.0298,  0.0065,  0.0064,\n",
      "         0.0330, -0.0243,  0.0362,  0.0036,  0.0459,  0.0177,  0.0129,  0.0052,\n",
      "         0.0191,  0.0139, -0.0021, -0.0118,  0.0068, -0.0211, -0.0450, -0.0266,\n",
      "        -0.0481, -0.0327, -0.0134, -0.0112,  0.0093, -0.0130,  0.0733,  0.0171])\n",
      "Gradient for layer4.0.shortcut.0.weight:\n",
      "tensor([[[[ 0.0175]],\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0143]],\n",
      "\n",
      "         [[ 0.0162]],\n",
      "\n",
      "         [[ 0.0259]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         [[-0.0095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0114]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[ 0.0078]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0064]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[-0.0136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         [[-0.0072]],\n",
      "\n",
      "         [[-0.0083]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0236]],\n",
      "\n",
      "         [[ 0.0151]],\n",
      "\n",
      "         [[-0.0201]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0478]],\n",
      "\n",
      "         [[-0.0119]],\n",
      "\n",
      "         [[ 0.0501]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0316]],\n",
      "\n",
      "         [[ 0.0109]],\n",
      "\n",
      "         [[-0.0768]]],\n",
      "\n",
      "\n",
      "        [[[-0.0145]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0137]],\n",
      "\n",
      "         [[ 0.0162]],\n",
      "\n",
      "         [[ 0.0010]]]])\n",
      "Gradient for layer4.0.shortcut.1.weight:\n",
      "tensor([-1.2361e-02, -2.3856e-03, -1.3396e-02, -4.4305e-03,  3.8008e-02,\n",
      "         1.5898e-02,  2.9649e-02, -1.0821e-02,  2.5029e-02,  6.4282e-03,\n",
      "         1.0048e-02, -1.0694e-02,  1.9024e-02, -2.7242e-02, -1.2189e-02,\n",
      "        -1.4590e-02, -3.1691e-02,  4.1573e-03, -2.2286e-02, -9.1934e-03,\n",
      "         1.8838e-02,  2.5286e-02, -1.2312e-02,  3.4159e-02,  2.6161e-02,\n",
      "         5.0128e-03, -2.8588e-03, -6.4128e-02, -9.4306e-03,  4.7589e-02,\n",
      "        -1.9336e-02,  1.1686e-02,  4.7105e-02, -1.2243e-02,  1.1008e-02,\n",
      "        -3.6676e-02, -2.1284e-02,  1.7841e-02,  4.5619e-03,  3.7004e-03,\n",
      "        -4.3918e-02, -1.1174e-02, -1.1507e-02,  1.7847e-02, -1.2215e-02,\n",
      "         4.1183e-02, -4.3748e-02, -6.2069e-03,  1.8905e-02, -7.6391e-03,\n",
      "        -2.3181e-02,  8.2601e-03, -1.9630e-02,  1.8079e-02, -1.1596e-02,\n",
      "        -2.4688e-02, -1.2782e-03, -3.6995e-03,  2.9520e-02, -4.7383e-04,\n",
      "         7.0070e-02, -1.8907e-02, -7.3528e-03,  9.3081e-03, -6.6351e-03,\n",
      "         1.1860e-02,  4.3358e-03, -2.4559e-02,  5.2901e-02, -6.5783e-03,\n",
      "         3.5535e-02, -2.3110e-02, -2.4507e-03,  1.2535e-02, -4.2783e-02,\n",
      "        -1.4228e-02, -3.5582e-02,  8.5403e-03,  7.7401e-03, -5.7402e-03,\n",
      "        -2.7579e-02,  1.6368e-02, -2.3477e-03, -2.7759e-02, -1.8049e-02,\n",
      "        -4.6751e-02, -3.1527e-02,  8.2234e-03,  9.0424e-04, -3.0423e-02,\n",
      "         3.1357e-02,  2.6825e-03, -2.3576e-02, -4.0415e-02,  3.9503e-02,\n",
      "        -3.9255e-03, -2.8139e-02,  4.5029e-02,  2.4647e-02,  2.7544e-02,\n",
      "        -2.3090e-02, -1.3584e-03, -4.4466e-02, -2.3428e-03,  9.3433e-03,\n",
      "        -2.4418e-03, -2.8473e-02, -2.5838e-02, -1.6089e-02, -2.6495e-02,\n",
      "        -4.1696e-03,  3.4007e-02,  1.3069e-02, -5.1658e-03, -4.5487e-02,\n",
      "         5.6310e-03,  1.5405e-02,  1.6289e-02,  3.8408e-02,  4.6056e-03,\n",
      "         6.6842e-03, -2.6304e-02,  6.7180e-03, -4.4839e-03,  5.3397e-03,\n",
      "        -1.7404e-02, -1.5842e-02, -1.8626e-02, -1.1128e-02,  7.0988e-03,\n",
      "        -5.8302e-03,  1.5259e-02, -2.5808e-02, -5.8674e-02,  4.3386e-03,\n",
      "         1.3642e-03, -1.5815e-02, -2.2897e-02,  2.4453e-02,  4.0208e-04,\n",
      "        -1.3930e-02,  1.7577e-02, -6.6273e-03,  3.0350e-02, -4.6267e-03,\n",
      "        -4.5440e-03,  4.5955e-03, -3.4589e-02, -1.6666e-02,  3.4262e-02,\n",
      "         3.9015e-02,  1.0053e-02, -3.4940e-03, -3.9714e-02,  1.7432e-02,\n",
      "         2.5759e-02, -5.8989e-03,  3.6255e-02, -1.1453e-02,  3.0062e-02,\n",
      "         1.3493e-03, -3.9767e-03,  1.5341e-02,  2.3188e-02, -4.1387e-02,\n",
      "         2.3067e-02,  1.0535e-02,  1.2063e-02, -4.0875e-03, -2.8598e-03,\n",
      "         1.3849e-02,  1.0779e-02,  2.0360e-02, -3.0394e-03,  4.7673e-02,\n",
      "         1.7138e-02,  1.6908e-02, -1.0073e-02,  1.5825e-04, -1.2540e-02,\n",
      "        -2.6950e-02, -2.1534e-02, -4.5569e-03,  2.5508e-02, -2.8373e-02,\n",
      "        -1.7206e-03, -1.0764e-02,  4.1885e-04, -3.3869e-03,  2.8387e-02,\n",
      "         2.5927e-02,  1.9110e-03,  1.9904e-02, -4.5221e-03,  1.1484e-04,\n",
      "         2.0196e-02, -1.6771e-03, -4.5040e-02,  1.4729e-02,  3.1080e-02,\n",
      "         3.0811e-02, -2.0238e-02, -2.4848e-02,  4.9380e-03, -1.8557e-02,\n",
      "         3.6264e-02,  1.3200e-04, -3.3403e-02, -4.3714e-02, -2.6114e-02,\n",
      "        -7.2762e-03, -3.6065e-02,  9.7511e-03,  6.6797e-04,  3.0146e-03,\n",
      "         9.3512e-03,  1.6859e-02, -2.9874e-02,  3.2650e-02, -5.1595e-02,\n",
      "        -2.6444e-03,  2.2452e-02,  1.5024e-02, -8.6739e-03, -1.1417e-03,\n",
      "         5.1108e-03, -1.7766e-03, -1.6835e-02,  1.5853e-02,  2.0135e-02,\n",
      "        -9.8856e-03,  7.1202e-04, -8.0596e-03,  9.8039e-05,  1.5813e-02,\n",
      "         1.3052e-02,  2.0781e-02,  1.9728e-02,  3.3147e-02,  1.0445e-02,\n",
      "         2.5629e-02,  9.8633e-03,  8.4631e-03,  3.7588e-02, -2.6363e-02,\n",
      "         1.2142e-02, -1.7146e-03,  7.8070e-03,  1.5881e-02,  3.1539e-02,\n",
      "        -5.9477e-04,  1.6596e-04, -2.9758e-04, -2.8098e-03,  4.3303e-03,\n",
      "        -6.6524e-04,  2.2695e-03,  2.1227e-02,  1.0825e-02,  1.8311e-02,\n",
      "        -9.3758e-03,  7.7945e-03,  2.4899e-02,  9.9165e-03,  1.1903e-02,\n",
      "         1.5241e-03,  3.1824e-02,  2.9529e-02,  4.6162e-02, -2.6613e-02,\n",
      "         3.9668e-03,  3.5274e-02,  5.9447e-03,  8.0569e-03, -1.3343e-02,\n",
      "         1.3985e-02,  3.5176e-03, -3.9311e-03, -4.1845e-02,  3.2755e-02,\n",
      "         8.1220e-03,  1.7932e-02, -3.4667e-02, -2.6103e-03, -1.3333e-02,\n",
      "         6.3678e-02, -5.2428e-03,  2.0907e-02,  1.8400e-02, -7.0832e-03,\n",
      "        -1.6150e-02, -7.9421e-03, -1.5378e-02, -1.6275e-02, -1.1571e-02,\n",
      "         8.4341e-03,  2.2679e-02, -2.3689e-02, -5.9902e-02, -9.5778e-03,\n",
      "         8.4774e-04, -1.5192e-02,  1.1956e-02, -4.7015e-03,  4.5410e-02,\n",
      "         4.4780e-02,  1.1091e-02, -2.3271e-02, -1.9498e-02, -5.6451e-03,\n",
      "         1.4985e-02,  3.9293e-02,  4.4759e-03, -2.4274e-02, -8.5686e-03,\n",
      "        -7.6565e-03, -2.5795e-02,  6.7841e-03, -2.4631e-02,  1.8126e-02,\n",
      "        -2.7988e-02,  2.0380e-02,  5.3378e-02, -1.9666e-03,  9.5413e-03,\n",
      "        -6.8879e-03,  9.1223e-03, -2.3071e-02,  7.5203e-03,  2.1313e-02,\n",
      "        -1.9995e-02,  5.8620e-02, -4.9519e-03, -1.1855e-02,  2.1527e-02,\n",
      "        -5.7937e-02, -3.6408e-02,  2.8475e-02,  3.8852e-03, -1.3403e-02,\n",
      "        -3.6750e-03, -4.5971e-02, -1.1963e-02, -3.5583e-02, -5.3006e-03,\n",
      "         4.8039e-03, -3.1094e-02,  3.5190e-04,  9.0174e-03,  5.7635e-02,\n",
      "         2.2549e-02, -5.9962e-02, -2.0970e-02,  3.9213e-02, -1.6085e-02,\n",
      "        -2.1961e-02, -1.0808e-02, -1.7062e-02,  1.4469e-02,  3.6780e-02,\n",
      "         4.0877e-03,  3.2482e-02,  4.4332e-02, -1.2230e-02,  3.2668e-03,\n",
      "         4.5495e-03, -2.1567e-02,  4.2233e-03, -9.4658e-03,  1.4417e-02,\n",
      "        -3.1859e-02, -3.2807e-02, -1.9922e-02, -6.3067e-03,  1.7846e-03,\n",
      "        -1.3524e-02, -2.0961e-02,  3.4973e-04,  1.5737e-02, -8.0477e-03,\n",
      "        -2.8478e-03,  1.8368e-02,  4.4599e-03, -4.7215e-02, -1.7737e-02,\n",
      "        -1.1762e-02,  1.6338e-03,  3.0223e-02, -3.2492e-03,  2.8503e-02,\n",
      "         4.4795e-03, -3.3767e-02,  8.7666e-04, -2.8523e-02,  5.5490e-03,\n",
      "        -1.7637e-02,  1.7923e-02,  2.0328e-02, -2.4453e-03, -5.8497e-03,\n",
      "         1.0724e-02,  2.5902e-02, -1.0010e-02,  1.3542e-02,  2.2749e-03,\n",
      "         1.5625e-03,  2.3266e-03,  4.7734e-03, -5.2298e-03, -3.8206e-02,\n",
      "        -2.2423e-02, -1.9452e-02,  2.5495e-02,  4.6639e-02, -3.8047e-02,\n",
      "        -6.5683e-03,  2.5305e-02, -1.7008e-02,  2.1819e-02, -4.0307e-03,\n",
      "         2.0260e-02,  4.4541e-03,  1.5363e-03, -2.2017e-02,  6.9640e-03,\n",
      "        -6.2685e-02,  1.7876e-02,  2.9604e-04,  2.2562e-02,  1.5151e-02,\n",
      "         1.6398e-02, -4.8634e-03,  3.6362e-02, -2.3166e-03,  2.0916e-02,\n",
      "        -8.3490e-03,  5.1157e-02,  8.3470e-03, -2.4464e-02, -1.4642e-02,\n",
      "         2.3622e-02,  6.2261e-03,  3.2972e-03,  5.2625e-02,  2.4184e-02,\n",
      "        -1.4276e-02, -1.6600e-02,  9.1547e-03,  1.4955e-02,  2.0917e-02,\n",
      "         3.4324e-02, -2.1557e-02, -1.8329e-02,  5.7651e-03,  8.6241e-03,\n",
      "        -2.7396e-03,  1.3004e-02, -3.0554e-02,  1.4467e-04,  5.5781e-03,\n",
      "         1.4012e-02,  2.0984e-02,  1.5981e-02, -3.3535e-02, -1.2154e-02,\n",
      "        -4.2536e-02,  2.0684e-02,  2.7821e-02, -6.8371e-03,  1.8179e-03,\n",
      "        -5.5441e-03, -6.4384e-02,  2.5858e-03, -4.5514e-02, -4.7298e-02,\n",
      "        -3.6307e-02,  9.7832e-03,  3.5860e-03, -4.8524e-02, -3.4021e-02,\n",
      "        -6.3081e-03,  2.0404e-02, -1.2201e-02, -1.2222e-02, -1.1661e-02,\n",
      "         4.4650e-02,  4.5236e-03,  6.7136e-03, -2.3479e-03, -1.3374e-03,\n",
      "         4.9748e-02,  4.2388e-03,  2.0679e-02,  2.0228e-02,  2.8020e-03,\n",
      "        -1.0525e-02,  4.3855e-03,  7.7723e-03, -5.1933e-03, -3.9048e-03,\n",
      "         3.6680e-03, -1.3898e-02, -3.6662e-02, -2.3977e-02, -1.1725e-02,\n",
      "        -1.6959e-02, -1.0114e-02, -1.0835e-02,  1.2801e-02, -6.1093e-03,\n",
      "         3.4618e-02,  2.2331e-04])\n",
      "Gradient for layer4.0.shortcut.1.bias:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0246, -0.0034, -0.0155,  0.0175,  0.0586,  0.0165,  0.0168, -0.0171,\n",
      "         0.0248,  0.0055,  0.0092, -0.0133,  0.0156, -0.0464, -0.0095, -0.0120,\n",
      "        -0.0498,  0.0130, -0.0372, -0.0231,  0.0219,  0.0317, -0.0116,  0.0281,\n",
      "         0.0426,  0.0089, -0.0050, -0.0785, -0.0155,  0.0544, -0.0272,  0.0219,\n",
      "         0.0560, -0.0414,  0.0434, -0.0402, -0.0180,  0.0477,  0.0233, -0.0090,\n",
      "        -0.0547, -0.0391, -0.0430,  0.0197, -0.0093,  0.0477, -0.0608, -0.0093,\n",
      "         0.0423, -0.0329, -0.0388,  0.0050, -0.0238,  0.0201, -0.0158, -0.0440,\n",
      "        -0.0024, -0.0021,  0.0697, -0.0093,  0.0940, -0.0105, -0.0157,  0.0172,\n",
      "        -0.0149,  0.0130,  0.0080, -0.0430,  0.0723, -0.0088,  0.0424, -0.0048,\n",
      "         0.0070,  0.0090, -0.0559, -0.0180, -0.0400,  0.0116,  0.0073, -0.0075,\n",
      "        -0.0280,  0.0217, -0.0051, -0.0465, -0.0409, -0.0418, -0.0506,  0.0153,\n",
      "         0.0038, -0.0603,  0.0542,  0.0031, -0.0228, -0.0433,  0.0404, -0.0330,\n",
      "        -0.0306,  0.0640,  0.0647,  0.0354, -0.0143,  0.0423, -0.0446, -0.0040,\n",
      "         0.0087, -0.0028, -0.0416, -0.0257, -0.0829, -0.0611,  0.0015,  0.0604,\n",
      "         0.0338, -0.0042, -0.0649, -0.0141,  0.0099,  0.0280,  0.0467, -0.0075,\n",
      "         0.0172, -0.0202,  0.0087, -0.0068, -0.0034, -0.0218, -0.0200, -0.0174,\n",
      "        -0.0041,  0.0019,  0.0024,  0.0132, -0.0429, -0.0583,  0.0232, -0.0076,\n",
      "        -0.0067, -0.0304,  0.0333,  0.0006, -0.0220,  0.0589, -0.0379,  0.0580,\n",
      "        -0.0207, -0.0079,  0.0048, -0.0327, -0.0276,  0.0439,  0.0460,  0.0250,\n",
      "         0.0192, -0.0742,  0.0278,  0.0350, -0.0101,  0.0525, -0.0033,  0.0183,\n",
      "        -0.0055, -0.0343,  0.0216,  0.0350, -0.0519,  0.0644,  0.0185,  0.0059,\n",
      "        -0.0119,  0.0011,  0.0218,  0.0016,  0.0218, -0.0206,  0.0712,  0.0157,\n",
      "         0.0187, -0.0045, -0.0040, -0.0234, -0.0322, -0.0549, -0.0122,  0.0510,\n",
      "        -0.0239, -0.0124, -0.0230,  0.0086, -0.0143,  0.0207,  0.0446,  0.0084,\n",
      "         0.0236, -0.0022, -0.0082,  0.0292,  0.0314, -0.0483,  0.0131,  0.0405,\n",
      "         0.0820, -0.0218, -0.0251, -0.0016, -0.0508,  0.0394, -0.0022, -0.0491,\n",
      "        -0.0629, -0.0404, -0.0127, -0.0443,  0.0122, -0.0031,  0.0028,  0.0069,\n",
      "         0.0281, -0.0348,  0.0266, -0.0755,  0.0025,  0.0294,  0.0160, -0.0082,\n",
      "         0.0139,  0.0154, -0.0066, -0.0299,  0.0186,  0.0255, -0.0618,  0.0016,\n",
      "        -0.0101,  0.0167,  0.0610,  0.0249,  0.0309,  0.0758,  0.0528,  0.0075,\n",
      "         0.0397,  0.0327, -0.0018,  0.0527, -0.0492,  0.0494, -0.0038,  0.0108,\n",
      "         0.0211,  0.0464,  0.0126, -0.0007, -0.0237, -0.0057,  0.0091,  0.0048,\n",
      "         0.0041,  0.0398,  0.0242,  0.0414, -0.0043,  0.0245,  0.0479,  0.0189,\n",
      "         0.0166, -0.0095,  0.0426,  0.0373,  0.0531, -0.0268,  0.0044,  0.0593,\n",
      "        -0.0014,  0.0111, -0.0360,  0.0207, -0.0054, -0.0237, -0.0572,  0.0355,\n",
      "         0.0181,  0.0274, -0.0153, -0.0052, -0.0257,  0.0738, -0.0114,  0.0245,\n",
      "         0.0304, -0.0118, -0.0204, -0.0263, -0.0243, -0.0140, -0.0198,  0.0079,\n",
      "         0.0519, -0.0218, -0.0759, -0.0081,  0.0052, -0.0162,  0.0163, -0.0045,\n",
      "         0.0420,  0.0626,  0.0114, -0.0121, -0.0278, -0.0419,  0.0137,  0.0357,\n",
      "         0.0198, -0.0279, -0.0099, -0.0095, -0.0158,  0.0236, -0.0269,  0.0371,\n",
      "        -0.0431,  0.0364,  0.0774, -0.0059, -0.0352, -0.0249,  0.0466, -0.0224,\n",
      "        -0.0046,  0.0193, -0.0288,  0.0648, -0.0167, -0.0271,  0.0328, -0.0725,\n",
      "        -0.0422,  0.0404,  0.0078, -0.0186,  0.0071, -0.0457, -0.0383, -0.0645,\n",
      "        -0.0117, -0.0018, -0.0599, -0.0023,  0.0154,  0.0710,  0.0265, -0.1079,\n",
      "        -0.0117,  0.0547, -0.0184, -0.0348, -0.0046, -0.0147,  0.0096,  0.0211,\n",
      "         0.0114,  0.0445,  0.0450, -0.0211,  0.0035,  0.0023, -0.0164, -0.0138,\n",
      "        -0.0108,  0.0138, -0.0411, -0.0661, -0.0314, -0.0256,  0.0059, -0.0251,\n",
      "        -0.0886,  0.0046,  0.0156, -0.0155, -0.0023,  0.0355, -0.0066, -0.0460,\n",
      "        -0.0089,  0.0044,  0.0039,  0.0543, -0.0078,  0.0290,  0.0168, -0.0460,\n",
      "        -0.0094, -0.0384, -0.0017, -0.0297,  0.0180,  0.0170, -0.0053, -0.0128,\n",
      "         0.0123,  0.0134, -0.0165,  0.0088,  0.0216, -0.0054, -0.0205, -0.0183,\n",
      "        -0.0584, -0.0444, -0.0382, -0.0144,  0.0307,  0.0597, -0.0439, -0.0095,\n",
      "         0.0795, -0.0144,  0.0372,  0.0027,  0.0475,  0.0141,  0.0060, -0.0205,\n",
      "         0.0002, -0.0965,  0.0218, -0.0103,  0.0235,  0.0157,  0.0256, -0.0168,\n",
      "         0.0316, -0.0074,  0.0347, -0.0037,  0.0762,  0.0076, -0.0559, -0.0146,\n",
      "         0.0234,  0.0034, -0.0077,  0.0482,  0.0246, -0.0188, -0.0296,  0.0166,\n",
      "         0.0110,  0.0094,  0.0410, -0.0318, -0.0274,  0.0167, -0.0011,  0.0042,\n",
      "         0.0614, -0.0444,  0.0027,  0.0142,  0.0392,  0.0374,  0.0249, -0.0367,\n",
      "        -0.0145, -0.0573,  0.0355,  0.0311, -0.0186,  0.0028, -0.0061, -0.0602,\n",
      "         0.0082, -0.0488, -0.0475, -0.0508,  0.0023,  0.0031, -0.0603, -0.0280,\n",
      "        -0.0133,  0.0281, -0.0252, -0.0367, -0.0157,  0.0298,  0.0065,  0.0064,\n",
      "         0.0330, -0.0243,  0.0362,  0.0036,  0.0459,  0.0177,  0.0129,  0.0052,\n",
      "         0.0191,  0.0139, -0.0021, -0.0118,  0.0068, -0.0211, -0.0450, -0.0266,\n",
      "        -0.0481, -0.0327, -0.0134, -0.0112,  0.0093, -0.0130,  0.0733,  0.0171])\n",
      "Gradient for layer4.1.conv1.weight:\n",
      "tensor([[[[ 0.0002, -0.0121, -0.0069],\n",
      "          [-0.0318, -0.0127,  0.0010],\n",
      "          [ 0.0034,  0.0036,  0.0004]],\n",
      "\n",
      "         [[-0.0095,  0.0133,  0.0005],\n",
      "          [ 0.0135,  0.0050, -0.0020],\n",
      "          [-0.0021, -0.0269,  0.0127]],\n",
      "\n",
      "         [[-0.0190, -0.0027,  0.0034],\n",
      "          [-0.0330,  0.0176, -0.0054],\n",
      "          [ 0.0091,  0.0035,  0.0025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0091,  0.0053,  0.0023],\n",
      "          [ 0.0006,  0.0077,  0.0022],\n",
      "          [-0.0037,  0.0044, -0.0092]],\n",
      "\n",
      "         [[-0.0087,  0.0076, -0.0028],\n",
      "          [ 0.0085, -0.0044,  0.0051],\n",
      "          [-0.0101, -0.0038, -0.0175]],\n",
      "\n",
      "         [[ 0.0019,  0.0034,  0.0058],\n",
      "          [ 0.0009, -0.0106, -0.0090],\n",
      "          [ 0.0002, -0.0065, -0.0116]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0158,  0.0207,  0.0236],\n",
      "          [-0.0032, -0.0014, -0.0266],\n",
      "          [-0.0255, -0.0158, -0.0082]],\n",
      "\n",
      "         [[ 0.0036,  0.0050,  0.0116],\n",
      "          [ 0.0146, -0.0085,  0.0061],\n",
      "          [-0.0020,  0.0147,  0.0046]],\n",
      "\n",
      "         [[ 0.0092,  0.0176, -0.0012],\n",
      "          [-0.0019, -0.0008, -0.0274],\n",
      "          [-0.0098, -0.0181, -0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0125,  0.0061, -0.0063],\n",
      "          [ 0.0123,  0.0011,  0.0096],\n",
      "          [ 0.0134, -0.0046,  0.0020]],\n",
      "\n",
      "         [[ 0.0003,  0.0104, -0.0132],\n",
      "          [-0.0054,  0.0174, -0.0021],\n",
      "          [-0.0024,  0.0103, -0.0057]],\n",
      "\n",
      "         [[ 0.0069,  0.0093,  0.0054],\n",
      "          [ 0.0024,  0.0037, -0.0011],\n",
      "          [-0.0070, -0.0038, -0.0094]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0523,  0.0388,  0.0197],\n",
      "          [-0.0083, -0.0590, -0.0583],\n",
      "          [ 0.0291,  0.0309,  0.0184]],\n",
      "\n",
      "         [[-0.0001, -0.0101, -0.0047],\n",
      "          [-0.0068,  0.0161, -0.0087],\n",
      "          [ 0.0065,  0.0085, -0.0287]],\n",
      "\n",
      "         [[ 0.0169, -0.0390,  0.0139],\n",
      "          [ 0.0032, -0.0632, -0.0082],\n",
      "          [ 0.0070, -0.0068,  0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0045, -0.0177, -0.0027],\n",
      "          [-0.0069, -0.0058,  0.0008],\n",
      "          [-0.0145,  0.0060, -0.0028]],\n",
      "\n",
      "         [[-0.0033, -0.0123, -0.0054],\n",
      "          [ 0.0211,  0.0211,  0.0188],\n",
      "          [ 0.0106, -0.0040,  0.0261]],\n",
      "\n",
      "         [[-0.0023,  0.0387,  0.0141],\n",
      "          [-0.0025,  0.0269, -0.0151],\n",
      "          [-0.0020,  0.0310,  0.0308]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0040,  0.0023,  0.0008],\n",
      "          [-0.0126, -0.0115, -0.0208],\n",
      "          [ 0.0102, -0.0149,  0.0187]],\n",
      "\n",
      "         [[ 0.0018,  0.0012, -0.0176],\n",
      "          [-0.0033, -0.0076,  0.0138],\n",
      "          [-0.0117,  0.0073,  0.0011]],\n",
      "\n",
      "         [[-0.0105,  0.0021, -0.0095],\n",
      "          [-0.0067, -0.0206,  0.0041],\n",
      "          [ 0.0067, -0.0258,  0.0388]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0118,  0.0055, -0.0015],\n",
      "          [-0.0059, -0.0007,  0.0002],\n",
      "          [-0.0115,  0.0045, -0.0028]],\n",
      "\n",
      "         [[-0.0037, -0.0043, -0.0004],\n",
      "          [ 0.0099, -0.0209,  0.0333],\n",
      "          [-0.0010, -0.0048, -0.0076]],\n",
      "\n",
      "         [[-0.0002,  0.0017,  0.0032],\n",
      "          [-0.0068,  0.0069, -0.0027],\n",
      "          [-0.0034,  0.0015, -0.0007]]],\n",
      "\n",
      "\n",
      "        [[[-0.0205, -0.0209,  0.0021],\n",
      "          [-0.0208, -0.0318, -0.0063],\n",
      "          [ 0.0473,  0.0317, -0.0198]],\n",
      "\n",
      "         [[-0.0205,  0.0333,  0.0050],\n",
      "          [ 0.0067, -0.0085,  0.0158],\n",
      "          [-0.0026, -0.0427, -0.0053]],\n",
      "\n",
      "         [[-0.0121, -0.0139, -0.0019],\n",
      "          [-0.0069, -0.0081, -0.0277],\n",
      "          [ 0.0229,  0.0013, -0.0261]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0027, -0.0034,  0.0055],\n",
      "          [-0.0024,  0.0173, -0.0048],\n",
      "          [-0.0151, -0.0120, -0.0055]],\n",
      "\n",
      "         [[ 0.0076, -0.0213,  0.0232],\n",
      "          [ 0.0356, -0.0151,  0.0207],\n",
      "          [-0.0068, -0.0100, -0.0080]],\n",
      "\n",
      "         [[-0.0124, -0.0214,  0.0277],\n",
      "          [-0.0052, -0.0452,  0.0199],\n",
      "          [ 0.0084,  0.0035,  0.0050]]],\n",
      "\n",
      "\n",
      "        [[[-0.0018, -0.0026,  0.0142],\n",
      "          [ 0.0091,  0.0386,  0.0378],\n",
      "          [-0.0020, -0.0437, -0.0464]],\n",
      "\n",
      "         [[ 0.0029,  0.0135,  0.0006],\n",
      "          [ 0.0120, -0.0072, -0.0033],\n",
      "          [-0.0241,  0.0034,  0.0292]],\n",
      "\n",
      "         [[ 0.0061,  0.0226,  0.0200],\n",
      "          [ 0.0137,  0.0114,  0.0014],\n",
      "          [ 0.0050, -0.0447, -0.0106]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0018,  0.0125,  0.0018],\n",
      "          [ 0.0081, -0.0025, -0.0012],\n",
      "          [ 0.0009,  0.0088,  0.0009]],\n",
      "\n",
      "         [[ 0.0043,  0.0125,  0.0054],\n",
      "          [-0.0015, -0.0221,  0.0041],\n",
      "          [ 0.0032,  0.0095,  0.0140]],\n",
      "\n",
      "         [[ 0.0067,  0.0011, -0.0020],\n",
      "          [-0.0069, -0.0008,  0.0166],\n",
      "          [-0.0053, -0.0017, -0.0037]]]])\n",
      "Gradient for layer4.1.norm1.weight:\n",
      "tensor([ 4.1913e-04, -2.9098e-03, -5.1458e-03, -2.1754e-03,  6.5962e-03,\n",
      "         2.3185e-02,  3.9862e-03, -2.5257e-03,  4.4154e-03,  6.7011e-03,\n",
      "        -9.7117e-03, -3.7735e-03, -4.5809e-03,  1.1943e-02, -2.9504e-03,\n",
      "        -2.2050e-03, -5.0695e-03, -5.7850e-03,  8.2287e-03,  3.2543e-03,\n",
      "        -4.5375e-03, -1.5160e-03, -1.1635e-02,  4.7639e-03, -6.0013e-03,\n",
      "         1.0024e-02, -4.3705e-03,  3.9528e-03,  8.3369e-03, -5.4230e-04,\n",
      "         4.2964e-03, -4.3138e-03, -1.2959e-04,  1.1688e-03, -3.4183e-03,\n",
      "        -1.1747e-02, -3.2181e-03, -2.3349e-03, -2.7181e-03,  1.3112e-02,\n",
      "        -1.2147e-03, -2.6286e-03, -1.7355e-03,  1.0104e-02, -9.8876e-03,\n",
      "         1.7960e-02,  5.0240e-03,  1.6634e-02,  2.4914e-03, -3.9744e-03,\n",
      "        -4.2387e-03,  1.1539e-02,  2.2601e-03, -4.0766e-03,  1.6018e-02,\n",
      "        -7.8641e-03,  4.0028e-03, -8.4716e-03, -9.6341e-03, -4.5394e-03,\n",
      "         9.9967e-03, -9.0534e-04,  5.4040e-03, -1.7669e-03, -1.3852e-02,\n",
      "        -3.2067e-03, -1.8990e-03,  4.1874e-03, -6.3813e-03, -1.9068e-03,\n",
      "         3.9516e-03, -2.8693e-03,  5.8831e-03, -2.4554e-03, -1.4681e-02,\n",
      "        -8.6575e-03, -1.0931e-03, -6.9152e-03,  2.9157e-03, -8.3992e-03,\n",
      "         1.0226e-02,  5.7846e-03, -1.0222e-02, -1.2752e-03, -2.3095e-03,\n",
      "        -1.7269e-03,  9.4066e-03,  1.0085e-02, -1.4515e-02, -9.6824e-03,\n",
      "         8.2531e-03, -3.3899e-03, -5.6162e-04, -1.2662e-02, -3.8420e-03,\n",
      "        -2.8102e-03,  6.4261e-03, -9.3666e-03,  1.0511e-03, -9.4174e-03,\n",
      "         5.7198e-03, -7.0504e-03,  2.4396e-03, -6.6085e-03, -8.1177e-03,\n",
      "        -1.9129e-03,  1.3595e-03, -2.7275e-03,  6.3683e-03, -8.8540e-03,\n",
      "         3.4440e-03, -5.1056e-03,  1.0914e-02,  1.3229e-02,  6.0585e-03,\n",
      "        -4.9315e-03, -4.7944e-03, -1.0507e-02, -1.1902e-02, -4.0180e-03,\n",
      "         9.7878e-03, -7.7504e-03, -1.0083e-02, -3.8660e-03,  6.1502e-03,\n",
      "         9.7598e-03,  2.7193e-04,  1.0261e-02, -1.5803e-05,  1.1049e-03,\n",
      "         1.1544e-03,  4.3187e-03, -6.1950e-03, -3.4263e-03, -4.8696e-03,\n",
      "        -8.8358e-03,  2.5298e-03, -1.7503e-03, -5.1051e-03,  2.3112e-03,\n",
      "         3.4017e-03,  8.3346e-03,  5.6059e-03,  1.6520e-03,  1.2397e-02,\n",
      "         5.7860e-03,  9.4463e-03, -2.7765e-03,  2.3388e-03, -1.6826e-03,\n",
      "        -7.6521e-03, -1.0138e-02,  2.7979e-03,  6.4421e-03,  4.0457e-03,\n",
      "         9.3831e-04,  7.2448e-03, -1.0644e-02, -1.1001e-03,  6.2387e-03,\n",
      "         1.1886e-02,  1.2781e-03, -1.8253e-02,  6.6865e-03, -8.3694e-03,\n",
      "         1.0371e-02, -1.8646e-03,  7.1190e-03, -2.3772e-03, -1.4949e-02,\n",
      "        -1.0904e-02, -2.9913e-05, -1.2297e-03, -1.1556e-02,  5.0476e-03,\n",
      "        -5.9148e-03,  2.6193e-03,  6.1588e-03, -9.4812e-03,  4.7237e-03,\n",
      "        -5.6495e-03, -6.5862e-03,  1.8593e-03, -7.4785e-03, -9.1819e-03,\n",
      "         9.6488e-03, -4.9166e-03,  1.0814e-02,  2.2358e-04,  2.4614e-03,\n",
      "         2.8831e-03, -3.4709e-03,  7.8915e-03, -9.9850e-03, -7.7515e-04,\n",
      "         4.9880e-04,  2.7963e-03, -8.5040e-03,  3.8190e-03, -1.2966e-03,\n",
      "        -3.6013e-03, -2.2830e-03,  1.1709e-02,  4.1146e-03, -6.2665e-03,\n",
      "         1.7872e-03, -2.9779e-03,  7.5505e-03,  1.5146e-04, -5.2269e-03,\n",
      "         5.1971e-03, -8.5352e-03, -1.3522e-02,  2.4805e-03,  9.5864e-03,\n",
      "        -1.9600e-03, -1.8195e-02, -1.9058e-02, -1.4633e-02,  2.0909e-03,\n",
      "        -3.7455e-03,  1.0374e-03, -1.2427e-02, -9.1239e-03,  1.9874e-03,\n",
      "        -4.5509e-03,  5.8908e-03, -4.8158e-03,  1.3849e-02,  6.9301e-03,\n",
      "        -6.1369e-04, -5.4016e-03,  3.9573e-03,  7.9682e-03, -7.4810e-03,\n",
      "        -6.5638e-03, -3.3552e-03,  1.1095e-03,  7.7785e-03,  1.3617e-02,\n",
      "        -6.2830e-03,  1.4400e-03, -3.9740e-03,  1.1043e-03,  4.1634e-04,\n",
      "        -1.0391e-02,  3.4712e-03, -2.5295e-02,  5.5877e-03,  8.0179e-03,\n",
      "        -1.1320e-03,  1.0646e-02,  3.6773e-03,  4.9972e-03, -1.6740e-02,\n",
      "        -3.0080e-03, -1.7504e-03, -8.5399e-03, -7.3603e-04,  9.6043e-04,\n",
      "         7.9974e-03, -5.7805e-03, -7.8786e-03, -2.5293e-03,  4.7830e-03,\n",
      "         2.6772e-03, -4.3143e-04,  7.3357e-03,  2.9952e-03, -5.4653e-03,\n",
      "         1.8889e-02,  6.6018e-03,  8.0909e-03,  3.8875e-03, -3.3769e-03,\n",
      "         4.4835e-03,  1.9249e-02,  8.9872e-03, -2.8080e-03,  3.7671e-03,\n",
      "        -4.9721e-03,  2.1375e-03,  5.6985e-03, -7.5787e-03, -1.8941e-02,\n",
      "         1.4574e-02, -3.2243e-04,  1.8961e-03, -2.1843e-03, -1.0645e-03,\n",
      "         3.0353e-03, -1.4833e-02,  9.4219e-03,  2.6538e-03,  4.4020e-04,\n",
      "         1.5408e-02, -9.6708e-03,  1.7523e-03,  9.9281e-04,  4.3382e-03,\n",
      "         3.8916e-03, -3.4903e-03,  1.2401e-02,  3.0954e-03, -1.6618e-03,\n",
      "        -1.2375e-02, -1.2482e-02,  7.8133e-03, -2.0310e-03,  1.9956e-02,\n",
      "         7.9305e-03,  6.5085e-04,  5.1729e-03, -1.9661e-03, -1.5120e-02,\n",
      "         8.0189e-03, -2.5101e-03,  5.5636e-03, -1.6406e-02, -3.9424e-03,\n",
      "         7.4409e-03,  2.5643e-03,  1.5703e-04, -2.9762e-03, -1.0076e-03,\n",
      "        -6.9823e-03,  5.2049e-03,  4.8376e-03,  1.8287e-03,  3.6512e-03,\n",
      "         3.0239e-03, -2.5466e-03, -7.7775e-03, -1.5579e-02,  3.5487e-03,\n",
      "        -1.2146e-02, -9.4240e-03,  1.4580e-03, -1.7615e-03, -6.8647e-03,\n",
      "         2.0127e-02,  9.8538e-04,  9.7647e-04, -6.8851e-03,  7.4748e-03,\n",
      "         9.9876e-03, -9.7245e-03,  9.5653e-03,  8.7767e-03,  5.7938e-03,\n",
      "         1.1043e-02,  7.1560e-03,  1.8843e-03,  4.4598e-03, -8.4316e-03,\n",
      "        -3.7278e-03,  4.1783e-04,  8.5838e-03, -3.1340e-03, -8.8611e-03,\n",
      "         6.8077e-03,  9.7215e-03, -1.5145e-03,  8.8857e-03,  2.7239e-02,\n",
      "         1.5069e-04, -3.2579e-03,  1.0470e-02, -2.6486e-03,  1.5375e-03,\n",
      "         2.2857e-03,  5.1864e-03,  1.0242e-02, -7.0234e-03,  8.0272e-03,\n",
      "         2.9799e-03,  2.2046e-04,  3.6139e-03,  8.6380e-03,  1.3539e-03,\n",
      "         4.9729e-03,  1.3427e-02, -4.6088e-03,  1.3840e-03, -9.4218e-03,\n",
      "         1.0242e-02, -1.0203e-02,  1.1473e-02,  1.9648e-03,  7.4658e-03,\n",
      "         1.7728e-02, -2.6392e-04,  1.4068e-03,  5.3740e-03, -9.0856e-03,\n",
      "        -2.9901e-03,  9.3463e-03, -2.4265e-02, -2.9042e-03, -5.2186e-03,\n",
      "         2.5332e-02,  4.2660e-04,  4.0277e-03, -6.3845e-03,  9.3306e-03,\n",
      "        -2.8823e-03, -3.4294e-04, -9.5773e-03, -1.2653e-03,  1.3755e-02,\n",
      "        -1.1556e-03, -1.4953e-03, -3.1444e-03,  5.0852e-03, -5.9366e-03,\n",
      "        -2.2820e-03, -1.2591e-05,  6.6133e-03,  1.6242e-02, -3.0824e-03,\n",
      "        -6.4006e-03, -6.0413e-04,  1.3804e-02, -4.9867e-03, -2.1588e-02,\n",
      "        -4.1881e-03,  2.8984e-04, -2.8178e-03,  3.9120e-03, -5.3371e-03,\n",
      "         3.0980e-03, -6.6057e-04,  1.0665e-03, -2.0782e-02,  6.7926e-03,\n",
      "        -1.1257e-02,  5.5185e-03,  6.6103e-03,  6.4514e-03,  6.2533e-04,\n",
      "        -8.4107e-03, -6.9915e-03,  8.9781e-03, -3.6387e-03, -3.7473e-03,\n",
      "        -5.1235e-05,  5.5578e-03,  3.3192e-03,  1.2370e-03,  7.7973e-03,\n",
      "         1.0793e-03, -1.9704e-03,  8.2029e-03,  5.5675e-03, -1.2845e-02,\n",
      "         5.3332e-03, -1.2481e-02,  2.3477e-04,  2.6191e-03, -1.0931e-02,\n",
      "         2.5909e-03,  5.3925e-03, -4.2714e-03, -7.3251e-04,  4.4266e-03,\n",
      "        -4.3790e-03, -8.1473e-03,  7.0273e-04, -1.2474e-02,  1.5490e-03,\n",
      "        -1.5052e-02, -2.3225e-03, -2.5143e-03, -3.1260e-03, -2.1241e-03,\n",
      "         1.0708e-03,  2.8072e-03,  8.8822e-03,  2.1439e-02,  4.8952e-03,\n",
      "        -8.8937e-03, -5.1686e-03, -8.3042e-03,  7.5146e-04,  5.4823e-04,\n",
      "         1.4750e-02,  5.8983e-03,  5.2183e-03, -4.1189e-03, -1.7041e-03,\n",
      "         2.1671e-03,  8.0928e-03, -2.2475e-03, -1.7677e-02, -8.7735e-03,\n",
      "        -9.8373e-04,  3.7056e-03,  1.3449e-03, -1.2500e-02, -7.4809e-04,\n",
      "         9.6742e-04,  3.8926e-03, -1.5551e-02,  4.9934e-03, -2.2975e-04,\n",
      "        -6.4013e-03, -2.8282e-03, -7.3678e-03,  3.6473e-04, -3.7479e-03,\n",
      "        -4.7673e-03,  4.3728e-04])\n",
      "Gradient for layer4.1.norm1.bias:\n",
      "tensor([-3.9222e-03,  9.0458e-04,  3.9190e-03, -6.8182e-03,  3.9641e-03,\n",
      "         2.2676e-02, -3.0957e-03, -1.1523e-03,  4.4290e-03, -1.2994e-03,\n",
      "        -8.9874e-03,  1.3111e-03,  1.4146e-04,  6.8880e-03, -8.4122e-03,\n",
      "        -1.5163e-03, -1.2533e-02,  2.0442e-05,  1.1596e-02,  3.3874e-03,\n",
      "        -4.7906e-03,  2.9002e-04, -1.0676e-02,  8.3112e-03, -1.1676e-02,\n",
      "         1.0646e-02,  6.9133e-04, -1.6957e-03,  9.4243e-03, -4.8922e-03,\n",
      "         6.4366e-03, -7.9448e-03,  3.4821e-03,  1.2545e-02, -5.3248e-03,\n",
      "        -6.2321e-03,  7.9539e-04, -1.9626e-03, -7.7081e-03,  6.0937e-03,\n",
      "        -2.7671e-03,  3.1888e-03, -2.5660e-03,  2.4670e-03, -8.3674e-03,\n",
      "         4.5152e-03,  1.9201e-03,  1.4535e-02, -3.0687e-03, -9.4263e-03,\n",
      "        -1.5738e-03,  8.3916e-03,  4.0174e-03, -9.0517e-03,  4.3849e-03,\n",
      "        -3.0380e-06,  7.4556e-03, -1.5050e-02, -5.8625e-03, -1.7711e-03,\n",
      "         2.6523e-03, -1.7694e-03,  4.3758e-03, -5.2456e-03, -1.0488e-02,\n",
      "        -7.1943e-03, -2.3310e-03,  2.2814e-03, -7.0987e-03, -4.6644e-03,\n",
      "        -1.8267e-03,  2.4031e-06,  8.0659e-03, -3.3985e-03, -1.1895e-02,\n",
      "        -4.5174e-03,  5.6317e-03, -7.9529e-03, -8.4139e-04, -6.7082e-03,\n",
      "         5.0015e-03,  3.7151e-03, -6.8337e-03,  1.4254e-03,  8.0873e-03,\n",
      "        -4.1852e-03,  9.0724e-03,  9.9184e-03, -7.3530e-03, -3.6328e-03,\n",
      "         1.3776e-02,  2.7744e-04,  3.5323e-03, -4.6173e-03,  4.2043e-03,\n",
      "         9.9954e-05,  7.6444e-03, -8.7088e-03,  2.2429e-03, -5.9813e-03,\n",
      "        -2.4769e-03, -9.4117e-03,  3.9440e-03,  4.0570e-03,  3.0037e-03,\n",
      "        -3.7050e-04,  3.8770e-03, -4.7749e-03,  7.0692e-03, -4.0355e-03,\n",
      "         4.5018e-03, -1.0007e-02,  1.2784e-02,  1.5599e-02,  1.7012e-03,\n",
      "        -1.7970e-03, -7.1278e-04, -2.1659e-03, -2.3412e-04, -3.6602e-03,\n",
      "         6.9992e-03, -1.0074e-02, -5.1079e-03, -2.0758e-03,  2.0836e-03,\n",
      "         1.0333e-02, -6.5457e-03,  1.2149e-02,  7.6736e-03,  4.0393e-03,\n",
      "         4.6218e-03,  5.1953e-03, -6.9180e-03, -1.4943e-03,  4.4635e-03,\n",
      "        -8.9282e-03,  3.1145e-03, -1.2798e-04, -8.6311e-03, -4.2519e-03,\n",
      "        -1.5271e-03,  9.8525e-03,  5.8924e-04,  1.2274e-03,  5.0047e-03,\n",
      "         1.9640e-03,  2.2521e-03, -8.2225e-03,  7.4380e-03, -9.4647e-04,\n",
      "         1.0676e-03, -6.8279e-03,  6.7579e-04,  3.7011e-03,  5.9474e-03,\n",
      "         8.3698e-04,  7.4327e-03, -1.0747e-02, -5.6171e-03,  1.1235e-03,\n",
      "         1.2154e-02, -2.8016e-03, -1.5075e-02,  6.2217e-04, -9.6066e-03,\n",
      "         1.2326e-02, -1.0659e-03,  7.1763e-03, -7.0002e-03, -4.4037e-03,\n",
      "        -1.8484e-03,  5.2432e-04, -2.9160e-03,  6.8697e-03,  9.5234e-03,\n",
      "        -6.4906e-03,  1.2941e-02,  4.9992e-03, -8.6991e-03,  2.7779e-03,\n",
      "        -3.5564e-03, -2.5214e-03,  4.6418e-04, -7.3335e-03, -4.0210e-03,\n",
      "         1.0207e-02, -6.4463e-03, -3.9807e-04,  1.3973e-03,  1.4144e-04,\n",
      "         1.0757e-03, -1.0216e-02,  7.1678e-03, -9.4823e-03, -4.5945e-03,\n",
      "        -7.2659e-06,  5.4768e-03, -6.4762e-03,  1.2693e-02, -5.9286e-04,\n",
      "         1.8630e-03, -4.3758e-03,  7.0538e-03,  3.6292e-03, -2.2291e-03,\n",
      "        -1.7691e-03, -3.4249e-03,  2.7239e-03,  6.5404e-04,  2.9856e-03,\n",
      "         1.4956e-04, -5.2178e-03, -7.7992e-03,  4.9878e-03,  1.1830e-02,\n",
      "        -4.3877e-03, -1.3360e-02, -1.6207e-02, -1.2636e-02,  6.1583e-03,\n",
      "         2.6637e-03, -1.2037e-02, -1.5205e-02, -6.1442e-03, -3.4137e-03,\n",
      "        -1.1128e-02,  3.4359e-03,  2.4550e-03,  7.0787e-03,  6.1111e-03,\n",
      "        -8.7291e-03,  1.4033e-03, -1.3079e-03, -7.8851e-04, -9.6792e-03,\n",
      "        -6.0883e-03, -5.2474e-03,  7.2768e-03,  9.5720e-03,  1.2025e-02,\n",
      "        -9.9416e-03, -2.0801e-03,  2.5788e-03,  7.7400e-03,  5.2096e-03,\n",
      "        -3.9907e-03, -3.9913e-03, -1.7530e-02,  9.9803e-03,  2.0131e-03,\n",
      "        -1.7051e-03,  3.6729e-03,  3.7024e-03,  4.5697e-03, -3.8718e-03,\n",
      "        -2.6360e-03,  1.9204e-03, -6.8700e-03,  1.9133e-03,  1.7025e-03,\n",
      "        -9.0763e-04, -1.4192e-04, -1.7078e-03,  4.5309e-03,  8.4186e-03,\n",
      "         3.8603e-03,  5.7539e-03,  3.9528e-03,  1.0625e-03, -1.4631e-02,\n",
      "         1.1425e-02,  6.8557e-04,  5.2455e-03,  5.3401e-03,  1.9963e-03,\n",
      "        -1.8056e-04,  1.0151e-02,  6.3372e-03, -7.7248e-04,  2.8695e-03,\n",
      "        -1.4285e-03,  2.6227e-03,  1.0699e-02, -1.0827e-02, -1.9316e-02,\n",
      "         5.3305e-03, -4.6571e-03, -2.0581e-03, -6.5195e-03, -3.3791e-03,\n",
      "         8.5835e-04, -1.3361e-02,  3.8446e-03,  2.8751e-03,  6.1223e-03,\n",
      "         2.9138e-03, -3.5501e-03,  4.3749e-03,  1.6998e-03,  5.3734e-03,\n",
      "         4.3692e-03, -1.0802e-03,  1.2308e-02,  3.7289e-03, -3.6642e-03,\n",
      "        -1.0503e-02, -6.5264e-03,  2.3246e-03,  2.6961e-03,  1.7891e-02,\n",
      "         4.2058e-03, -4.9036e-03,  5.5389e-03, -6.4757e-03,  2.6645e-03,\n",
      "         1.1230e-02,  1.0260e-02,  4.1045e-03, -1.1348e-02, -5.4101e-03,\n",
      "         1.1094e-02,  3.9187e-04,  4.1149e-03, -7.6939e-03,  2.7193e-04,\n",
      "        -3.1306e-03,  8.3506e-03,  7.9054e-03, -2.8161e-04, -1.4512e-03,\n",
      "        -3.5259e-04, -3.0236e-03, -3.9088e-03, -1.8838e-03, -2.0641e-03,\n",
      "        -7.5141e-03, -6.6477e-03, -1.1644e-02, -6.4140e-04, -6.7213e-03,\n",
      "         1.0697e-02,  2.8467e-03,  4.1307e-03, -6.8617e-03,  2.2112e-03,\n",
      "         3.9284e-03, -1.0151e-02,  1.2256e-02,  2.3753e-04,  4.1352e-03,\n",
      "         8.9392e-03,  8.9940e-03, -4.7593e-05,  5.0571e-03, -9.3305e-03,\n",
      "        -1.1364e-03, -2.1333e-03,  1.9908e-03, -4.4910e-03, -8.1962e-03,\n",
      "         7.9960e-03,  1.8316e-03,  3.8448e-03, -3.9069e-04,  1.3129e-02,\n",
      "         2.7196e-03,  3.1278e-03,  4.2511e-03, -9.3568e-04,  1.0576e-03,\n",
      "        -1.8909e-03,  5.1657e-04,  1.2983e-02, -8.1047e-03, -2.5983e-03,\n",
      "         4.2736e-04, -1.7110e-03,  7.4155e-03, -1.0557e-03,  2.7917e-04,\n",
      "        -3.0607e-03,  1.1137e-02, -2.4844e-03, -1.9550e-03, -2.1747e-03,\n",
      "         2.9598e-03, -1.4812e-02, -4.1074e-04,  1.1072e-03,  1.7799e-04,\n",
      "         1.7133e-02, -3.5671e-03,  5.2531e-04,  5.3908e-03,  3.6374e-03,\n",
      "         4.2206e-04,  3.7408e-03, -6.1696e-03,  4.4652e-04,  6.6437e-04,\n",
      "         4.9872e-03, -3.3681e-03,  8.5958e-03,  1.4306e-04,  5.6103e-03,\n",
      "        -8.3394e-03, -1.3093e-03,  9.9438e-04,  4.4450e-04, -7.7997e-04,\n",
      "        -6.3090e-04, -3.8261e-03, -2.8278e-03,  7.9117e-03, -1.1926e-02,\n",
      "         2.3299e-03,  9.4385e-03, -5.5996e-04,  1.5980e-02,  2.9853e-03,\n",
      "        -6.5121e-03, -1.0157e-03,  7.6555e-03,  4.4844e-04, -1.6234e-02,\n",
      "        -5.5901e-03, -5.5123e-04,  2.7518e-03,  5.0421e-03, -3.0998e-03,\n",
      "         2.4556e-03, -3.2021e-03,  6.1512e-03, -5.0857e-03, -2.4372e-03,\n",
      "        -8.3968e-03,  2.2088e-03,  7.7251e-03,  6.9026e-03, -4.4882e-03,\n",
      "        -9.6739e-03, -7.6339e-03,  4.7289e-03, -3.1987e-03,  3.4153e-03,\n",
      "         5.3925e-03, -4.8782e-03,  2.3626e-03,  6.6006e-03,  7.0509e-03,\n",
      "         4.8414e-03, -3.6484e-03,  3.3901e-03, -9.7903e-04, -4.3906e-03,\n",
      "         4.7537e-03, -8.8912e-03, -3.5663e-03,  9.9815e-04, -1.8834e-03,\n",
      "         8.4814e-03,  5.7598e-03, -6.3865e-03, -2.4861e-03,  8.3381e-03,\n",
      "         2.5662e-03, -5.7286e-03, -1.4799e-03, -5.6185e-03, -2.6333e-05,\n",
      "        -1.0901e-02, -5.0636e-03, -2.5642e-03, -1.7082e-03, -7.1685e-03,\n",
      "        -2.1624e-03,  1.2714e-03, -5.1891e-04,  2.1713e-02,  5.9059e-03,\n",
      "        -1.9300e-03, -3.5131e-03, -1.2287e-02, -2.3430e-03, -3.5146e-04,\n",
      "        -1.8819e-03,  9.3216e-04,  4.9786e-03, -7.4473e-03,  9.1429e-04,\n",
      "         1.1416e-03,  3.4674e-03, -5.0966e-03, -1.0045e-02, -1.3161e-02,\n",
      "         8.1928e-04, -2.4440e-03,  1.4349e-03, -5.0477e-03,  4.7014e-05,\n",
      "        -2.8327e-03,  1.2288e-03, -1.3113e-02, -1.7944e-03,  1.1186e-04,\n",
      "        -5.4285e-03, -1.8140e-03, -7.9243e-03, -5.7528e-03,  4.4393e-05,\n",
      "         1.5077e-03,  2.4320e-03])\n",
      "Gradient for layer4.1.conv2.weight:\n",
      "tensor([[[[-8.8955e-03,  1.3484e-02,  2.2690e-02],\n",
      "          [-1.7453e-02, -1.2813e-03,  1.2616e-02],\n",
      "          [ 8.2883e-03,  8.4857e-05,  1.0544e-03]],\n",
      "\n",
      "         [[-4.8417e-03, -1.6812e-03,  5.7220e-05],\n",
      "          [-2.9063e-04,  2.7645e-02,  2.4078e-02],\n",
      "          [-8.0637e-03,  1.0307e-03,  6.3263e-03]],\n",
      "\n",
      "         [[ 7.8747e-03,  2.1781e-02,  1.1642e-02],\n",
      "          [-9.6161e-03, -1.0910e-02, -1.0868e-03],\n",
      "          [-5.6834e-03, -2.2967e-02, -1.1015e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1043e-02, -5.1242e-03,  7.5958e-03],\n",
      "          [-1.4792e-02,  2.7874e-03,  1.1960e-02],\n",
      "          [ 2.4931e-03,  2.1503e-02,  9.4786e-03]],\n",
      "\n",
      "         [[-1.6466e-02, -9.8519e-03,  1.9092e-04],\n",
      "          [ 1.5024e-04,  2.9115e-03, -5.6920e-03],\n",
      "          [ 9.5334e-03,  9.0951e-03, -1.0836e-03]],\n",
      "\n",
      "         [[ 8.3324e-04,  1.3641e-02,  6.4509e-03],\n",
      "          [ 2.5943e-04, -2.1425e-02, -1.4070e-02],\n",
      "          [ 7.6356e-03, -1.4418e-02, -8.9274e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.3661e-04,  2.5353e-03,  4.3531e-03],\n",
      "          [-1.3326e-03,  2.3050e-04, -5.5455e-03],\n",
      "          [ 4.4681e-04,  1.3636e-03,  4.4648e-03]],\n",
      "\n",
      "         [[ 8.5534e-04, -8.1946e-05, -1.4307e-03],\n",
      "          [ 6.4888e-04,  1.4819e-05,  1.5436e-03],\n",
      "          [-1.2157e-03,  3.6032e-03,  1.0476e-04]],\n",
      "\n",
      "         [[ 1.9258e-03,  5.7467e-03,  2.2169e-03],\n",
      "          [-3.4925e-04, -5.5284e-03, -2.3603e-03],\n",
      "          [-6.7416e-04,  1.9742e-05,  1.3220e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.7704e-04, -2.4784e-03,  6.7311e-04],\n",
      "          [-1.1834e-03, -9.0075e-04, -4.0139e-04],\n",
      "          [ 8.3568e-04,  5.3501e-03,  4.0455e-03]],\n",
      "\n",
      "         [[-1.6978e-03, -2.5346e-03,  1.1195e-04],\n",
      "          [-2.1123e-04,  2.9087e-05, -5.1634e-04],\n",
      "          [ 1.6030e-03,  2.3158e-03,  9.8446e-04]],\n",
      "\n",
      "         [[ 4.4476e-04,  6.8555e-04, -9.9684e-04],\n",
      "          [ 3.6593e-04, -2.5103e-03, -3.2328e-03],\n",
      "          [ 8.1835e-04,  5.4227e-04,  2.8247e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2559e-03,  3.2259e-03, -8.3262e-03],\n",
      "          [-6.9471e-04, -4.6078e-03, -6.4783e-03],\n",
      "          [ 1.2197e-02, -2.9743e-04, -4.4543e-03]],\n",
      "\n",
      "         [[-1.1164e-03, -8.4781e-03, -4.1936e-03],\n",
      "          [ 3.2307e-03,  6.4584e-03, -1.3477e-03],\n",
      "          [ 4.2395e-05,  2.8690e-03, -9.7231e-03]],\n",
      "\n",
      "         [[ 5.8098e-03, -8.5584e-03, -7.0472e-03],\n",
      "          [-5.3873e-03, -3.2699e-03, -2.3772e-03],\n",
      "          [-8.0767e-04, -1.3111e-03,  5.9271e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4727e-03,  7.6805e-04, -4.9781e-04],\n",
      "          [-1.9837e-03,  3.9713e-04,  4.8703e-04],\n",
      "          [ 6.5172e-03,  4.6850e-03, -1.1537e-02]],\n",
      "\n",
      "         [[-3.1365e-03,  1.1247e-03, -2.4258e-03],\n",
      "          [ 1.7598e-03,  2.0281e-03,  4.6347e-04],\n",
      "          [ 4.3887e-03,  2.0225e-03, -7.1694e-03]],\n",
      "\n",
      "         [[ 1.7102e-03, -9.8579e-03, -1.1240e-02],\n",
      "          [-2.7660e-03, -6.7569e-03,  2.6606e-03],\n",
      "          [ 3.6277e-03, -1.0441e-03,  6.6668e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.6140e-03, -8.1489e-03,  1.5737e-03],\n",
      "          [-3.6003e-03,  2.5495e-03, -1.3128e-02],\n",
      "          [ 8.9293e-03,  7.8164e-03,  6.3054e-03]],\n",
      "\n",
      "         [[-1.4211e-02,  4.8873e-03,  1.7194e-02],\n",
      "          [-3.1590e-03, -1.5146e-02,  8.3790e-03],\n",
      "          [ 1.0030e-02,  5.8394e-03, -2.8718e-03]],\n",
      "\n",
      "         [[-8.1206e-03, -3.2028e-03,  1.9482e-03],\n",
      "          [ 2.1367e-02, -6.8562e-03, -2.1024e-02],\n",
      "          [ 7.0947e-03,  1.2449e-02,  1.3815e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.3357e-03, -5.8399e-03, -7.8572e-03],\n",
      "          [-1.5873e-02, -1.9302e-03,  5.4413e-03],\n",
      "          [ 7.3111e-04,  2.6728e-03,  1.0799e-02]],\n",
      "\n",
      "         [[ 5.2330e-03,  4.1990e-03, -7.2481e-03],\n",
      "          [-8.1932e-03,  8.3274e-03,  9.8238e-03],\n",
      "          [ 5.2855e-04, -8.3485e-04,  1.0714e-02]],\n",
      "\n",
      "         [[ 1.4433e-02, -2.4885e-03, -8.9958e-03],\n",
      "          [ 2.4165e-02,  1.1703e-02, -1.1405e-02],\n",
      "          [ 7.6294e-03,  7.4158e-03,  8.2314e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.6539e-04,  2.0744e-02, -2.4165e-02],\n",
      "          [ 4.3906e-02, -1.3145e-03, -6.4895e-02],\n",
      "          [-6.3549e-03,  1.8509e-03, -1.3380e-02]],\n",
      "\n",
      "         [[ 4.5543e-02,  2.9239e-02, -3.0337e-02],\n",
      "          [-2.6449e-02, -1.1924e-02, -4.0183e-02],\n",
      "          [-5.7997e-03, -5.0385e-03, -2.0682e-02]],\n",
      "\n",
      "         [[ 2.1743e-02, -2.1180e-03, -1.5015e-02],\n",
      "          [-1.1319e-03, -4.3335e-02,  5.2863e-03],\n",
      "          [ 9.4193e-03,  1.4271e-02, -3.3094e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8063e-02, -2.4856e-02,  8.6801e-03],\n",
      "          [ 4.9149e-02, -2.4631e-03, -4.8845e-02],\n",
      "          [ 3.4598e-03,  1.7033e-03, -1.1652e-02]],\n",
      "\n",
      "         [[ 3.4555e-02, -5.9070e-03, -3.3633e-03],\n",
      "          [ 2.2838e-02,  1.1809e-02, -3.7717e-02],\n",
      "          [-5.9263e-03,  1.5697e-02,  2.4918e-03]],\n",
      "\n",
      "         [[-1.6510e-02, -3.9326e-02, -2.6344e-02],\n",
      "          [-6.7947e-03, -4.7086e-02, -2.3495e-02],\n",
      "          [-1.2395e-02, -1.8951e-03, -1.1188e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0456e-03,  3.2629e-03, -1.4433e-02],\n",
      "          [ 1.4858e-03, -5.4042e-03, -1.4452e-02],\n",
      "          [ 1.2869e-02,  5.1075e-03, -3.8590e-03]],\n",
      "\n",
      "         [[-4.7931e-04,  2.7073e-03, -8.6791e-03],\n",
      "          [-5.9937e-03,  5.1857e-03, -5.8278e-03],\n",
      "          [ 7.0193e-03, -2.7993e-03, -8.4405e-03]],\n",
      "\n",
      "         [[ 4.7516e-03, -1.4293e-02, -4.5474e-03],\n",
      "          [-1.7526e-03, -7.7592e-03, -5.4934e-03],\n",
      "          [ 7.1469e-03,  7.9531e-03,  3.0645e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9728e-03, -2.5201e-03, -1.2370e-03],\n",
      "          [ 1.8115e-03,  1.0261e-03, -1.1933e-02],\n",
      "          [ 9.0158e-03,  1.6486e-03, -5.1804e-03]],\n",
      "\n",
      "         [[ 5.2995e-03,  1.6204e-03, -8.2074e-03],\n",
      "          [ 5.1051e-03,  5.7607e-03, -4.1460e-03],\n",
      "          [ 3.1587e-03,  3.0419e-03,  1.6331e-03]],\n",
      "\n",
      "         [[ 1.7309e-03, -1.6948e-02, -1.0711e-02],\n",
      "          [ 3.3707e-03, -5.8697e-03, -2.3204e-03],\n",
      "          [ 5.3760e-03,  5.2785e-03,  2.8489e-03]]]])\n",
      "Gradient for layer4.1.norm2.weight:\n",
      "tensor([-1.7506e-02, -8.6776e-03, -9.9372e-03,  2.3889e-02,  4.0776e-02,\n",
      "         1.5308e-02,  1.9965e-02, -1.9593e-02,  1.8238e-02, -4.8898e-04,\n",
      "         1.8078e-02, -1.0262e-02,  1.0683e-02, -3.1984e-02, -8.7421e-03,\n",
      "        -8.1321e-03, -2.5272e-02,  2.2877e-02, -1.0999e-02, -1.7204e-02,\n",
      "         3.2100e-02,  2.6015e-02, -5.0781e-03,  1.8169e-02,  3.7465e-02,\n",
      "         1.5834e-03, -8.6098e-03, -5.3878e-02, -3.9581e-03,  4.4738e-02,\n",
      "        -1.8824e-02,  2.2867e-02,  3.3223e-02, -3.2714e-02,  2.1813e-02,\n",
      "        -1.3479e-02, -1.9824e-02,  2.8843e-02,  1.1073e-02, -1.6995e-03,\n",
      "        -4.8943e-02, -3.0699e-02, -2.5634e-02,  1.8783e-02, -4.7961e-03,\n",
      "         2.7059e-02, -4.0607e-02, -6.8786e-03,  3.6462e-02, -1.8494e-02,\n",
      "        -1.3810e-02,  4.0427e-03, -3.1534e-02,  1.3879e-02, -9.3723e-03,\n",
      "        -1.5413e-02, -1.6278e-03,  3.8156e-03,  5.0838e-02, -7.2236e-03,\n",
      "         3.9872e-02, -1.4512e-02, -9.5501e-03,  1.5401e-02, -1.0763e-02,\n",
      "         1.0111e-02,  1.8219e-02, -3.5504e-02,  5.1077e-02, -3.4074e-03,\n",
      "         4.2082e-02, -2.3071e-03,  3.5535e-03,  8.8263e-03, -6.0440e-02,\n",
      "        -1.3876e-02, -3.8780e-02,  1.1110e-02, -1.7654e-04, -1.5023e-02,\n",
      "        -2.4578e-02,  1.5706e-02, -8.8848e-03, -2.5004e-02, -3.4261e-02,\n",
      "        -3.8179e-02, -2.7430e-02,  1.3126e-02, -4.2266e-03, -5.6239e-02,\n",
      "         3.6267e-02,  5.3764e-03, -3.1484e-02, -6.0133e-02,  2.1556e-02,\n",
      "        -1.4120e-02, -2.5259e-02,  6.2412e-02,  4.7330e-02,  2.5594e-02,\n",
      "        -1.3120e-02,  3.0949e-02, -4.0500e-02, -8.6631e-03,  1.2586e-02,\n",
      "         1.1144e-03, -3.1847e-02, -3.0468e-02, -2.6373e-02, -3.5576e-02,\n",
      "         1.1206e-02,  2.4671e-02,  3.0715e-02, -7.5901e-03, -4.7543e-02,\n",
      "        -9.8416e-03,  9.6448e-03,  2.3463e-02,  3.2866e-02,  7.3183e-03,\n",
      "         2.1100e-02, -2.0046e-02,  6.0366e-03, -9.9904e-03, -5.3854e-04,\n",
      "        -1.9054e-02, -6.8714e-03, -7.2475e-03, -1.0288e-02, -5.4884e-03,\n",
      "         1.3901e-03,  7.6614e-03, -3.7218e-02, -3.5008e-02,  1.7725e-02,\n",
      "        -7.0267e-03, -1.1589e-02, -3.2497e-02,  2.0549e-02,  3.1356e-03,\n",
      "        -1.5315e-02,  2.4211e-02, -2.0759e-02,  4.1936e-02, -1.2563e-02,\n",
      "        -1.3120e-02,  1.0911e-03, -3.5267e-02, -2.0775e-02,  3.9324e-02,\n",
      "         4.8018e-02,  2.2299e-02,  1.9837e-02, -3.8045e-02,  2.4781e-02,\n",
      "         2.9753e-02, -9.7529e-03,  2.8961e-02, -6.7112e-03,  2.2846e-02,\n",
      "         2.8356e-03, -1.6510e-02,  1.1844e-02,  3.3439e-02, -3.0763e-02,\n",
      "         3.1266e-02,  2.3148e-02,  1.0722e-02, -1.3825e-02,  7.4309e-03,\n",
      "         2.6254e-02,  8.9648e-04,  1.7451e-02, -1.6239e-02,  3.4652e-02,\n",
      "         2.9233e-02,  5.0953e-03, -4.3319e-03, -1.3387e-02, -1.3315e-02,\n",
      "        -1.7361e-02, -3.8910e-02, -2.1664e-02,  2.0442e-02, -1.6656e-02,\n",
      "        -9.2687e-03, -3.2545e-02,  7.4388e-03, -2.2638e-02,  3.0022e-02,\n",
      "         2.2721e-02,  5.6159e-04,  2.6656e-02, -7.0796e-03, -1.5388e-03,\n",
      "         1.3283e-02,  2.2854e-02, -2.9498e-02,  1.4213e-02,  4.1800e-02,\n",
      "         4.2676e-02, -1.3484e-02, -1.9517e-02, -8.5567e-04, -2.7983e-02,\n",
      "         4.8562e-02, -4.7510e-03, -3.8208e-02, -4.5270e-02, -4.2888e-02,\n",
      "        -1.7825e-02, -5.0837e-02,  1.0005e-02,  5.8699e-03,  1.9286e-02,\n",
      "         5.8333e-03,  1.6522e-02, -5.7771e-02,  1.8344e-02, -3.2190e-02,\n",
      "         7.3044e-03,  1.7136e-02,  1.3477e-02, -6.3165e-03,  1.7329e-02,\n",
      "         5.0395e-03, -1.4427e-02, -2.5484e-02,  1.7513e-02,  9.1890e-03,\n",
      "        -6.3240e-02,  1.2633e-03, -3.1669e-03,  8.7524e-03,  2.4198e-02,\n",
      "         1.9737e-02,  2.0691e-02,  5.2847e-02,  3.9139e-02, -1.2811e-03,\n",
      "         2.9771e-02,  2.3990e-02, -5.3701e-03,  5.9171e-02, -3.0186e-02,\n",
      "         3.1680e-02, -9.6216e-04,  1.9276e-02,  4.4315e-02,  2.9565e-02,\n",
      "         9.8757e-03,  6.4827e-04, -3.0155e-02, -2.1456e-03,  8.0706e-03,\n",
      "         2.8404e-03,  5.6099e-03,  2.1132e-02,  1.1542e-02,  4.2783e-02,\n",
      "         4.5238e-03,  2.2920e-02,  3.4726e-02,  2.1920e-02,  9.5642e-03,\n",
      "        -9.1498e-03,  3.1043e-02,  3.9145e-02,  3.9752e-02, -2.8458e-02,\n",
      "         2.4343e-03,  2.7696e-02, -4.0530e-03,  6.2568e-03, -3.0003e-02,\n",
      "         3.0972e-02, -1.1726e-02, -8.0817e-03, -4.2030e-02,  1.7650e-02,\n",
      "         2.6349e-02,  1.9910e-02, -2.6692e-02, -6.1995e-03, -1.8788e-02,\n",
      "         3.1730e-02, -2.0395e-02,  2.3281e-02,  2.5743e-02, -5.2620e-03,\n",
      "        -1.3396e-02, -2.0632e-02, -2.6618e-02, -2.9186e-02, -1.6837e-02,\n",
      "         2.8435e-03,  3.6917e-02, -2.1270e-02, -7.8395e-02, -8.4674e-03,\n",
      "        -2.5401e-05, -1.9022e-02,  8.1514e-03,  6.4619e-03,  3.7582e-02,\n",
      "         7.0036e-02,  1.4043e-02, -9.2626e-03, -2.2251e-02, -4.5026e-02,\n",
      "         3.0429e-02,  3.8199e-02,  6.5238e-03, -1.3731e-02, -1.7123e-02,\n",
      "        -1.0039e-02, -1.6570e-02,  1.7314e-02, -2.8584e-02,  2.3201e-02,\n",
      "        -3.4058e-02,  2.7416e-02,  6.5997e-02, -1.1312e-02, -3.2728e-02,\n",
      "        -1.5579e-02,  4.4325e-02, -2.0917e-02, -1.0958e-02,  2.2807e-02,\n",
      "        -1.2031e-02,  3.2757e-02, -7.4390e-03, -1.8109e-02,  2.5108e-02,\n",
      "        -2.3177e-02, -3.7074e-02,  3.8509e-02, -4.3717e-03, -1.3080e-02,\n",
      "         6.1510e-03, -6.5401e-02, -2.2445e-02, -4.8874e-02, -9.1676e-03,\n",
      "        -1.2169e-03, -3.1311e-02, -1.7083e-04,  1.3268e-02,  6.1649e-02,\n",
      "         1.3125e-02, -3.0909e-02, -1.3342e-02,  4.6340e-02, -1.6942e-02,\n",
      "        -2.3046e-02, -3.5288e-03, -1.6917e-02,  8.8813e-03,  2.3245e-02,\n",
      "         5.5062e-03,  2.9719e-02,  4.6309e-02, -1.4522e-02, -7.4449e-04,\n",
      "         2.2797e-03, -1.1239e-02, -2.1216e-02, -5.7452e-03,  5.1649e-03,\n",
      "        -2.5097e-02, -3.5453e-02, -2.4190e-02, -1.4482e-02,  1.0516e-02,\n",
      "        -8.6129e-03, -6.3541e-02,  8.0287e-04,  2.4363e-02, -9.2508e-03,\n",
      "        -4.7400e-03,  3.0375e-02, -2.0479e-03, -4.8771e-02, -4.1573e-03,\n",
      "         1.1142e-02,  3.6000e-03,  4.2245e-02, -4.5904e-03,  2.4070e-02,\n",
      "         9.3884e-03, -4.0539e-02, -1.0507e-02, -1.9904e-02, -7.5555e-03,\n",
      "        -2.2924e-02,  3.6121e-02,  5.5523e-03, -2.3420e-03, -7.1652e-03,\n",
      "         8.8587e-03,  7.9527e-03, -5.8723e-03,  4.0572e-03,  1.2675e-02,\n",
      "         1.0689e-03, -1.6545e-02, -1.4281e-02, -3.8616e-02, -5.7182e-02,\n",
      "        -1.6501e-02, -2.4924e-02,  2.3274e-02,  3.5274e-02, -4.2961e-02,\n",
      "        -1.0677e-02,  4.0752e-02, -5.4361e-03,  5.6868e-02,  4.1122e-03,\n",
      "         2.2651e-02,  1.4489e-02, -8.8727e-04, -2.0860e-02, -6.7589e-03,\n",
      "        -7.5020e-02,  1.8701e-02, -5.0215e-03,  2.8818e-02,  1.2176e-02,\n",
      "         1.1665e-02, -1.7727e-02,  1.5823e-02, -4.3725e-03,  3.2924e-02,\n",
      "         1.4661e-03,  4.5511e-02,  6.3969e-03, -3.5182e-02, -1.4587e-02,\n",
      "         3.6670e-02,  9.4687e-03, -8.6207e-03,  4.7844e-02,  2.0658e-02,\n",
      "        -1.6854e-02, -1.2189e-02,  1.6808e-02,  4.7647e-03,  3.1134e-04,\n",
      "         2.8355e-02, -2.2188e-02, -2.3190e-02,  1.3981e-02,  1.2565e-03,\n",
      "         1.1011e-02,  3.6020e-02, -4.7150e-02,  3.4081e-03,  1.2590e-02,\n",
      "         1.3689e-02,  1.1502e-02,  2.4389e-02, -2.6371e-02, -3.0797e-03,\n",
      "        -3.9264e-02,  1.5981e-02,  2.7217e-02, -3.0083e-02, -6.3215e-04,\n",
      "        -5.9076e-03, -3.2663e-02,  2.8564e-03, -5.0122e-02, -5.1343e-02,\n",
      "        -3.8040e-02,  6.2868e-06,  1.3053e-02, -6.3379e-02, -1.9660e-02,\n",
      "        -9.7209e-03,  1.5105e-02, -1.1832e-02, -3.3471e-02, -1.0055e-02,\n",
      "         3.0429e-02,  6.9189e-03,  3.8424e-03,  1.5305e-02, -1.5841e-02,\n",
      "         2.9951e-02, -2.0651e-03,  2.9599e-02,  8.1106e-03,  6.9333e-03,\n",
      "         1.3265e-03,  1.3380e-02,  1.1380e-02,  1.3016e-03, -8.0296e-03,\n",
      "         1.7361e-02, -2.1093e-02, -1.9739e-02, -8.7830e-03, -7.8645e-02,\n",
      "        -2.5288e-02, -9.8586e-03, -1.0862e-02,  6.1031e-03, -1.9556e-02,\n",
      "         5.8528e-02,  1.0377e-02])\n",
      "Gradient for layer4.1.norm2.bias:\n",
      "tensor([-3.9676e-02, -1.5986e-02, -2.4838e-02,  4.0123e-02,  8.5608e-02,\n",
      "         2.3220e-02,  3.4042e-02, -3.3809e-02,  3.5569e-02, -1.1607e-03,\n",
      "         2.9179e-02, -1.9400e-02,  3.1317e-02, -7.6842e-02, -1.1567e-02,\n",
      "        -1.2294e-02, -5.4851e-02,  5.0190e-02, -3.3262e-02, -3.0326e-02,\n",
      "         5.7992e-02,  5.6725e-02, -9.5579e-03,  2.9713e-02,  7.9255e-02,\n",
      "         3.3969e-03, -1.7247e-02, -1.0968e-01, -9.9535e-03,  8.3869e-02,\n",
      "        -4.8088e-02,  3.6425e-02,  8.8472e-02, -5.2571e-02,  5.5708e-02,\n",
      "        -4.7988e-02, -4.0696e-02,  7.2349e-02,  4.4484e-02, -3.0910e-03,\n",
      "        -1.2482e-01, -4.7323e-02, -4.3667e-02,  3.9941e-02, -1.0640e-02,\n",
      "         6.2027e-02, -8.4797e-02, -1.4364e-02,  4.3916e-02, -4.1689e-02,\n",
      "        -5.2824e-02,  7.5137e-03, -4.1549e-02,  2.8478e-02, -1.4435e-02,\n",
      "        -5.4808e-02, -3.2720e-03,  4.3781e-03,  9.5785e-02, -1.9638e-02,\n",
      "         1.4839e-01, -1.7208e-02, -3.0445e-02,  3.2798e-02, -4.9524e-02,\n",
      "         2.4130e-02,  2.7697e-02, -7.3018e-02,  1.1665e-01, -6.5124e-03,\n",
      "         7.2647e-02, -4.6565e-03,  5.4850e-03,  2.5678e-02, -7.3710e-02,\n",
      "        -3.6556e-02, -6.3156e-02,  1.9014e-02, -5.3384e-04, -2.3674e-02,\n",
      "        -4.3255e-02,  2.6657e-02, -1.5762e-02, -4.9040e-02, -6.1062e-02,\n",
      "        -7.5840e-02, -1.0430e-01,  2.6344e-02, -6.6146e-03, -1.1949e-01,\n",
      "         1.0587e-01,  8.3253e-03, -4.1638e-02, -8.3260e-02,  6.4079e-02,\n",
      "        -5.0407e-02, -3.6464e-02,  1.2006e-01,  8.4562e-02,  7.0006e-02,\n",
      "        -3.6976e-02,  5.2866e-02, -6.6014e-02, -1.4128e-02,  2.4367e-02,\n",
      "         2.3341e-03, -7.2527e-02, -4.3834e-02, -1.1712e-01, -7.8743e-02,\n",
      "         1.9882e-02,  8.0323e-02,  5.9205e-02, -2.4577e-02, -1.0995e-01,\n",
      "        -1.0812e-02,  2.9134e-02,  4.6813e-02,  6.9385e-02,  1.0336e-02,\n",
      "         3.0477e-02, -3.1513e-02,  1.6235e-02, -1.8028e-02, -1.6350e-03,\n",
      "        -3.2787e-02, -1.4536e-02, -1.5787e-02, -1.4354e-02, -9.5937e-03,\n",
      "         4.4882e-03,  1.7828e-02, -7.6400e-02, -1.1802e-01,  3.6236e-02,\n",
      "        -1.3530e-02, -1.7702e-02, -5.5479e-02,  4.0582e-02,  5.1813e-03,\n",
      "        -5.2589e-02,  8.5660e-02, -6.2978e-02,  5.9818e-02, -2.7241e-02,\n",
      "        -2.0971e-02,  1.6463e-03, -7.1610e-02, -6.8465e-02,  4.7632e-02,\n",
      "         7.3203e-02,  2.9335e-02,  4.3173e-02, -1.3127e-01,  4.2364e-02,\n",
      "         3.6388e-02, -1.6664e-02,  6.9535e-02, -1.4252e-02,  4.9755e-02,\n",
      "         4.6423e-03, -4.8592e-02,  2.4131e-02,  5.5215e-02, -6.5833e-02,\n",
      "         7.5364e-02,  3.3913e-02,  2.4049e-02, -2.2515e-02,  1.2304e-02,\n",
      "         4.1974e-02,  1.3611e-03,  2.5580e-02, -3.1108e-02,  9.8898e-02,\n",
      "         4.6586e-02,  2.6516e-02, -7.5595e-03, -2.5019e-02, -2.2712e-02,\n",
      "        -5.8603e-02, -7.3307e-02, -3.8950e-02,  1.1532e-01, -3.2985e-02,\n",
      "        -1.9029e-02, -4.3021e-02,  1.5849e-02, -3.0745e-02,  4.0399e-02,\n",
      "         5.5592e-02,  7.6488e-04,  4.8602e-02, -1.1542e-02, -2.7095e-03,\n",
      "         3.1079e-02,  5.5485e-02, -7.7278e-02,  2.4064e-02,  7.2885e-02,\n",
      "         9.6837e-02, -2.5625e-02, -3.2401e-02, -2.4541e-03, -7.3703e-02,\n",
      "         7.6898e-02, -1.1990e-02, -8.5744e-02, -8.7786e-02, -5.8342e-02,\n",
      "        -3.2622e-02, -8.1558e-02,  2.3222e-02,  1.0303e-02,  3.3308e-02,\n",
      "         1.3315e-02,  3.2026e-02, -7.6580e-02,  4.9072e-02, -9.8256e-02,\n",
      "         1.5673e-02,  4.8692e-02,  3.5501e-02, -1.1603e-02,  3.0619e-02,\n",
      "         7.9121e-03, -2.7622e-02, -5.7104e-02,  2.7715e-02,  5.5833e-02,\n",
      "        -9.0778e-02,  3.2296e-03, -9.5665e-03,  2.9487e-02,  7.5165e-02,\n",
      "         3.9004e-02,  3.6964e-02,  1.3011e-01,  8.6418e-02, -2.5211e-03,\n",
      "         6.2809e-02,  5.0209e-02, -1.5425e-02,  9.1801e-02, -6.6210e-02,\n",
      "         7.6821e-02, -1.8037e-03,  2.6159e-02,  6.7200e-02,  6.5446e-02,\n",
      "         1.1365e-02,  1.1700e-03, -5.0827e-02, -4.6264e-03,  2.3420e-02,\n",
      "         5.2906e-03,  1.1953e-02,  4.4721e-02,  2.5678e-02,  7.3558e-02,\n",
      "         8.0696e-03,  5.9579e-02,  5.4330e-02,  4.2458e-02,  1.7875e-02,\n",
      "        -2.1815e-02,  7.6843e-02,  6.7868e-02,  7.9511e-02, -5.1591e-02,\n",
      "         3.6007e-03,  9.3177e-02, -9.3882e-03,  1.0279e-02, -5.9406e-02,\n",
      "         3.5283e-02, -1.6627e-02, -1.8270e-02, -1.0581e-01,  7.7381e-02,\n",
      "         4.8947e-02,  5.0022e-02, -4.9736e-02, -1.1216e-02, -5.9734e-02,\n",
      "         9.5467e-02, -2.8734e-02,  6.3958e-02,  4.4487e-02, -1.4564e-02,\n",
      "        -2.6691e-02, -3.8611e-02, -4.0385e-02, -3.6973e-02, -3.2216e-02,\n",
      "         7.4014e-03,  7.9010e-02, -4.9482e-02, -1.5388e-01, -1.2010e-02,\n",
      "        -4.4235e-05, -4.0308e-02,  1.9002e-02,  1.2307e-02,  4.9828e-02,\n",
      "         9.2802e-02,  3.8678e-02, -2.0728e-02, -3.8554e-02, -7.8123e-02,\n",
      "         4.4735e-02,  5.2312e-02,  1.8884e-02, -3.0344e-02, -1.9354e-02,\n",
      "        -2.0744e-02, -2.7410e-02,  3.8281e-02, -3.8316e-02,  8.1951e-02,\n",
      "        -7.8768e-02,  5.7647e-02,  1.1413e-01, -2.3078e-02, -7.3873e-02,\n",
      "        -3.3558e-02,  6.8044e-02, -4.1786e-02, -1.4317e-02,  5.1024e-02,\n",
      "        -3.4216e-02,  1.1593e-01, -2.4153e-02, -3.7743e-02,  7.1888e-02,\n",
      "        -1.0361e-01, -7.0765e-02,  5.2090e-02, -1.5025e-02, -2.6218e-02,\n",
      "         1.2560e-02, -8.7392e-02, -5.5558e-02, -8.5232e-02, -2.0640e-02,\n",
      "        -3.1238e-03, -7.7709e-02, -4.7083e-04,  1.9997e-02,  1.0070e-01,\n",
      "         2.7056e-02, -1.4143e-01, -2.5139e-02,  8.5868e-02, -2.6815e-02,\n",
      "        -4.6005e-02, -6.0729e-03, -3.1451e-02,  1.3876e-02,  4.8830e-02,\n",
      "         1.2606e-02,  6.6496e-02,  9.2036e-02, -4.3484e-02, -1.2389e-03,\n",
      "         3.6095e-03, -2.3296e-02, -2.2509e-02, -3.0537e-02,  1.0678e-02,\n",
      "        -5.7741e-02, -6.9525e-02, -5.4674e-02, -2.3423e-02,  1.9495e-02,\n",
      "        -2.7678e-02, -1.3458e-01,  1.5748e-03,  3.7876e-02, -2.2256e-02,\n",
      "        -8.2685e-03,  5.4106e-02, -5.5424e-03, -7.9518e-02, -8.0054e-03,\n",
      "         1.4323e-02,  1.1637e-02,  7.8480e-02, -1.1360e-02,  3.5440e-02,\n",
      "         1.7296e-02, -7.7307e-02, -2.2295e-02, -6.5125e-02, -1.1306e-02,\n",
      "        -5.5314e-02,  4.5669e-02,  2.6719e-02, -8.9007e-03, -1.6746e-02,\n",
      "         2.3367e-02,  1.6669e-02, -3.8300e-02,  1.7061e-02,  2.8263e-02,\n",
      "         2.0167e-03, -3.2826e-02, -4.5356e-02, -8.5056e-02, -8.2335e-02,\n",
      "        -4.7104e-02, -4.3787e-02,  5.0771e-02,  1.0233e-01, -8.2935e-02,\n",
      "        -2.1365e-02,  1.4117e-01, -1.7749e-02,  9.0454e-02,  6.0373e-03,\n",
      "         6.2400e-02,  2.6050e-02, -2.2715e-03, -3.2609e-02, -1.2951e-02,\n",
      "        -1.3667e-01,  3.7958e-02, -9.6720e-03,  5.6253e-02,  4.0013e-02,\n",
      "         3.9205e-02, -3.2485e-02,  5.7253e-02, -1.2436e-02,  6.8750e-02,\n",
      "         3.8038e-03,  9.9419e-02,  1.3215e-02, -6.0971e-02, -3.3453e-02,\n",
      "         6.8152e-02,  2.1609e-02, -1.4666e-02,  1.0590e-01,  3.0370e-02,\n",
      "        -2.3058e-02, -2.5805e-02,  3.1749e-02,  6.8379e-03,  5.9545e-04,\n",
      "         7.1265e-02, -4.1093e-02, -3.1660e-02,  3.0117e-02,  2.6820e-03,\n",
      "         1.4881e-02,  8.9767e-02, -8.7170e-02,  6.0289e-03,  2.3328e-02,\n",
      "         8.7621e-02,  4.9290e-02,  4.7042e-02, -4.8511e-02, -9.1046e-03,\n",
      "        -1.0074e-01,  3.6189e-02,  5.6772e-02, -6.6586e-02, -1.1468e-03,\n",
      "        -1.4415e-02, -1.1040e-01,  6.4558e-03, -6.0826e-02, -8.6154e-02,\n",
      "        -8.4951e-02,  3.1024e-05,  1.9223e-02, -1.3032e-01, -5.5257e-02,\n",
      "        -1.8252e-02,  3.0667e-02, -3.3452e-02, -4.7754e-02, -2.0398e-02,\n",
      "         5.3903e-02,  1.5544e-02,  6.9563e-03,  2.8537e-02, -3.2345e-02,\n",
      "         5.1937e-02, -5.1029e-03,  7.0872e-02,  1.6296e-02,  1.9986e-02,\n",
      "         3.3512e-03,  2.1564e-02,  2.2104e-02,  1.9342e-03, -2.2255e-02,\n",
      "         3.3173e-02, -3.8396e-02, -5.1860e-02, -1.9350e-02, -1.1625e-01,\n",
      "        -6.1422e-02, -2.2371e-02, -1.9110e-02,  1.1766e-02, -2.9707e-02,\n",
      "         8.0792e-02,  2.4346e-02])\n",
      "Gradient for fc.weight:\n",
      "tensor([[0.8184, 0.7765, 0.8048,  ..., 0.7218, 0.9564, 0.8606],\n",
      "        [0.8184, 0.7765, 0.8048,  ..., 0.7218, 0.9564, 0.8606],\n",
      "        [0.8184, 0.7765, 0.8048,  ..., 0.7218, 0.9564, 0.8606],\n",
      "        ...,\n",
      "        [0.8184, 0.7765, 0.8048,  ..., 0.7218, 0.9564, 0.8606],\n",
      "        [0.8184, 0.7765, 0.8048,  ..., 0.7218, 0.9564, 0.8606],\n",
      "        [0.8184, 0.7765, 0.8048,  ..., 0.7218, 0.9564, 0.8606]])\n",
      "Gradient for fc.bias:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "        # If the input and output dimensions don't match, add a 1x1 convolution\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        out += self.shortcut(residual)  # Adding the shortcut connection\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Create a simple neural network with a residual block\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(64, 64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self.make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self.make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self.make_layer(256, 512, num_blocks=2, stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the ResNet model\n",
    "model = ResNet()\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = torch.randn(1, 3, 32, 32)  # (batch_size, num_channels, height, width)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss = torch.sum(output)  # Example loss function\n",
    "loss.backward()\n",
    "\n",
    "# Gradients for the parameters can be accessed via model parameters:\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Gradient for {name}:\")\n",
    "        print(param.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1205898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([-0.0276], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # Input Layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Skip Connection\n",
    "        self.skip_connection = nn.Identity()  # Identity mapping\n",
    "        \n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        \n",
    "        # Input Layer\n",
    "        x_input = self.input_layer(x)\n",
    "        \n",
    "        # Hidden Layer\n",
    "        x_hidden = self.hidden_layer(x_input)\n",
    "        \n",
    "        # Skip Connection\n",
    "        x_skip = self.skip_connection(x_input)  # Identity mapping\n",
    "        \n",
    "        # Combine Hidden Layer Output and Skip Connection Output\n",
    "#         x_combined = x_hidden + x_skip  # Residual connection\n",
    "        \n",
    "        # Apply Activation Function (e.g., ReLU)\n",
    "        x_combined = torch.relu(x_skip)\n",
    "        \n",
    "        # Output Layer\n",
    "        x_output = self.output_layer(x_combined)\n",
    "        \n",
    "        return x_output\n",
    "\n",
    "# Example network with 2 input features, 4 hidden units, and 1 output unit\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Create an instance of the network\n",
    "net = SimpleNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sample input tensor (batch size of 1 and 2 input features)\n",
    "sample_input = torch.tensor([2.0, 3.0])\n",
    "\n",
    "# Forward pass to obtain the output\n",
    "output = net(sample_input)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0313f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer.weight Parameter containing:\n",
      "tensor([[-0.0027, -0.6629],\n",
      "        [-0.2289, -0.6598],\n",
      "        [ 0.0102, -0.1350],\n",
      "        [-0.6924, -0.7064]], requires_grad=True)\n",
      "input_layer.bias Parameter containing:\n",
      "tensor([0.5520, 0.1886, 0.3485, 0.1736], requires_grad=True)\n",
      "hidden_layer.weight Parameter containing:\n",
      "tensor([[ 0.3314, -0.0601, -0.2322,  0.2847],\n",
      "        [-0.2038, -0.2777, -0.0528, -0.0239],\n",
      "        [-0.1964, -0.3639,  0.3963,  0.2717],\n",
      "        [-0.1706, -0.1071, -0.2087,  0.3690]], requires_grad=True)\n",
      "hidden_layer.bias Parameter containing:\n",
      "tensor([-0.4768,  0.0874,  0.2764, -0.0373], requires_grad=True)\n",
      "output_layer.weight Parameter containing:\n",
      "tensor([[-0.2453, -0.4177,  0.1424,  0.1436]], requires_grad=True)\n",
      "output_layer.bias Parameter containing:\n",
      "tensor([-0.0276], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name , param in net.named_parameters():\n",
    "    print(name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca77072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHiddenLayerWithSkipConnection(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SingleHiddenLayerWithSkipConnection, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)  # Input layer\n",
    "        self.hidden_layer = nn.Linear(hidden_size, output_size)  # Hidden layer\n",
    "        \n",
    "        self.activation = nn.ReLU()  # Activation function (ReLU in this case)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        \n",
    "        # Input layer\n",
    "        x_input = self.input_layer(x)\n",
    "        \n",
    "        # Apply activation function (ReLU)\n",
    "        x_hidden = self.activation(x_input)\n",
    "        \n",
    "        # Skip connection (identity mapping)\n",
    "        x_skip = x\n",
    "        \n",
    "        # Combine the output of the hidden layer and the skip connection\n",
    "        x_combined = x_hidden + x_skip  # Residual connection\n",
    "        \n",
    "        # Hidden layer\n",
    "        x_output = self.hidden_layer(x_combined)\n",
    "        \n",
    "        return x_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6513ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the neural network\n",
    "input_size = 5  # Number of input features\n",
    "hidden_size = 10  # Number of units in the hidden layer\n",
    "output_size = 3  # Number of output units (e.g., for classification)\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SingleHiddenLayerWithSkipConnection(input_size, hidden_size, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b6e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent (SGD) optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a00a16",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n",
      "File \u001b[1;32mD:\\Conda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mSingleHiddenLayerWithSkipConnection.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m x_skip \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Combine the output of the hidden layer and the skip connection\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m x_combined \u001b[38;5;241m=\u001b[39m x_hidden \u001b[38;5;241m+\u001b[39m x_skip  \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Hidden layer\u001b[39;00m\n\u001b[0;32m     27\u001b[0m x_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(x_combined)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Sample data (for demonstration purposes)\n",
    "X_train = torch.randn(100, input_size)  # 100 samples, 5 features each\n",
    "y_train = torch.randint(0, output_size, (100,))  # Random integer labels\n",
    "\n",
    "# Training loop (for demonstration)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss for monitoring training progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4db8e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0757385f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5330e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
